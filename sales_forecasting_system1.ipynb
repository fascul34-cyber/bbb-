{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –°–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–¥–∞–∂ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≥—Ä—É–∑–æ–∫\n",
        "## Wildberries & Ozon\n",
        "\n",
        "**–¶–µ–ª—å:** –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–∏—Å—Ç–µ–º—É –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–¥–∞–∂ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≥—Ä—É–∑–æ–∫ —Å rolling-–ø—Ä–æ–≥–Ω–æ–∑–æ–º –Ω–∞ 18 –º–µ—Å—è—Ü–µ–≤ –≤–ø–µ—Ä—ë–¥ –ø–æ –∫–∞–∂–¥–æ–º—É –ø—Ä–æ–¥—É–∫—Ç—É (solo-code).\n",
        "\n",
        "**–ü–µ—Ä–∏–æ–¥:** 26.03.2024 ‚Äì 20.12.2025  \n",
        "**–ß–∞—Å—Ç–æ—Ç–∞:** –î–Ω–µ–≤–Ω–∞—è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from prophet import Prophet\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ñ–æ—Ä–º–∞—Ç—ã CSV –∏ Excel. –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –Ω–∏–∂–µ.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# –ù–ê–°–¢–†–û–ô–ö–ê –ü–£–¢–ï–ô –ö –§–ê–ô–õ–ê–ú\n",
        "# ============================================\n",
        "# –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç–∏ –∫ –≤–∞—à–∏–º —Ñ–∞–π–ª–∞–º –∏–ª–∏ –æ—Å—Ç–∞–≤—å—Ç–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "\n",
        "FILE_PATHS = {\n",
        "    'wb_sales': 'wb_sales.csv',  # –î–∞—Ç–∞, solo-code, SKU, –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.\n",
        "    'ozon_sales': 'ozon_sales.csv',  # –î–∞—Ç–∞, solo-code, SKU, –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.\n",
        "    'historical_shipment': 'historical_shipment.csv',  # solo-code, –î–∞—Ç–∞, –ö–æ–ª–∏—á–µ—Å—Ç–≤–∞\n",
        "    'wb_stocks': 'wb_stocks.csv',  # –î–∞—Ç–∞, –°–∫–ª–∞–¥, solo-code, SKU, –û—Å—Ç–∞—Ç–æ–∫\n",
        "    'ozon_stocks': 'ozon_stocks.csv',  # –î–∞—Ç–∞, –°–∫–ª–∞–¥, solo-code, SKU, –û—Å—Ç–∞—Ç–æ–∫\n",
        "    'our_stocks': 'our_stocks.csv',  # –î–∞—Ç–∞, solo-code, SKU, –û—Å—Ç–∞—Ç–æ–∫\n",
        "    'withdraw': 'withdraw.csv',  # solo-code, SKU\n",
        "    'defecture': 'defecture.csv',  # solo-code, SKU, –î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –¥–µ—Ñ–µ–∫—Ç—É—Ä—ã\n",
        "    'count_box': 'count_box.csv'  # solo-code, –ö–æ–ª-–≤–æ\n",
        "}\n",
        "\n",
        "def load_file(file_path, file_type='csv'):\n",
        "    \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ —Ñ–∞–π–ª–∞ CSV –∏–ª–∏ Excel\"\"\"\n",
        "    try:\n",
        "        if file_type == 'csv':\n",
        "            df = pd.read_csv(file_path, encoding='utf-8')\n",
        "        else:\n",
        "            df = pd.read_excel(file_path)\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ö†Ô∏è –§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "print(\"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "data = {}\n",
        "\n",
        "for key, path in FILE_PATHS.items():\n",
        "    # –ü—Ä–æ–±—É–µ–º CSV\n",
        "    df = load_file(path, 'csv')\n",
        "    if df is None:\n",
        "        # –ü—Ä–æ–±—É–µ–º Excel\n",
        "        excel_path = path.replace('.csv', '.xlsx')\n",
        "        df = load_file(excel_path, 'excel')\n",
        "    \n",
        "    if df is not None:\n",
        "        data[key] = df\n",
        "        print(f\"‚úÖ {key}: {len(df)} —Å—Ç—Ä–æ–∫, –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns)}\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è {key}: —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω, –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω —Ç–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç\")\n",
        "\n",
        "print(f\"\\nüìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len([k for k, v in data.items() if v is not None])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ —Ñ–∞–π–ª—ã –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\n",
        "def create_sample_data():\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\"\"\"\n",
        "    np.random.seed(42)\n",
        "    dates = pd.date_range('2024-03-26', '2025-12-20', freq='D')\n",
        "    solo_codes = ['SC001', 'SC002', 'SC003']\n",
        "    skus = {sc: [f'{sc}_SKU{i}' for i in range(1, 4)] for sc in solo_codes}\n",
        "    \n",
        "    sample_data = {}\n",
        "    \n",
        "    # –ü—Ä–æ–¥–∞–∂–∏ WB\n",
        "    wb_sales = []\n",
        "    for date in dates:\n",
        "        for sc in solo_codes:\n",
        "            for sku in skus[sc]:\n",
        "                qty = max(0, int(np.random.poisson(10) * (1 + 0.3 * np.sin(date.dayofyear / 365 * 2 * np.pi))))\n",
        "                wb_sales.append({'–î–∞—Ç–∞': date, 'solo-code': sc, 'SKU': sku, '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.': qty})\n",
        "    sample_data['wb_sales'] = pd.DataFrame(wb_sales)\n",
        "    \n",
        "    # –ü—Ä–æ–¥–∞–∂–∏ Ozon\n",
        "    ozon_sales = []\n",
        "    for date in dates:\n",
        "        for sc in solo_codes:\n",
        "            for sku in skus[sc]:\n",
        "                qty = max(0, int(np.random.poisson(8) * (1 + 0.2 * np.sin(date.dayofyear / 365 * 2 * np.pi))))\n",
        "                ozon_sales.append({'–î–∞—Ç–∞': date, 'solo-code': sc, 'SKU': sku, '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.': qty})\n",
        "    sample_data['ozon_sales'] = pd.DataFrame(ozon_sales)\n",
        "    \n",
        "    # –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –æ—Ç–≥—Ä—É–∑–∫–∏\n",
        "    shipments = []\n",
        "    for sc in solo_codes:\n",
        "        shipment_dates = dates[::30]  # –ü—Ä–∏–º–µ—Ä–Ω–æ —Ä–∞–∑ –≤ –º–µ—Å—è—Ü\n",
        "        for date in shipment_dates:\n",
        "            shipments.append({\n",
        "                'solo-code': sc,\n",
        "                '–î–∞—Ç–∞': date,\n",
        "                '–ö–æ–ª–∏—á–µ—Å—Ç–≤–∞': int(np.random.uniform(100, 500))\n",
        "            })\n",
        "    sample_data['historical_shipment'] = pd.DataFrame(shipments)\n",
        "    \n",
        "    # –û—Å—Ç–∞—Ç–∫–∏\n",
        "    for key in ['wb_stocks', 'ozon_stocks', 'our_stocks']:\n",
        "        stocks = []\n",
        "        for date in dates[::3]:  # –ö–∞–∂–¥—ã–µ 3 –¥–Ω—è\n",
        "            for sc in solo_codes:\n",
        "                for sku in skus[sc]:\n",
        "                    stock = max(0, int(np.random.uniform(50, 300)))\n",
        "                    row = {'–î–∞—Ç–∞': date, 'solo-code': sc, 'SKU': sku, '–û—Å—Ç–∞—Ç–æ–∫': stock}\n",
        "                    if key in ['wb_stocks', 'ozon_stocks']:\n",
        "                        row['–°–∫–ª–∞–¥'] = 'WH1'\n",
        "                    stocks.append(row)\n",
        "        sample_data[key] = pd.DataFrame(stocks)\n",
        "    \n",
        "    # –°–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏\n",
        "    sample_data['withdraw'] = pd.DataFrame({\n",
        "        'solo-code': ['SC003'],\n",
        "        'SKU': ['SC003_SKU1']\n",
        "    })\n",
        "    \n",
        "    sample_data['defecture'] = pd.DataFrame({\n",
        "        'solo-code': ['SC002'],\n",
        "        'SKU': ['SC002_SKU2'],\n",
        "        '–î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –¥–µ—Ñ–µ–∫—Ç—É—Ä—ã': ['2024-06-15']\n",
        "    })\n",
        "    \n",
        "    sample_data['count_box'] = pd.DataFrame({\n",
        "        'solo-code': solo_codes,\n",
        "        '–ö–æ–ª-–≤–æ': [12, 24, 18]\n",
        "    })\n",
        "    \n",
        "    return sample_data\n",
        "\n",
        "# –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã, —Å–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ\n",
        "if len(data) == 0 or all(v is None for v in data.values()):\n",
        "    print(\"üìù –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...\")\n",
        "    data = create_sample_data()\n",
        "    print(\"‚úÖ –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ–∑–¥–∞–Ω—ã\")\n",
        "\n",
        "# –û–±–Ω–æ–≤–ª—è–µ–º data, –∑–∞–ø–æ–ª–Ω—è—è None –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "for key in FILE_PATHS.keys():\n",
        "    if key not in data or data.get(key) is None:\n",
        "        if 'sample_data' not in locals():\n",
        "            sample_data = create_sample_data()\n",
        "        data[key] = sample_data.get(key, pd.DataFrame())\n",
        "\n",
        "print(\"\\nüìã –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "for key, df in data.items():\n",
        "    if df is not None and len(df) > 0:\n",
        "        print(f\"{key}: {len(df)} —Å—Ç—Ä–æ–∫\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "### 2.1. –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞–ª–µ–Ω–¥–∞—Ä—è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –¥–Ω–µ–≤–Ω–æ–≥–æ –∫–∞–ª–µ–Ω–¥–∞—Ä—è\n",
        "START_DATE = pd.to_datetime('2024-03-26')\n",
        "END_DATE = pd.to_datetime('2025-12-20')\n",
        "FULL_CALENDAR = pd.date_range(START_DATE, END_DATE, freq='D')\n",
        "\n",
        "print(f\"üìÖ –ö–∞–ª–µ–Ω–¥–∞—Ä—å —Å–æ–∑–¥–∞–Ω: {len(FULL_CALENDAR)} –¥–Ω–µ–π\")\n",
        "print(f\"   –ü–µ—Ä–∏–æ–¥: {START_DATE.date()} - {END_DATE.date()}\")\n",
        "\n",
        "# –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ solo-code –∏–∑ –¥–∞–Ω–Ω—ã—Ö\n",
        "all_solo_codes = set()\n",
        "for key, df in data.items():\n",
        "    if df is not None and 'solo-code' in df.columns:\n",
        "        all_solo_codes.update(df['solo-code'].unique())\n",
        "\n",
        "all_solo_codes = sorted(list(all_solo_codes))\n",
        "print(f\"\\nüì¶ –ù–∞–π–¥–µ–Ω–æ solo-code: {len(all_solo_codes)}\")\n",
        "print(f\"   –ü—Ä–∏–º–µ—Ä—ã: {all_solo_codes[:5]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–¥–∞–∂\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_sales_data(sales_df, marketplace_name, date_col='–î–∞—Ç–∞', solo_code_col='solo-code', \n",
        "                       sku_col='SKU', qty_col='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'):\n",
        "    \"\"\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ–¥–∞–∂–∞—Ö: –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –¥–Ω–µ–≤–Ω–æ–π —á–∞—Å—Ç–æ—Ç–µ, –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤\"\"\"\n",
        "    if sales_df is None or len(sales_df) == 0:\n",
        "        return None\n",
        "    \n",
        "    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫\n",
        "    sales_df = sales_df.copy()\n",
        "    sales_df[date_col] = pd.to_datetime(sales_df[date_col])\n",
        "    \n",
        "    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –¥–∞—Ç–µ –∏ solo-code (—Å—É–º–º–∏—Ä—É–µ–º –ø–æ SKU)\n",
        "    sales_agg = sales_df.groupby([date_col, solo_code_col])[qty_col].sum().reset_index()\n",
        "    sales_agg.columns = ['date', 'solo_code', 'sales']\n",
        "    \n",
        "    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∫–∞–ª–µ–Ω–¥–∞—Ä—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ solo-code\n",
        "    calendar_df = pd.DataFrame({\n",
        "        'date': FULL_CALENDAR\n",
        "    })\n",
        "    \n",
        "    result_list = []\n",
        "    for solo_code in all_solo_codes:\n",
        "        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –ø–æ solo-code\n",
        "        sc_data = sales_agg[sales_agg['solo_code'] == solo_code].copy()\n",
        "        \n",
        "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –∫–∞–ª–µ–Ω–¥–∞—Ä–µ–º\n",
        "        sc_calendar = calendar_df.copy()\n",
        "        sc_calendar['solo_code'] = solo_code\n",
        "        sc_calendar = sc_calendar.merge(\n",
        "            sc_data[['date', 'sales']], \n",
        "            on='date', \n",
        "            how='left'\n",
        "        )\n",
        "        sc_calendar['sales'] = sc_calendar['sales'].fillna(0)\n",
        "        \n",
        "        result_list.append(sc_calendar)\n",
        "    \n",
        "    result = pd.concat(result_list, ignore_index=True)\n",
        "    result['marketplace'] = marketplace_name\n",
        "    \n",
        "    return result\n",
        "\n",
        "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–¥–∞–∂ WB –∏ Ozon\n",
        "print(\"üìä –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–¥–∞–∂...\")\n",
        "wb_sales_clean = prepare_sales_data(data.get('wb_sales'), 'WB')\n",
        "ozon_sales_clean = prepare_sales_data(data.get('ozon_sales'), 'Ozon')\n",
        "\n",
        "# –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø—Ä–æ–¥–∞–∂\n",
        "all_sales = pd.concat([wb_sales_clean, ozon_sales_clean], ignore_index=True)\n",
        "\n",
        "# Total sales (—Å—É–º–º–∞ –ø–æ –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–∞–º)\n",
        "total_sales = all_sales.groupby(['date', 'solo_code'])['sales'].sum().reset_index()\n",
        "total_sales['marketplace'] = 'Total'\n",
        "\n",
        "# –§–∏–Ω–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –ø—Ä–æ–¥–∞–∂\n",
        "sales_final = pd.concat([all_sales, total_sales], ignore_index=True)\n",
        "\n",
        "print(f\"‚úÖ –ü—Ä–æ–¥–∞–∂–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã:\")\n",
        "print(f\"   WB: {len(wb_sales_clean)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "print(f\"   Ozon: {len(ozon_sales_clean)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "print(f\"   –í—Å–µ–≥–æ: {len(sales_final)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤\n",
        "print(\"\\nüìà –ü—Ä–∏–º–µ—Ä –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ (–ø–µ—Ä–≤—ã–µ 10 –¥–Ω–µ–π –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ solo-code):\")\n",
        "example_sc = all_solo_codes[0]\n",
        "example_data = sales_final[\n",
        "    (sales_final['solo_code'] == example_sc) & \n",
        "    (sales_final['marketplace'] == 'Total')\n",
        "].head(10)\n",
        "print(example_data[['date', 'sales']].to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_stocks_data(stocks_df, stock_type, date_col='–î–∞—Ç–∞', solo_code_col='solo-code', \n",
        "                        sku_col='SKU', stock_col='–û—Å—Ç–∞—Ç–æ–∫'):\n",
        "    \"\"\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ–± –æ—Å—Ç–∞—Ç–∫–∞—Ö: forward fill, backward fill, –ø—Ä–æ–≤–µ—Ä–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π\"\"\"\n",
        "    if stocks_df is None or len(stocks_df) == 0:\n",
        "        return None\n",
        "    \n",
        "    stocks_df = stocks_df.copy()\n",
        "    stocks_df[date_col] = pd.to_datetime(stocks_df[date_col])\n",
        "    \n",
        "    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –¥–∞—Ç–µ –∏ solo-code (—Å—É–º–º–∏—Ä—É–µ–º –ø–æ SKU –∏ —Å–∫–ª–∞–¥–∞–º)\n",
        "    stocks_agg = stocks_df.groupby([date_col, solo_code_col])[stock_col].sum().reset_index()\n",
        "    stocks_agg.columns = ['date', 'solo_code', 'stock']\n",
        "    \n",
        "    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∫–∞–ª–µ–Ω–¥–∞—Ä—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ solo-code\n",
        "    result_list = []\n",
        "    for solo_code in all_solo_codes:\n",
        "        sc_data = stocks_agg[stocks_agg['solo_code'] == solo_code].copy()\n",
        "        \n",
        "        if len(sc_data) == 0:\n",
        "            continue\n",
        "        \n",
        "        # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Å –∫–∞–ª–µ–Ω–¥–∞—Ä–µ–º\n",
        "        sc_calendar = pd.DataFrame({'date': FULL_CALENDAR})\n",
        "        sc_calendar = sc_calendar.merge(sc_data, on='date', how='left')\n",
        "        \n",
        "        # Forward fill\n",
        "        sc_calendar['stock'] = sc_calendar['stock'].ffill()\n",
        "        \n",
        "        # Backward fill –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä—è–¥–∞\n",
        "        sc_calendar['stock'] = sc_calendar['stock'].bfill()\n",
        "        \n",
        "        # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –µ—Å—Ç—å –ø—Ä–æ–ø—É—Å–∫–∏, –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏\n",
        "        sc_calendar['stock'] = sc_calendar['stock'].fillna(0)\n",
        "        \n",
        "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)\n",
        "        sc_calendar['stock'] = sc_calendar['stock'].clip(lower=0)\n",
        "        \n",
        "        sc_calendar['solo_code'] = solo_code\n",
        "        result_list.append(sc_calendar)\n",
        "    \n",
        "    if not result_list:\n",
        "        return None\n",
        "    \n",
        "    result = pd.concat(result_list, ignore_index=True)\n",
        "    result['stock_type'] = stock_type\n",
        "    \n",
        "    return result\n",
        "\n",
        "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤\n",
        "print(\"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤...\")\n",
        "wb_stocks_clean = prepare_stocks_data(data.get('wb_stocks'), 'WB')\n",
        "ozon_stocks_clean = prepare_stocks_data(data.get('ozon_stocks'), 'Ozon')\n",
        "our_stocks_clean = prepare_stocks_data(data.get('our_stocks'), 'Our')\n",
        "\n",
        "# –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ\n",
        "all_stocks = pd.concat([df for df in [wb_stocks_clean, ozon_stocks_clean, our_stocks_clean] \n",
        "                       if df is not None], ignore_index=True)\n",
        "\n",
        "print(f\"‚úÖ –û—Å—Ç–∞—Ç–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã:\")\n",
        "if wb_stocks_clean is not None:\n",
        "    print(f\"   WB: {len(wb_stocks_clean)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "if ozon_stocks_clean is not None:\n",
        "    print(f\"   Ozon: {len(ozon_stocks_clean)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "if our_stocks_clean is not None:\n",
        "    print(f\"   Our: {len(our_stocks_clean)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "\n",
        "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤\n",
        "if wb_stocks_clean is not None:\n",
        "    print(\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–æ–ø—É—Å–∫–æ–≤ (–¥–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏):\")\n",
        "    for sc in all_solo_codes[:3]:\n",
        "        sc_data = wb_stocks_clean[wb_stocks_clean['solo_code'] == sc]\n",
        "        if len(sc_data) > 0:\n",
        "            original_dates = set(data.get('wb_stocks', pd.DataFrame()).get('–î–∞—Ç–∞', []))\n",
        "            if len(original_dates) > 0:\n",
        "                original_dates = set(pd.to_datetime(list(original_dates)))\n",
        "                total_days = len(FULL_CALENDAR)\n",
        "                available_days = len([d for d in FULL_CALENDAR if d in original_dates])\n",
        "                print(f\"   {sc}: {available_days}/{total_days} –¥–Ω–µ–π ({100*available_days/total_days:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≥—Ä—É–∑–æ–∫\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_shipments_data(shipments_df, date_col='–î–∞—Ç–∞', solo_code_col='solo-code', qty_col='–ö–æ–ª–∏—á–µ—Å—Ç–≤–∞'):\n",
        "    \"\"\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ–± –æ—Ç–≥—Ä—É–∑–∫–∞—Ö: –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ –¥–Ω–µ–≤–Ω–æ–π —á–∞—Å—Ç–æ—Ç–µ\"\"\"\n",
        "    if shipments_df is None or len(shipments_df) == 0:\n",
        "        return None\n",
        "    \n",
        "    shipments_df = shipments_df.copy()\n",
        "    shipments_df[date_col] = pd.to_datetime(shipments_df[date_col])\n",
        "    \n",
        "    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –¥–∞—Ç–µ –∏ solo-code\n",
        "    shipments_agg = shipments_df.groupby([date_col, solo_code_col])[qty_col].sum().reset_index()\n",
        "    shipments_agg.columns = ['date', 'solo_code', 'shipment']\n",
        "    \n",
        "    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∫–∞–ª–µ–Ω–¥–∞—Ä—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ solo-code\n",
        "    result_list = []\n",
        "    for solo_code in all_solo_codes:\n",
        "        sc_data = shipments_agg[shipments_agg['solo_code'] == solo_code].copy()\n",
        "        \n",
        "        sc_calendar = pd.DataFrame({'date': FULL_CALENDAR})\n",
        "        sc_calendar = sc_calendar.merge(sc_data, on='date', how='left')\n",
        "        sc_calendar['shipment'] = sc_calendar['shipment'].fillna(0)\n",
        "        sc_calendar['solo_code'] = solo_code\n",
        "        \n",
        "        result_list.append(sc_calendar)\n",
        "    \n",
        "    result = pd.concat(result_list, ignore_index=True)\n",
        "    return result\n",
        "\n",
        "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≥—Ä—É–∑–æ–∫\n",
        "print(\"üöö –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≥—Ä—É–∑–æ–∫...\")\n",
        "shipments_clean = prepare_shipments_data(data.get('historical_shipment'))\n",
        "\n",
        "if shipments_clean is not None:\n",
        "    print(f\"‚úÖ –û—Ç–≥—Ä—É–∑–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã: {len(shipments_clean)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "    \n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: –æ—Ç–≥—Ä—É–∑–∫–∏ vs –ø—Ä–æ–¥–∞–∂–∏\n",
        "    if len(all_solo_codes) > 0:\n",
        "        example_sc = all_solo_codes[0]\n",
        "        sc_shipments = shipments_clean[shipments_clean['solo_code'] == example_sc]\n",
        "        sc_sales = total_sales[total_sales['solo_code'] == example_sc]\n",
        "        \n",
        "        fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "        \n",
        "        # –û—Ç–≥—Ä—É–∑–∫–∏ vs –ø—Ä–æ–¥–∞–∂–∏\n",
        "        ax1 = axes[0]\n",
        "        ax1.plot(sc_shipments['date'], sc_shipments['shipment'], \n",
        "                label='–û—Ç–≥—Ä—É–∑–∫–∏', marker='o', markersize=4, alpha=0.7)\n",
        "        ax1.plot(sc_sales['date'], sc_sales['sales'], \n",
        "                label='–ü—Ä–æ–¥–∞–∂–∏ (Total)', alpha=0.7)\n",
        "        ax1.set_title(f'–û—Ç–≥—Ä—É–∑–∫–∏ vs –ü—Ä–æ–¥–∞–∂–∏: {example_sc}')\n",
        "        ax1.set_xlabel('–î–∞—Ç–∞')\n",
        "        ax1.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # –ò–Ω—Ç–µ—Ä–≤–∞–ª—ã –º–µ–∂–¥—É –æ—Ç–≥—Ä—É–∑–∫–∞–º–∏\n",
        "        ax2 = axes[1]\n",
        "        shipment_dates = sc_shipments[sc_shipments['shipment'] > 0]['date'].values\n",
        "        if len(shipment_dates) > 1:\n",
        "            intervals = np.diff(shipment_dates).astype('timedelta64[D]').astype(int)\n",
        "            ax2.bar(range(len(intervals)), intervals, alpha=0.7)\n",
        "            ax2.set_title(f'–ò–Ω—Ç–µ—Ä–≤–∞–ª—ã –º–µ–∂–¥—É –æ—Ç–≥—Ä—É–∑–∫–∞–º–∏: {example_sc}')\n",
        "            ax2.set_xlabel('–ù–æ–º–µ—Ä –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞')\n",
        "            ax2.set_ylabel('–î–Ω–µ–π')\n",
        "            ax2.grid(True, alpha=0.3, axis='y')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(f\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤ –º–µ–∂–¥—É –æ—Ç–≥—Ä—É–∑–∫–∞–º–∏ ({example_sc}):\")\n",
        "        if len(shipment_dates) > 1:\n",
        "            print(f\"   –°—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª: {np.mean(intervals):.1f} –¥–Ω–µ–π\")\n",
        "            print(f\"   –ú–µ–¥–∏–∞–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª: {np.median(intervals):.1f} –¥–Ω–µ–π\")\n",
        "            print(f\"   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π: {np.min(intervals)} –¥–Ω–µ–π\")\n",
        "            print(f\"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π: {np.max(intervals)} –¥–Ω–µ–π\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è –î–∞–Ω–Ω—ã–µ –æ–± –æ—Ç–≥—Ä—É–∑–∫–∞—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. EDA –∏ –∞–Ω–∞–ª–∏–∑ –ø–æ–∫—Ä—ã—Ç–∏—è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –†–∞—Å—á–µ—Ç –¥–Ω–µ–π –ø–æ–∫—Ä—ã—Ç–∏—è (Days Cover)\n",
        "def calculate_days_cover(sales_df, stocks_wb, stocks_ozon):\n",
        "    \"\"\"–†–∞—Å—á–µ—Ç –¥–Ω–µ–π –ø–æ–∫—Ä—ã—Ç–∏—è –∑–∞–ø–∞—Å–∞–º–∏\"\"\"\n",
        "    result_list = []\n",
        "    \n",
        "    for solo_code in all_solo_codes:\n",
        "        # –ü—Ä–æ–¥–∞–∂–∏\n",
        "        sc_sales = sales_df[\n",
        "            (sales_df['solo_code'] == solo_code) & \n",
        "            (sales_df['marketplace'] == 'Total')\n",
        "        ].copy()\n",
        "        \n",
        "        if len(sc_sales) == 0:\n",
        "            continue\n",
        "        \n",
        "        # –°—Ä–µ–¥–Ω–∏–π –¥–Ω–µ–≤–Ω–æ–π —Å–ø—Ä–æ—Å\n",
        "        avg_daily_sales = sc_sales['sales'].mean()\n",
        "        if avg_daily_sales == 0:\n",
        "            continue\n",
        "        \n",
        "        # –û—Å—Ç–∞—Ç–∫–∏ WB\n",
        "        sc_stocks_wb = None\n",
        "        if stocks_wb is not None:\n",
        "            sc_stocks_wb = stocks_wb[stocks_wb['solo_code'] == solo_code].copy()\n",
        "        \n",
        "        # –û—Å—Ç–∞—Ç–∫–∏ Ozon\n",
        "        sc_stocks_ozon = None\n",
        "        if stocks_ozon is not None:\n",
        "            sc_stocks_ozon = stocks_ozon[stocks_ozon['solo_code'] == solo_code].copy()\n",
        "        \n",
        "        # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ –¥–∞—Ç–∞–º\n",
        "        cover_df = sc_sales[['date', 'sales']].copy()\n",
        "        \n",
        "        if sc_stocks_wb is not None:\n",
        "            cover_df = cover_df.merge(\n",
        "                sc_stocks_wb[['date', 'stock']], \n",
        "                on='date', \n",
        "                how='left', \n",
        "                suffixes=('', '_wb')\n",
        "            )\n",
        "            cover_df['stock_wb'] = cover_df['stock'].fillna(0)\n",
        "            cover_df = cover_df.drop('stock', axis=1)\n",
        "        else:\n",
        "            cover_df['stock_wb'] = 0\n",
        "        \n",
        "        if sc_stocks_ozon is not None:\n",
        "            cover_df = cover_df.merge(\n",
        "                sc_stocks_ozon[['date', 'stock']], \n",
        "                on='date', \n",
        "                how='left', \n",
        "                suffixes=('', '_ozon')\n",
        "            )\n",
        "            cover_df['stock_ozon'] = cover_df['stock'].fillna(0)\n",
        "            cover_df = cover_df.drop('stock', axis=1)\n",
        "        else:\n",
        "            cover_df['stock_ozon'] = 0\n",
        "        \n",
        "        # Days Cover\n",
        "        cover_df['total_stock'] = cover_df['stock_wb'] + cover_df['stock_ozon']\n",
        "        cover_df['days_cover'] = cover_df['total_stock'] / avg_daily_sales\n",
        "        cover_df['days_cover'] = cover_df['days_cover'].replace([np.inf, -np.inf], 0)\n",
        "        cover_df['solo_code'] = solo_code\n",
        "        \n",
        "        # OOS –ø–µ—Ä–∏–æ–¥—ã (out of stock)\n",
        "        cover_df['is_oos'] = (cover_df['total_stock'] == 0) & (cover_df['sales'] > 0)\n",
        "        \n",
        "        result_list.append(cover_df)\n",
        "    \n",
        "    if not result_list:\n",
        "        return None\n",
        "    \n",
        "    return pd.concat(result_list, ignore_index=True)\n",
        "\n",
        "# –†–∞—Å—á–µ—Ç –ø–æ–∫—Ä—ã—Ç–∏—è\n",
        "print(\"üìä –†–∞—Å—á–µ—Ç –¥–Ω–µ–π –ø–æ–∫—Ä—ã—Ç–∏—è...\")\n",
        "days_cover_df = calculate_days_cover(sales_final, wb_stocks_clean, ozon_stocks_clean)\n",
        "\n",
        "if days_cover_df is not None:\n",
        "    print(f\"‚úÖ –†–∞—Å—á–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {len(days_cover_df)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "    \n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞\n",
        "    if len(all_solo_codes) > 0:\n",
        "        example_sc = all_solo_codes[0]\n",
        "        sc_cover = days_cover_df[days_cover_df['solo_code'] == example_sc]\n",
        "        \n",
        "        fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
        "        \n",
        "        # –ü—Ä–æ–¥–∞–∂–∏ vs –æ—Ç–≥—Ä—É–∑–∫–∏\n",
        "        ax1 = axes[0]\n",
        "        ax1.plot(sc_cover['date'], sc_cover['sales'], label='–ü—Ä–æ–¥–∞–∂–∏', alpha=0.7)\n",
        "        if shipments_clean is not None:\n",
        "            sc_ship = shipments_clean[shipments_clean['solo_code'] == example_sc]\n",
        "            ax1.bar(sc_ship['date'], sc_ship['shipment'], \n",
        "                   label='–û—Ç–≥—Ä—É–∑–∫–∏', alpha=0.5, width=1)\n",
        "        ax1.set_title(f'–ü—Ä–æ–¥–∞–∂–∏ vs –û—Ç–≥—Ä—É–∑–∫–∏: {example_sc}')\n",
        "        ax1.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        \n",
        "        # –î–∏–Ω–∞–º–∏–∫–∞ –ø–æ–∫—Ä—ã—Ç–∏—è\n",
        "        ax2 = axes[1]\n",
        "        ax2.plot(sc_cover['date'], sc_cover['days_cover'], \n",
        "                label='Days Cover', color='green', alpha=0.7)\n",
        "        ax2.axhline(y=30, color='r', linestyle='--', label='–ú–∏–Ω–∏–º—É–º 30 –¥–Ω–µ–π', alpha=0.5)\n",
        "        ax2.set_title(f'–î–∏–Ω–∞–º–∏–∫–∞ –ø–æ–∫—Ä—ã—Ç–∏—è: {example_sc}')\n",
        "        ax2.set_ylabel('–î–Ω–µ–π –ø–æ–∫—Ä—ã—Ç–∏—è')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True, alpha=0.3)\n",
        "        \n",
        "        # OOS –ø–µ—Ä–∏–æ–¥—ã\n",
        "        ax3 = axes[2]\n",
        "        oos_periods = sc_cover[sc_cover['is_oos']]\n",
        "        if len(oos_periods) > 0:\n",
        "            ax3.scatter(oos_periods['date'], oos_periods['sales'], \n",
        "                       color='red', marker='x', s=100, label='OOS')\n",
        "        ax3.plot(sc_cover['date'], sc_cover['sales'], \n",
        "                label='–ü—Ä–æ–¥–∞–∂–∏', alpha=0.5)\n",
        "        ax3.set_title(f'OOS –ø–µ—Ä–∏–æ–¥—ã: {example_sc}')\n",
        "        ax3.set_xlabel('–î–∞—Ç–∞')\n",
        "        ax3.set_ylabel('–ü—Ä–æ–¥–∞–∂–∏')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∫—Ä—ã—Ç–∏—è\n",
        "        print(f\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–∫—Ä—ã—Ç–∏—è ({example_sc}):\")\n",
        "        print(f\"   –°—Ä–µ–¥–Ω–∏–π Days Cover: {sc_cover['days_cover'].mean():.1f} –¥–Ω–µ–π\")\n",
        "        print(f\"   –ú–µ–¥–∏–∞–Ω–Ω—ã–π Days Cover: {sc_cover['days_cover'].median():.1f} –¥–Ω–µ–π\")\n",
        "        print(f\"   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π: {sc_cover['days_cover'].min():.1f} –¥–Ω–µ–π\")\n",
        "        print(f\"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π: {sc_cover['days_cover'].max():.1f} –¥–Ω–µ–π\")\n",
        "        print(f\"   OOS –¥–Ω–µ–π: {sc_cover['is_oos'].sum()} ({100*sc_cover['is_oos'].sum()/len(sc_cover):.1f}%)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –ø–æ–∫—Ä—ã—Ç–∏–µ\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–ø—Ä–æ—Å–∞\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_demand(sales_series):\n",
        "    \"\"\"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–ø—Ä–æ—Å–∞: stable, seasonal, intermittent\"\"\"\n",
        "    if len(sales_series) == 0:\n",
        "        return 'intermittent'\n",
        "    \n",
        "    sales_array = np.array(sales_series)\n",
        "    \n",
        "    # –ü—Ä–æ—Ü–µ–Ω—Ç –Ω—É–ª–µ–≤—ã—Ö –¥–Ω–µ–π\n",
        "    zero_pct = np.sum(sales_array == 0) / len(sales_array)\n",
        "    \n",
        "    # –°—Ä–µ–¥–Ω–∏–π —Å–ø—Ä–æ—Å (—Ç–æ–ª—å–∫–æ –Ω–µ–Ω—É–ª–µ–≤—ã–µ –¥–Ω–∏)\n",
        "    non_zero = sales_array[sales_array > 0]\n",
        "    if len(non_zero) == 0:\n",
        "        return 'intermittent'\n",
        "    \n",
        "    avg_demand = np.mean(non_zero)\n",
        "    \n",
        "    # CV (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏)\n",
        "    cv = np.std(non_zero) / avg_demand if avg_demand > 0 else np.inf\n",
        "    \n",
        "    # –°—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –ø—Ä–æ–¥–∞–∂–∞–º–∏\n",
        "    sale_days = np.where(sales_array > 0)[0]\n",
        "    if len(sale_days) > 1:\n",
        "        avg_interval = np.mean(np.diff(sale_days))\n",
        "    else:\n",
        "        avg_interval = len(sales_array)\n",
        "    \n",
        "    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n",
        "    if zero_pct > 0.5:  # –ë–æ–ª–µ–µ 50% –Ω—É–ª–µ–≤—ã—Ö –¥–Ω–µ–π\n",
        "        return 'intermittent'\n",
        "    elif cv > 1.5:  # –í—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞–±–µ–ª—å–Ω–æ—Å—Ç—å\n",
        "        return 'seasonal'\n",
        "    else:\n",
        "        return 'stable'\n",
        "\n",
        "# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–ø—Ä–æ—Å–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ solo-code\n",
        "print(\"üîç –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–ø—Ä–æ—Å–∞...\")\n",
        "demand_classification = []\n",
        "\n",
        "for solo_code in all_solo_codes:\n",
        "    sc_sales = total_sales[total_sales['solo_code'] == solo_code]['sales'].values\n",
        "    \n",
        "    if len(sc_sales) == 0:\n",
        "        continue\n",
        "    \n",
        "    # –ú–µ—Ç—Ä–∏–∫–∏\n",
        "    zero_pct = np.sum(sc_sales == 0) / len(sc_sales)\n",
        "    non_zero = sc_sales[sc_sales > 0]\n",
        "    avg_demand = np.mean(non_zero) if len(non_zero) > 0 else 0\n",
        "    cv = np.std(non_zero) / avg_demand if avg_demand > 0 else np.inf\n",
        "    \n",
        "    sale_days = np.where(sc_sales > 0)[0]\n",
        "    if len(sale_days) > 1:\n",
        "        avg_interval = np.mean(np.diff(sale_days))\n",
        "    else:\n",
        "        avg_interval = len(sc_sales)\n",
        "    \n",
        "    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è\n",
        "    demand_class = classify_demand(sc_sales)\n",
        "    \n",
        "    demand_classification.append({\n",
        "        'solo_code': solo_code,\n",
        "        'zero_pct': zero_pct,\n",
        "        'avg_demand': avg_demand,\n",
        "        'cv': cv,\n",
        "        'avg_interval': avg_interval,\n",
        "        'demand_class': demand_class\n",
        "    })\n",
        "\n",
        "demand_df = pd.DataFrame(demand_classification)\n",
        "\n",
        "print(f\"‚úÖ –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è {len(demand_df)} solo-code\")\n",
        "print(\"\\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:\")\n",
        "print(demand_df['demand_class'].value_counts())\n",
        "\n",
        "print(\"\\nüìã –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
        "print(demand_df[['solo_code', 'zero_pct', 'avg_demand', 'cv', 'avg_interval', 'demand_class']].to_string(index=False))\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤\n",
        "ax1 = axes[0]\n",
        "demand_df['demand_class'].value_counts().plot(kind='bar', ax=ax1, color=['green', 'orange', 'red'])\n",
        "ax1.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ —Å–ø—Ä–æ—Å–∞')\n",
        "ax1.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\n",
        "ax1.set_xlabel('–ö–ª–∞—Å—Å')\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# CV vs Zero %\n",
        "ax2 = axes[1]\n",
        "for cls in demand_df['demand_class'].unique():\n",
        "    cls_data = demand_df[demand_df['demand_class'] == cls]\n",
        "    ax2.scatter(cls_data['zero_pct'], cls_data['cv'], label=cls, alpha=0.7, s=100)\n",
        "ax2.set_title('CV vs –ü—Ä–æ—Ü–µ–Ω—Ç –Ω—É–ª–µ–≤—ã—Ö –¥–Ω–µ–π')\n",
        "ax2.set_xlabel('–ü—Ä–æ—Ü–µ–Ω—Ç –Ω—É–ª–µ–≤—ã—Ö –¥–Ω–µ–π')\n",
        "ax2.set_ylabel('CV (–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏)')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. –ö–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_calendar_features(dates):\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\"\"\"\n",
        "    features = pd.DataFrame({'date': dates})\n",
        "    \n",
        "    # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "    features['day_of_week'] = features['date'].dt.dayofweek\n",
        "    features['is_weekend'] = (features['day_of_week'] >= 5).astype(int)\n",
        "    features['week_of_year'] = features['date'].dt.isocalendar().week\n",
        "    features['month'] = features['date'].dt.month\n",
        "    features['quarter'] = features['date'].dt.quarter\n",
        "    features['year'] = features['date'].dt.year\n",
        "    \n",
        "    # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (sin/cos)\n",
        "    features['week_sin'] = np.sin(2 * np.pi * features['week_of_year'] / 52)\n",
        "    features['week_cos'] = np.cos(2 * np.pi * features['week_of_year'] / 52)\n",
        "    features['month_sin'] = np.sin(2 * np.pi * features['month'] / 12)\n",
        "    features['month_cos'] = np.cos(2 * np.pi * features['month'] / 12)\n",
        "    \n",
        "    # –ü—Ä–∞–∑–¥–Ω–∏–∫–∏ –†–§ —Å –æ–∫–Ω–∞–º–∏\n",
        "    features['is_new_year_period'] = 0\n",
        "    features['is_feb_14_window'] = 0\n",
        "    features['is_feb_23_window'] = 0\n",
        "    features['is_mar_8_window'] = 0\n",
        "    features['is_black_friday_period'] = 0\n",
        "    \n",
        "    for idx, date in enumerate(features['date']):\n",
        "        year = date.year\n",
        "        month = date.month\n",
        "        day = date.day\n",
        "        day_of_year = date.timetuple().tm_yday\n",
        "        \n",
        "        # –ù–æ–≤—ã–π –≥–æ–¥: 15.12 - 10.01\n",
        "        if (month == 12 and day >= 15) or (month == 1 and day <= 10):\n",
        "            features.loc[idx, 'is_new_year_period'] = 1\n",
        "        \n",
        "        # 14 —Ñ–µ–≤—Ä–∞–ª—è ¬±7 –¥–Ω–µ–π\n",
        "        if month == 2 and abs(day - 14) <= 7:\n",
        "            features.loc[idx, 'is_feb_14_window'] = 1\n",
        "        \n",
        "        # 23 —Ñ–µ–≤—Ä–∞–ª—è ¬±7 –¥–Ω–µ–π\n",
        "        if month == 2 and abs(day - 23) <= 7:\n",
        "            features.loc[idx, 'is_feb_23_window'] = 1\n",
        "        \n",
        "        # 8 –º–∞—Ä—Ç–∞ ¬±7 –¥–Ω–µ–π\n",
        "        if month == 3 and abs(day - 8) <= 7:\n",
        "            features.loc[idx, 'is_mar_8_window'] = 1\n",
        "        \n",
        "        # –ß–µ—Ä–Ω–∞—è –ø—è—Ç–Ω–∏—Ü–∞: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –Ω–µ–¥–µ–ª–∏ –Ω–æ—è–±—Ä—è\n",
        "        if month == 11 and day >= 15:\n",
        "            features.loc[idx, 'is_black_friday_period'] = 1\n",
        "    \n",
        "    return features\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "print(\"üìÖ –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...\")\n",
        "calendar_features = create_calendar_features(FULL_CALENDAR)\n",
        "\n",
        "print(f\"‚úÖ –ö–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ–∑–¥–∞–Ω—ã: {len(calendar_features.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "print(f\"   –ö–æ–ª–æ–Ω–∫–∏: {list(calendar_features.columns)}\")\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–ª–∏—è–Ω–∏—è –ø—Ä–∞–∑–¥–Ω–∏–∫–æ–≤ –Ω–∞ –ø—Ä–æ–¥–∞–∂–∏\n",
        "if len(all_solo_codes) > 0:\n",
        "    example_sc = all_solo_codes[0]\n",
        "    sc_sales = total_sales[total_sales['solo_code'] == example_sc].copy()\n",
        "    sc_sales = sc_sales.merge(calendar_features, on='date', how='left')\n",
        "    \n",
        "    print(f\"\\nüìä –í–ª–∏—è–Ω–∏–µ –ø—Ä–∞–∑–¥–Ω–∏–∫–æ–≤ –Ω–∞ –ø—Ä–æ–¥–∞–∂–∏ ({example_sc}):\")\n",
        "    for holiday in ['is_new_year_period', 'is_feb_14_window', 'is_feb_23_window', \n",
        "                    'is_mar_8_window', 'is_black_friday_period']:\n",
        "        holiday_sales = sc_sales[sc_sales[holiday] == 1]['sales'].mean()\n",
        "        normal_sales = sc_sales[sc_sales[holiday] == 0]['sales'].mean()\n",
        "        if normal_sales > 0:\n",
        "            impact = (holiday_sales / normal_sales - 1) * 100\n",
        "            print(f\"   {holiday}: {holiday_sales:.1f} vs {normal_sales:.1f} ({impact:+.1f}%)\")\n",
        "    \n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "    \n",
        "    # –ü—Ä–æ–¥–∞–∂–∏ —Å –≤—ã–¥–µ–ª–µ–Ω–∏–µ–º –ø—Ä–∞–∑–¥–Ω–∏–∫–æ–≤\n",
        "    ax1 = axes[0]\n",
        "    ax1.plot(sc_sales['date'], sc_sales['sales'], alpha=0.5, label='–ü—Ä–æ–¥–∞–∂–∏')\n",
        "    \n",
        "    for holiday, color in [('is_new_year_period', 'red'), \n",
        "                          ('is_black_friday_period', 'orange'),\n",
        "                          ('is_mar_8_window', 'pink')]:\n",
        "        holiday_dates = sc_sales[sc_sales[holiday] == 1]['date']\n",
        "        if len(holiday_dates) > 0:\n",
        "            ax1.scatter(holiday_dates, sc_sales[sc_sales[holiday] == 1]['sales'],\n",
        "                       label=holiday, alpha=0.7, s=50, color=color)\n",
        "    \n",
        "    ax1.set_title(f'–ü—Ä–æ–¥–∞–∂–∏ —Å –≤—ã–¥–µ–ª–µ–Ω–∏–µ–º –ø—Ä–∞–∑–¥–Ω–∏–∫–æ–≤: {example_sc}')\n",
        "    ax1.set_ylabel('–ü—Ä–æ–¥–∞–∂–∏')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # –°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ–¥–∞–∂–∏ –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏\n",
        "    ax2 = axes[1]\n",
        "    weekday_sales = sc_sales.groupby('day_of_week')['sales'].mean()\n",
        "    weekday_names = ['–ü–Ω', '–í—Ç', '–°—Ä', '–ß—Ç', '–ü—Ç', '–°–±', '–í—Å']\n",
        "    ax2.bar(range(7), weekday_sales.values, alpha=0.7)\n",
        "    ax2.set_xticks(range(7))\n",
        "    ax2.set_xticklabels(weekday_names)\n",
        "    ax2.set_title(f'–°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ–¥–∞–∂–∏ –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏: {example_sc}')\n",
        "    ax2.set_ylabel('–°—Ä–µ–¥–Ω–∏–µ –ø—Ä–æ–¥–∞–∂–∏')\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. –ë–∏–∑–Ω–µ—Å-–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è\n",
        "\n",
        "### 6.1. Withdraw (–≤—ã–≤–æ–¥ –∏–∑ –ø—Ä–æ–¥–∞–∂–∏)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ withdraw\n",
        "withdraw_df = data.get('withdraw', pd.DataFrame())\n",
        "if len(withdraw_df) > 0:\n",
        "    withdraw_df.columns = [col.lower().replace('-', '_') for col in withdraw_df.columns]\n",
        "    if 'solo_code' not in withdraw_df.columns and 'solo-code' in withdraw_df.columns:\n",
        "        withdraw_df['solo_code'] = withdraw_df['solo-code']\n",
        "    \n",
        "    print(f\"‚ö†Ô∏è Withdraw: {len(withdraw_df)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "    print(f\"   Solo-codes: {withdraw_df['solo_code'].unique().tolist()}\")\n",
        "else:\n",
        "    print(\"‚úÖ Withdraw: –Ω–µ—Ç –∑–∞–ø–∏—Å–µ–π\")\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ withdraw\n",
        "def apply_withdraw_constraint(sales_series, solo_code, withdraw_df):\n",
        "    \"\"\"–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è withdraw: –Ω–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤—ã–π —Å–ø—Ä–æ—Å, —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–∏—Ç—å –æ—Å—Ç–∞—Ç–∫–∏\"\"\"\n",
        "    if len(withdraw_df) == 0:\n",
        "        return sales_series\n",
        "    \n",
        "    if solo_code in withdraw_df['solo_code'].values:\n",
        "        # –î–ª—è withdraw: –ø—Ä–æ–≥–Ω–æ–∑ = 0, –æ—Å—Ç–∞—Ç–∫–∏ –±—É–¥—É—Ç —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω—ã —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ –¥–æ –Ω—É–ª—è\n",
        "        return np.zeros_like(sales_series)\n",
        "    \n",
        "    return sales_series\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2. Defecture (–¥–µ—Ñ–µ–∫—Ç—É—Ä–∞)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ defecture\n",
        "defecture_df = data.get('defecture', pd.DataFrame())\n",
        "if len(defecture_df) > 0:\n",
        "    defecture_df.columns = [col.lower().replace('-', '_') for col in defecture_df.columns]\n",
        "    if 'solo_code' not in defecture_df.columns and 'solo-code' in defecture_df.columns:\n",
        "        defecture_df['solo_code'] = defecture_df['solo-code']\n",
        "    \n",
        "    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã –æ–∫–æ–Ω—á–∞–Ω–∏—è –¥–µ—Ñ–µ–∫—Ç—É—Ä—ã\n",
        "    date_col = [col for col in defecture_df.columns if '–¥–∞—Ç–∞' in col.lower() or 'date' in col.lower()]\n",
        "    if len(date_col) > 0:\n",
        "        defecture_df['defecture_end_date'] = pd.to_datetime(defecture_df[date_col[0]])\n",
        "    \n",
        "    print(f\"‚ö†Ô∏è Defecture: {len(defecture_df)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "    print(defecture_df.to_string(index=False))\n",
        "else:\n",
        "    print(\"‚úÖ Defecture: –Ω–µ—Ç –∑–∞–ø–∏—Å–µ–π\")\n",
        "\n",
        "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ defecture\n",
        "def apply_defecture_constraint(sales_series, dates, solo_code, defecture_df):\n",
        "    \"\"\"–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è defecture: –ø—Ä–æ–¥–∞–∂–∏ = 0 –≤ –ø–µ—Ä–∏–æ–¥ –¥–µ—Ñ–µ–∫—Ç—É—Ä—ã\"\"\"\n",
        "    if len(defecture_df) == 0:\n",
        "        return sales_series\n",
        "    \n",
        "    defecture_records = defecture_df[defecture_df['solo_code'] == solo_code]\n",
        "    if len(defecture_records) == 0:\n",
        "        return sales_series\n",
        "    \n",
        "    result = sales_series.copy()\n",
        "    \n",
        "    for _, record in defecture_records.iterrows():\n",
        "        if 'defecture_end_date' in record:\n",
        "            end_date = pd.to_datetime(record['defecture_end_date'])\n",
        "            # –ü—Ä–æ–¥–∞–∂–∏ = 0 –¥–æ –¥–∞—Ç—ã –æ–∫–æ–Ω—á–∞–Ω–∏—è –¥–µ—Ñ–µ–∫—Ç—É—Ä—ã\n",
        "            mask = dates <= end_date\n",
        "            result[mask] = 0\n",
        "    \n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. –ú–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–¥–∞–∂\n",
        "\n",
        "### 7.1. Baseline –º–æ–¥–µ–ª–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BaselineModels:\n",
        "    \"\"\"–ë–∞–∑–æ–≤—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def naive(series, horizon=1):\n",
        "        \"\"\"Naive: –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\"\"\"\n",
        "        if len(series) == 0:\n",
        "            return np.zeros(horizon)\n",
        "        return np.full(horizon, series[-1])\n",
        "    \n",
        "    @staticmethod\n",
        "    def moving_average(series, window=7, horizon=1):\n",
        "        \"\"\"Moving Average\"\"\"\n",
        "        if len(series) < window:\n",
        "            return BaselineModels.naive(series, horizon)\n",
        "        return np.full(horizon, np.mean(series[-window:]))\n",
        "    \n",
        "    @staticmethod\n",
        "    def seasonal_naive(series, season_length=7, horizon=1):\n",
        "        \"\"\"Seasonal Naive: –∑–Ω–∞—á–µ–Ω–∏–µ —Å–µ–∑–æ–Ω–∞ –Ω–∞–∑–∞–¥\"\"\"\n",
        "        if len(series) < season_length:\n",
        "            return BaselineModels.naive(series, horizon)\n",
        "        return np.tile(series[-season_length:], (horizon // season_length + 1))[:horizon]\n",
        "    \n",
        "    @staticmethod\n",
        "    def croston(series, horizon=1, alpha=0.1):\n",
        "        \"\"\"Croston –¥–ª—è intermittent demand\"\"\"\n",
        "        if len(series) == 0:\n",
        "            return np.zeros(horizon)\n",
        "        \n",
        "        non_zero = series[series > 0]\n",
        "        if len(non_zero) == 0:\n",
        "            return np.zeros(horizon)\n",
        "        \n",
        "        # –ü—Ä–æ—Å—Ç–æ–µ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –¥–ª—è –Ω–µ–Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "        if len(non_zero) == 1:\n",
        "            forecast = non_zero[0]\n",
        "        else:\n",
        "            forecast = non_zero[0]\n",
        "            for val in non_zero[1:]:\n",
        "                forecast = alpha * val + (1 - alpha) * forecast\n",
        "        \n",
        "        # –£—á–µ—Ç —á–∞—Å—Ç–æ—Ç—ã –ø—Ä–æ–¥–∞–∂\n",
        "        intervals = []\n",
        "        last_idx = -1\n",
        "        for i, val in enumerate(series):\n",
        "            if val > 0:\n",
        "                if last_idx >= 0:\n",
        "                    intervals.append(i - last_idx)\n",
        "                last_idx = i\n",
        "        \n",
        "        avg_interval = np.mean(intervals) if len(intervals) > 0 else len(series)\n",
        "        daily_rate = forecast / avg_interval if avg_interval > 0 else 0\n",
        "        \n",
        "        return np.full(horizon, daily_rate)\n",
        "    \n",
        "    @staticmethod\n",
        "    def sba(series, horizon=1, alpha=0.1):\n",
        "        \"\"\"SBA (Syntetos-Boylan Approximation)\"\"\"\n",
        "        croston_forecast = BaselineModels.croston(series, horizon, alpha)\n",
        "        return croston_forecast * 0.5  # SBA –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞\n",
        "    \n",
        "    @staticmethod\n",
        "    def tsb(series, horizon=1, alpha=0.1, beta=0.1):\n",
        "        \"\"\"TSB (Teunter-Syntetos-Babai)\"\"\"\n",
        "        if len(series) == 0:\n",
        "            return np.zeros(horizon)\n",
        "        \n",
        "        # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–æ–¥–∞–∂–∏\n",
        "        prob_sale = np.sum(series > 0) / len(series) if len(series) > 0 else 0\n",
        "        \n",
        "        # –†–∞–∑–º–µ—Ä –ø—Ä–æ–¥–∞–∂–∏ (–µ—Å–ª–∏ –±—ã–ª–∞)\n",
        "        non_zero = series[series > 0]\n",
        "        if len(non_zero) == 0:\n",
        "            return np.zeros(horizon)\n",
        "        \n",
        "        if len(non_zero) == 1:\n",
        "            size_forecast = non_zero[0]\n",
        "        else:\n",
        "            size_forecast = non_zero[0]\n",
        "            for val in non_zero[1:]:\n",
        "                size_forecast = beta * val + (1 - beta) * size_forecast\n",
        "        \n",
        "        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏\n",
        "        if len(series) > 0:\n",
        "            last_was_sale = 1 if series[-1] > 0 else 0\n",
        "            prob_sale = alpha * last_was_sale + (1 - alpha) * prob_sale\n",
        "        \n",
        "        forecast = prob_sale * size_forecast\n",
        "        return np.full(horizon, forecast)\n",
        "\n",
        "print(\"‚úÖ Baseline –º–æ–¥–µ–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2. –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdvancedModels:\n",
        "    \"\"\"–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def linear_regression(train_data, train_features, test_features, horizon=1):\n",
        "        \"\"\"Linear Regression —Å –ª–∞–≥–∞–º–∏ –∏ –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\"\"\"\n",
        "        try:\n",
        "            if len(train_data) < 10:\n",
        "                return np.full(horizon, np.mean(train_data) if len(train_data) > 0 else 0)\n",
        "            \n",
        "            # –°–æ–∑–¥–∞–Ω–∏–µ –ª–∞–≥–æ–≤\n",
        "            max_lag = min(7, len(train_data) - 1)\n",
        "            X_train = []\n",
        "            y_train = train_data[max_lag:]\n",
        "            \n",
        "            for i in range(max_lag, len(train_data)):\n",
        "                features = list(train_features.iloc[i, 1:].values)  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º date\n",
        "                lags = list(train_data[i-max_lag:i])\n",
        "                X_train.append(features + lags)\n",
        "            \n",
        "            X_train = np.array(X_train)\n",
        "            \n",
        "            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "            X_test = []\n",
        "            last_features = list(test_features.iloc[0, 1:].values)\n",
        "            last_values = list(train_data[-max_lag:])\n",
        "            \n",
        "            for h in range(horizon):\n",
        "                X_test.append(last_features + last_values)\n",
        "                # –û–±–Ω–æ–≤–ª—è–µ–º –ª–∞–≥–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–≥–Ω–æ–∑)\n",
        "                if h < horizon - 1:\n",
        "                    last_values = last_values[1:] + [0]  # –ó–∞–≥–ª—É—à–∫–∞\n",
        "            \n",
        "            X_test = np.array(X_test)\n",
        "            \n",
        "            # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "            model = LinearRegression()\n",
        "            model.fit(X_train, y_train)\n",
        "            \n",
        "            # –ü—Ä–æ–≥–Ω–æ–∑\n",
        "            forecast = model.predict(X_test)\n",
        "            forecast = np.maximum(forecast, 0)  # –ù–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "            \n",
        "            return forecast\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ Linear Regression: {e}\")\n",
        "            return np.full(horizon, np.mean(train_data) if len(train_data) > 0 else 0)\n",
        "    \n",
        "    @staticmethod\n",
        "    def arima(train_data, horizon=1, order=(1, 1, 1)):\n",
        "        \"\"\"ARIMA\"\"\"\n",
        "        try:\n",
        "            if len(train_data) < 10:\n",
        "                return np.full(horizon, np.mean(train_data) if len(train_data) > 0 else 0)\n",
        "            \n",
        "            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –ø–æ—Ä—è–¥–∫–∞ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ä—è–¥–æ–≤\n",
        "            if len(train_data) < 50:\n",
        "                order = (1, 0, 1)\n",
        "            \n",
        "            model = ARIMA(train_data, order=order)\n",
        "            fitted = model.fit()\n",
        "            forecast = fitted.forecast(steps=horizon)\n",
        "            \n",
        "            return np.maximum(forecast, 0)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ ARIMA: {e}\")\n",
        "            return np.full(horizon, np.mean(train_data) if len(train_data) > 0 else 0)\n",
        "    \n",
        "    @staticmethod\n",
        "    def sarima(train_data, horizon=1, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7)):\n",
        "        \"\"\"SARIMA\"\"\"\n",
        "        try:\n",
        "            if len(train_data) < 20:\n",
        "                return AdvancedModels.arima(train_data, horizon)\n",
        "            \n",
        "            model = SARIMAX(train_data, order=order, seasonal_order=seasonal_order)\n",
        "            fitted = model.fit(disp=False)\n",
        "            forecast = fitted.forecast(steps=horizon)\n",
        "            \n",
        "            return np.maximum(forecast, 0)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ SARIMA: {e}\")\n",
        "            return AdvancedModels.arima(train_data, horizon)\n",
        "    \n",
        "    @staticmethod\n",
        "    def sarimax(train_data, exog_train, exog_test, horizon=1):\n",
        "        \"\"\"SARIMAX —Å —ç–∫–∑–æ–≥–µ–Ω–Ω—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏\"\"\"\n",
        "        try:\n",
        "            if len(train_data) < 20:\n",
        "                return AdvancedModels.arima(train_data, horizon)\n",
        "            \n",
        "            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —ç–∫–∑–æ–≥–µ–Ω–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\n",
        "            exog_train_vals = exog_train.iloc[:, 1:].values  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º date\n",
        "            exog_test_vals = exog_test.iloc[:, 1:].values\n",
        "            \n",
        "            model = SARIMAX(train_data, exog=exog_train_vals, \n",
        "                          order=(1, 1, 1), seasonal_order=(1, 1, 1, 7))\n",
        "            fitted = model.fit(disp=False)\n",
        "            forecast = fitted.forecast(steps=horizon, exog=exog_test_vals)\n",
        "            \n",
        "            return np.maximum(forecast, 0)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ SARIMAX: {e}\")\n",
        "            return AdvancedModels.sarima(train_data, horizon)\n",
        "    \n",
        "    @staticmethod\n",
        "    def prophet(train_data, dates_train, dates_test, horizon=1):\n",
        "        \"\"\"Prophet —Å –ø—Ä–∞–∑–¥–Ω–∏–∫–∞–º–∏\"\"\"\n",
        "        try:\n",
        "            if len(train_data) < 30:\n",
        "                return np.full(horizon, np.mean(train_data) if len(train_data) > 0 else 0)\n",
        "            \n",
        "            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Prophet\n",
        "            df = pd.DataFrame({\n",
        "                'ds': dates_train,\n",
        "                'y': train_data\n",
        "            })\n",
        "            \n",
        "            # –ü—Ä–∞–∑–¥–Ω–∏–∫–∏\n",
        "            holidays = pd.DataFrame({\n",
        "                'holiday': ['new_year', 'feb_14', 'feb_23', 'mar_8', 'black_friday'],\n",
        "                'ds': [\n",
        "                    pd.to_datetime('2024-12-31'),\n",
        "                    pd.to_datetime('2024-02-14'),\n",
        "                    pd.to_datetime('2024-02-23'),\n",
        "                    pd.to_datetime('2024-03-08'),\n",
        "                    pd.to_datetime('2024-11-29')\n",
        "                ],\n",
        "                'lower_window': [-15, -7, -7, -7, -14],\n",
        "                'upper_window': [10, 7, 7, 7, 0]\n",
        "            })\n",
        "            \n",
        "            model = Prophet(holidays=holidays, yearly_seasonality=True, weekly_seasonality=True)\n",
        "            model.fit(df)\n",
        "            \n",
        "            # –ü—Ä–æ–≥–Ω–æ–∑\n",
        "            future = model.make_future_dataframe(periods=horizon)\n",
        "            forecast = model.predict(future)\n",
        "            \n",
        "            return np.maximum(forecast['yhat'].tail(horizon).values, 0)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ Prophet: {e}\")\n",
        "            return np.full(horizon, np.mean(train_data) if len(train_data) > 0 else 0)\n",
        "\n",
        "print(\"‚úÖ –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_mape(y_true, y_pred):\n",
        "    \"\"\"Robust MAPE\"\"\"\n",
        "    mask = y_true != 0\n",
        "    if mask.sum() == 0:\n",
        "        return np.inf\n",
        "    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "\n",
        "def rolling_validation(series, dates, calendar_features, demand_class, n_splits=5, test_size=30):\n",
        "    \"\"\"Rolling window validation\"\"\"\n",
        "    if len(series) < test_size * 2:\n",
        "        return {}\n",
        "    \n",
        "    min_train_size = max(30, len(series) - test_size * n_splits)\n",
        "    results = {}\n",
        "    \n",
        "    # –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "    models_to_test = {\n",
        "        'naive': lambda s: BaselineModels.naive(s, test_size),\n",
        "        'moving_average': lambda s: BaselineModels.moving_average(s, 7, test_size),\n",
        "        'seasonal_naive': lambda s: BaselineModels.seasonal_naive(s, 7, test_size),\n",
        "    }\n",
        "    \n",
        "    # –î–æ–±–∞–≤–ª—è–µ–º –º–æ–¥–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–ª–∞—Å—Å–∞ —Å–ø—Ä–æ—Å–∞\n",
        "    if demand_class == 'intermittent':\n",
        "        models_to_test.update({\n",
        "            'croston': lambda s: BaselineModels.croston(s, test_size),\n",
        "            'sba': lambda s: BaselineModels.sba(s, test_size),\n",
        "            'tsb': lambda s: BaselineModels.tsb(s, test_size),\n",
        "        })\n",
        "    else:\n",
        "        models_to_test.update({\n",
        "            'arima': lambda s: AdvancedModels.arima(s, test_size),\n",
        "            'sarima': lambda s: AdvancedModels.sarima(s, test_size),\n",
        "        })\n",
        "    \n",
        "    # –ï—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö, –¥–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏\n",
        "    if len(series) >= 50:\n",
        "        train_features = calendar_features.iloc[:len(series)-test_size]\n",
        "        test_features = calendar_features.iloc[len(series)-test_size:len(series)]\n",
        "        \n",
        "        models_to_test['linear_regression'] = lambda s: AdvancedModels.linear_regression(\n",
        "            s, train_features, test_features, test_size\n",
        "        )\n",
        "        models_to_test['sarimax'] = lambda s: AdvancedModels.sarimax(\n",
        "            s, train_features, test_features, test_size\n",
        "        )\n",
        "    \n",
        "    if len(series) >= 60:\n",
        "        train_dates = dates[:len(series)-test_size]\n",
        "        test_dates = dates[len(series)-test_size:len(series)]\n",
        "        models_to_test['prophet'] = lambda s: AdvancedModels.prophet(\n",
        "            s, train_dates, test_dates, test_size\n",
        "        )\n",
        "    \n",
        "    # –í–∞–ª–∏–¥–∞—Ü–∏—è\n",
        "    for model_name, model_func in models_to_test.items():\n",
        "        try:\n",
        "            # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Ñ–æ–ª–¥\n",
        "            train_data = series[:-test_size]\n",
        "            test_data = series[-test_size:]\n",
        "            \n",
        "            if len(train_data) < min_train_size:\n",
        "                continue\n",
        "            \n",
        "            forecast = model_func(train_data)\n",
        "            \n",
        "            if len(forecast) != len(test_data):\n",
        "                forecast = forecast[:len(test_data)]\n",
        "            \n",
        "            # –ú–µ—Ç—Ä–∏–∫–∏\n",
        "            mae = mean_absolute_error(test_data, forecast)\n",
        "            rmse = np.sqrt(mean_squared_error(test_data, forecast))\n",
        "            mape = calculate_mape(test_data, forecast)\n",
        "            \n",
        "            results[model_name] = {\n",
        "                'mae': mae,\n",
        "                'rmse': rmse,\n",
        "                'mape': mape\n",
        "            }\n",
        "        except Exception as e:\n",
        "            continue\n",
        "    \n",
        "    return results\n",
        "\n",
        "# –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ solo-code\n",
        "print(\"üîç –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π...\")\n",
        "validation_results = []\n",
        "\n",
        "for solo_code in all_solo_codes[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏\n",
        "    sc_sales = total_sales[total_sales['solo_code'] == solo_code].copy()\n",
        "    sc_sales = sc_sales.sort_values('date')\n",
        "    \n",
        "    if len(sc_sales) < 60:\n",
        "        continue\n",
        "    \n",
        "    sales_series = sc_sales['sales'].values\n",
        "    sales_dates = sc_sales['date'].values\n",
        "    \n",
        "    # –ü–æ–ª—É—á–∞–µ–º –∫–ª–∞—Å—Å —Å–ø—Ä–æ—Å–∞\n",
        "    demand_class = demand_df[demand_df['solo_code'] == solo_code]['demand_class'].values[0]\n",
        "    \n",
        "    # –í–∞–ª–∏–¥–∞—Ü–∏—è\n",
        "    results = rolling_validation(sales_series, sales_dates, calendar_features, demand_class)\n",
        "    \n",
        "    if len(results) > 0:\n",
        "        # –í—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –ø–æ MAPE\n",
        "        best_model = min(results.items(), key=lambda x: x[1]['mape'])\n",
        "        \n",
        "        validation_results.append({\n",
        "            'solo_code': solo_code,\n",
        "            'best_model': best_model[0],\n",
        "            'mape': best_model[1]['mape'],\n",
        "            'mae': best_model[1]['mae'],\n",
        "            'rmse': best_model[1]['rmse'],\n",
        "            'all_results': results\n",
        "        })\n",
        "\n",
        "if len(validation_results) > 0:\n",
        "    validation_df = pd.DataFrame(validation_results)\n",
        "else:\n",
        "    validation_df = pd.DataFrame()\n",
        "\n",
        "if len(validation_df) > 0:\n",
        "    print(f\"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –¥–ª—è {len(validation_df)} solo-code\")\n",
        "    print(\"\\nüìä –õ—É—á—à–∏–µ –º–æ–¥–µ–ª–∏:\")\n",
        "    print(validation_df[['solo_code', 'best_model', 'mape', 'mae', 'rmse']].to_string(index=False))\n",
        "    \n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π\n",
        "    ax1 = axes[0]\n",
        "    validation_df['best_model'].value_counts().plot(kind='bar', ax=ax1)\n",
        "    ax1.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π')\n",
        "    ax1.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\n",
        "    ax1.set_xlabel('–ú–æ–¥–µ–ª—å')\n",
        "    ax1.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # MAPE –ø–æ –º–æ–¥–µ–ª—è–º\n",
        "    ax2 = axes[1]\n",
        "    validation_df.boxplot(column='mape', by='best_model', ax=ax2)\n",
        "    ax2.set_title('MAPE –ø–æ –º–æ–¥–µ–ª—è–º')\n",
        "    ax2.set_xlabel('–ú–æ–¥–µ–ª—å')\n",
        "    ax2.set_ylabel('MAPE (%)')\n",
        "    plt.suptitle('')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Rolling-–ø—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂ (18 –º–µ—Å—è—Ü–µ–≤)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Rolling-–ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 18 –º–µ—Å—è—Ü–µ–≤ (548 –¥–Ω–µ–π)\n",
        "FORECAST_HORIZON_DAYS = 18 * 30  # 18 –º–µ—Å—è—Ü–µ–≤\n",
        "FORECAST_START_DATE = END_DATE + timedelta(days=1)\n",
        "FORECAST_DATES = pd.date_range(FORECAST_START_DATE, periods=FORECAST_HORIZON_DAYS, freq='D')\n",
        "\n",
        "print(f\"üìÖ –ü–µ—Ä–∏–æ–¥ –ø—Ä–æ–≥–Ω–æ–∑–∞: {FORECAST_START_DATE.date()} - {FORECAST_DATES[-1].date()}\")\n",
        "print(f\"   –ì–æ—Ä–∏–∑–æ–Ω—Ç: {FORECAST_HORIZON_DAYS} –¥–Ω–µ–π\")\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞\n",
        "forecast_calendar_features = create_calendar_features(FORECAST_DATES)\n",
        "\n",
        "def generate_forecast(solo_code, sales_data, marketplace='Total'):\n",
        "    \"\"\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –¥–ª—è solo-code\"\"\"\n",
        "    sc_sales = sales_data[\n",
        "        (sales_data['solo_code'] == solo_code) & \n",
        "        (sales_data['marketplace'] == marketplace)\n",
        "    ].copy()\n",
        "    \n",
        "    if len(sc_sales) == 0:\n",
        "        return None\n",
        "    \n",
        "    sc_sales = sc_sales.sort_values('date')\n",
        "    sales_series = sc_sales['sales'].values\n",
        "    sales_dates = sc_sales['date'].values\n",
        "    \n",
        "    # –ü–æ–ª—É—á–∞–µ–º –∫–ª–∞—Å—Å —Å–ø—Ä–æ—Å–∞ –∏ –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n",
        "    demand_class = demand_df[demand_df['solo_code'] == solo_code]['demand_class'].values[0] if len(demand_df[demand_df['solo_code'] == solo_code]) > 0 else 'stable'\n",
        "    \n",
        "    best_model_name = None\n",
        "    if 'validation_df' in locals() and len(validation_df) > 0 and solo_code in validation_df['solo_code'].values:\n",
        "        best_model_name = validation_df[validation_df['solo_code'] == solo_code]['best_model'].values[0]\n",
        "    \n",
        "    # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞\n",
        "    try:\n",
        "        if best_model_name == 'naive':\n",
        "            forecast = BaselineModels.naive(sales_series, FORECAST_HORIZON_DAYS)\n",
        "        elif best_model_name == 'moving_average':\n",
        "            forecast = BaselineModels.moving_average(sales_series, 7, FORECAST_HORIZON_DAYS)\n",
        "        elif best_model_name == 'seasonal_naive':\n",
        "            forecast = BaselineModels.seasonal_naive(sales_series, 7, FORECAST_HORIZON_DAYS)\n",
        "        elif best_model_name == 'croston':\n",
        "            forecast = BaselineModels.croston(sales_series, FORECAST_HORIZON_DAYS)\n",
        "        elif best_model_name == 'sba':\n",
        "            forecast = BaselineModels.sba(sales_series, FORECAST_HORIZON_DAYS)\n",
        "        elif best_model_name == 'tsb':\n",
        "            forecast = BaselineModels.tsb(sales_series, FORECAST_HORIZON_DAYS)\n",
        "        elif best_model_name == 'arima':\n",
        "            forecast = AdvancedModels.arima(sales_series, FORECAST_HORIZON_DAYS)\n",
        "        elif best_model_name == 'sarima':\n",
        "            forecast = AdvancedModels.sarima(sales_series, FORECAST_HORIZON_DAYS)\n",
        "        elif best_model_name == 'linear_regression':\n",
        "            train_features = calendar_features.iloc[:len(sales_series)]\n",
        "            forecast = AdvancedModels.linear_regression(sales_series, train_features, \n",
        "                                                       forecast_calendar_features, FORECAST_HORIZON_DAYS)\n",
        "        elif best_model_name == 'sarimax':\n",
        "            train_features = calendar_features.iloc[:len(sales_series)]\n",
        "            forecast = AdvancedModels.sarimax(sales_series, train_features, \n",
        "                                            forecast_calendar_features, FORECAST_HORIZON_DAYS)\n",
        "        elif best_model_name == 'prophet':\n",
        "            forecast = AdvancedModels.prophet(sales_series, sales_dates, FORECAST_DATES, FORECAST_HORIZON_DAYS)\n",
        "        else:\n",
        "            # Fallback: –≤—ã–±–æ—Ä –ø–æ –∫–ª–∞—Å—Å—É —Å–ø—Ä–æ—Å–∞\n",
        "            if demand_class == 'intermittent':\n",
        "                forecast = BaselineModels.tsb(sales_series, FORECAST_HORIZON_DAYS)\n",
        "            elif demand_class == 'seasonal':\n",
        "                forecast = AdvancedModels.sarima(sales_series, FORECAST_HORIZON_DAYS)\n",
        "            else:\n",
        "                forecast = BaselineModels.moving_average(sales_series, 7, FORECAST_HORIZON_DAYS)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞ –¥–ª—è {solo_code}: {e}\")\n",
        "        forecast = BaselineModels.naive(sales_series, FORECAST_HORIZON_DAYS)\n",
        "    \n",
        "    # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –±–∏–∑–Ω–µ—Å-–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π\n",
        "    forecast = apply_withdraw_constraint(forecast, solo_code, withdraw_df)\n",
        "    forecast = apply_defecture_constraint(forecast, FORECAST_DATES, solo_code, defecture_df)\n",
        "    \n",
        "    return forecast\n",
        "\n",
        "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤\n",
        "print(\"üîÆ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è rolling-–ø—Ä–æ–≥–Ω–æ–∑–æ–≤...\")\n",
        "forecasts = {}\n",
        "\n",
        "for solo_code in all_solo_codes:\n",
        "    # –ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è Total\n",
        "    forecast_total = generate_forecast(solo_code, sales_final, 'Total')\n",
        "    if forecast_total is not None:\n",
        "        forecasts[f\"{solo_code}_Total\"] = forecast_total\n",
        "    \n",
        "    # –ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è WB\n",
        "    forecast_wb = generate_forecast(solo_code, sales_final, 'WB')\n",
        "    if forecast_wb is not None:\n",
        "        forecasts[f\"{solo_code}_WB\"] = forecast_wb\n",
        "    \n",
        "    # –ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è Ozon\n",
        "    forecast_ozon = generate_forecast(solo_code, sales_final, 'Ozon')\n",
        "    if forecast_ozon is not None:\n",
        "        forecasts[f\"{solo_code}_Ozon\"] = forecast_ozon\n",
        "\n",
        "print(f\"‚úÖ –ü—Ä–æ–≥–Ω–æ–∑—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è {len(forecasts)} –∫–æ–º–±–∏–Ω–∞—Ü–∏–π\")\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤\n",
        "if len(all_solo_codes) > 0:\n",
        "    example_sc = all_solo_codes[0]\n",
        "    \n",
        "    fig, axes = plt.subplots(3, 1, figsize=(14, 12))\n",
        "    \n",
        "    for idx, marketplace in enumerate(['Total', 'WB', 'Ozon']):\n",
        "        key = f\"{example_sc}_{marketplace}\"\n",
        "        if key in forecasts:\n",
        "            ax = axes[idx]\n",
        "            \n",
        "            # –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ\n",
        "            hist_data = sales_final[\n",
        "                (sales_final['solo_code'] == example_sc) & \n",
        "                (sales_final['marketplace'] == marketplace)\n",
        "            ].sort_values('date')\n",
        "            \n",
        "            # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 90 –¥–Ω–µ–π –∏—Å—Ç–æ—Ä–∏–∏\n",
        "            hist_data = hist_data.tail(90)\n",
        "            \n",
        "            ax.plot(hist_data['date'], hist_data['sales'], \n",
        "                   label='–§–∞–∫—Ç', alpha=0.7, linewidth=2)\n",
        "            \n",
        "            # –ü—Ä–æ–≥–Ω–æ–∑\n",
        "            forecast_vals = forecasts[key]\n",
        "            ax.plot(FORECAST_DATES, forecast_vals, \n",
        "                   label='–ü—Ä–æ–≥–Ω–æ–∑', alpha=0.7, linewidth=2, linestyle='--')\n",
        "            \n",
        "            # –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ\n",
        "            if len(hist_data) > 0:\n",
        "                last_date = hist_data['date'].iloc[-1]\n",
        "                last_value = hist_data['sales'].iloc[-1]\n",
        "                ax.plot([last_date, FORECAST_DATES[0]], [last_value, forecast_vals[0]], \n",
        "                       'k--', alpha=0.3)\n",
        "            \n",
        "            ax.set_title(f'–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂: {example_sc} - {marketplace}')\n",
        "            ax.set_xlabel('–î–∞—Ç–∞')\n",
        "            ax.set_ylabel('–ü—Ä–æ–¥–∞–∂–∏')\n",
        "            ax.legend()\n",
        "            ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. –ê–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –æ—Ç–≥—Ä—É–∑–æ–∫ –∏ Lead Time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def analyze_shipments_and_leadtime(shipments_df, stocks_wb, stocks_ozon):\n",
        "    \"\"\"–ê–Ω–∞–ª–∏–∑ –æ—Ç–≥—Ä—É–∑–æ–∫ –∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ lead time\"\"\"\n",
        "    if shipments_df is None or len(shipments_df) == 0:\n",
        "        return None\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    for solo_code in all_solo_codes:\n",
        "        sc_shipments = shipments_df[shipments_df['solo_code'] == solo_code].copy()\n",
        "        sc_shipments = sc_shipments[sc_shipments['shipment'] > 0].sort_values('date')\n",
        "        \n",
        "        if len(sc_shipments) == 0:\n",
        "            continue\n",
        "        \n",
        "        # –ê–Ω–∞–ª–∏–∑ —á–∞—Å—Ç–æ—Ç—ã –∏ —Ä–∞–∑–º–µ—Ä–∞ –ø–∞—Ä—Ç–∏–π\n",
        "        shipment_dates = sc_shipments['date'].values\n",
        "        shipment_quantities = sc_shipments['shipment'].values\n",
        "        \n",
        "        if len(shipment_dates) > 1:\n",
        "            intervals = np.diff(shipment_dates).astype('timedelta64[D]').astype(int)\n",
        "            avg_interval = np.mean(intervals)\n",
        "            median_interval = np.median(intervals)\n",
        "        else:\n",
        "            avg_interval = 30\n",
        "            median_interval = 30\n",
        "        \n",
        "        avg_batch_size = np.mean(shipment_quantities)\n",
        "        std_batch_size = np.std(shipment_quantities)\n",
        "        \n",
        "        # –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ lead time (–æ—Ç –¥–∞—Ç—ã –æ—Ç–≥—Ä—É–∑–∫–∏ –¥–æ —Ä–æ—Å—Ç–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤)\n",
        "        lead_times = []\n",
        "        \n",
        "        if stocks_wb is not None:\n",
        "            sc_stocks_wb = stocks_wb[stocks_wb['solo_code'] == solo_code].sort_values('date')\n",
        "            \n",
        "            for ship_date, ship_qty in zip(shipment_dates, shipment_quantities):\n",
        "                # –ò—â–µ–º —Ä–æ—Å—Ç –æ—Å—Ç–∞—Ç–∫–æ–≤ –ø–æ—Å–ª–µ –æ—Ç–≥—Ä—É–∑–∫–∏\n",
        "                stocks_after = sc_stocks_wb[sc_stocks_wb['date'] > ship_date].head(10)\n",
        "                \n",
        "                if len(stocks_after) > 0:\n",
        "                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–æ—Å—Ç –æ—Å—Ç–∞—Ç–∫–æ–≤\n",
        "                    base_stock = sc_stocks_wb[sc_stocks_wb['date'] <= ship_date]\n",
        "                    if len(base_stock) > 0:\n",
        "                        base_stock_val = base_stock['stock'].iloc[-1]\n",
        "                        \n",
        "                        for idx, row in stocks_after.iterrows():\n",
        "                            if row['stock'] > base_stock_val * 1.1:  # –†–æ—Å—Ç –Ω–∞ 10%\n",
        "                                lead_time = (row['date'] - ship_date).days\n",
        "                                if 0 < lead_time <= 30:  # –†–∞–∑—É–º–Ω—ã–π –¥–∏–∞–ø–∞–∑–æ–Ω\n",
        "                                    lead_times.append(lead_time)\n",
        "                                break\n",
        "        \n",
        "        if len(lead_times) == 0:\n",
        "            # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –æ—Ç–≥—Ä—É–∑–∫–∞–º–∏\n",
        "            lead_times = [int(avg_interval)]\n",
        "        \n",
        "        avg_lead_time = np.mean(lead_times)\n",
        "        p90_lead_time = np.percentile(lead_times, 90) if len(lead_times) > 0 else avg_lead_time\n",
        "        \n",
        "        results.append({\n",
        "            'solo_code': solo_code,\n",
        "            'avg_interval': avg_interval,\n",
        "            'median_interval': median_interval,\n",
        "            'avg_batch_size': avg_batch_size,\n",
        "            'std_batch_size': std_batch_size,\n",
        "            'avg_lead_time': avg_lead_time,\n",
        "            'p90_lead_time': p90_lead_time,\n",
        "            'n_shipments': len(sc_shipments)\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# –ê–Ω–∞–ª–∏–∑ –æ—Ç–≥—Ä—É–∑–æ–∫ –∏ lead time\n",
        "print(\"üì¶ –ê–Ω–∞–ª–∏–∑ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –æ—Ç–≥—Ä—É–∑–æ–∫ –∏ lead time...\")\n",
        "shipment_analysis = analyze_shipments_and_leadtime(shipments_clean, wb_stocks_clean, ozon_stocks_clean)\n",
        "\n",
        "if shipment_analysis is not None and len(shipment_analysis) > 0:\n",
        "    print(f\"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –¥–ª—è {len(shipment_analysis)} solo-code\")\n",
        "    print(\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ç–≥—Ä—É–∑–æ–∫ –∏ lead time:\")\n",
        "    print(shipment_analysis.to_string(index=False))\n",
        "    \n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    # –°—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –æ—Ç–≥—Ä—É–∑–∫–∞–º–∏\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.bar(range(len(shipment_analysis)), shipment_analysis['avg_interval'], alpha=0.7)\n",
        "    ax1.set_title('–°—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –º–µ–∂–¥—É –æ—Ç–≥—Ä—É–∑–∫–∞–º–∏')\n",
        "    ax1.set_xlabel('Solo-code')\n",
        "    ax1.set_ylabel('–î–Ω–µ–π')\n",
        "    ax1.set_xticks(range(len(shipment_analysis)))\n",
        "    ax1.set_xticklabels(shipment_analysis['solo_code'], rotation=45)\n",
        "    ax1.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –ø–∞—Ä—Ç–∏–∏\n",
        "    ax2 = axes[0, 1]\n",
        "    ax2.bar(range(len(shipment_analysis)), shipment_analysis['avg_batch_size'], alpha=0.7)\n",
        "    ax2.set_title('–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –ø–∞—Ä—Ç–∏–∏')\n",
        "    ax2.set_xlabel('Solo-code')\n",
        "    ax2.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ')\n",
        "    ax2.set_xticks(range(len(shipment_analysis)))\n",
        "    ax2.set_xticklabels(shipment_analysis['solo_code'], rotation=45)\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Lead time\n",
        "    ax3 = axes[1, 0]\n",
        "    ax3.bar(range(len(shipment_analysis)), shipment_analysis['avg_lead_time'], \n",
        "           alpha=0.7, label='–°—Ä–µ–¥–Ω–∏–π')\n",
        "    ax3.bar(range(len(shipment_analysis)), shipment_analysis['p90_lead_time'], \n",
        "           alpha=0.5, label='P90')\n",
        "    ax3.set_title('Lead Time')\n",
        "    ax3.set_xlabel('Solo-code')\n",
        "    ax3.set_ylabel('–î–Ω–µ–π')\n",
        "    ax3.set_xticks(range(len(shipment_analysis)))\n",
        "    ax3.set_xticklabels(shipment_analysis['solo_code'], rotation=45)\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # –í–∞—Ä–∏–∞–±–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ä–∞ –ø–∞—Ä—Ç–∏–∏\n",
        "    ax4 = axes[1, 1]\n",
        "    ax4.scatter(shipment_analysis['avg_batch_size'], shipment_analysis['std_batch_size'], \n",
        "               alpha=0.7, s=100)\n",
        "    ax4.set_title('–í–∞—Ä–∏–∞–±–µ–ª—å–Ω–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ä–∞ –ø–∞—Ä—Ç–∏–∏')\n",
        "    ax4.set_xlabel('–°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –ø–∞—Ä—Ç–∏–∏')\n",
        "    ax4.set_ylabel('–°—Ç. –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\")\n",
        "    # –°–æ–∑–¥–∞–µ–º –∑–∞–≥–ª—É—à–∫—É\n",
        "    shipment_analysis = pd.DataFrame({\n",
        "        'solo_code': all_solo_codes,\n",
        "        'avg_interval': [30] * len(all_solo_codes),\n",
        "        'median_interval': [30] * len(all_solo_codes),\n",
        "        'avg_batch_size': [200] * len(all_solo_codes),\n",
        "        'std_batch_size': [50] * len(all_solo_codes),\n",
        "        'avg_lead_time': [7] * len(all_solo_codes),\n",
        "        'p90_lead_time': [14] * len(all_solo_codes),\n",
        "        'n_shipments': [10] * len(all_solo_codes)\n",
        "    })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. –†–∞—Å—á–µ—Ç —Å—Ç—Ä–∞—Ö–æ–≤–æ–≥–æ –∑–∞–ø–∞—Å–∞\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –†–∞—Å—á–µ—Ç —Å—Ç—Ä–∞—Ö–æ–≤–æ–≥–æ –∑–∞–ø–∞—Å–∞\n",
        "# Safety Stock = Z * œÉ * sqrt(lead_time)\n",
        "# Service Level = 95% -> Z = 1.65\n",
        "\n",
        "SERVICE_LEVEL = 0.95\n",
        "Z_SCORE = 1.65  # –î–ª—è 95% service level\n",
        "\n",
        "def calculate_safety_stock(sales_series, lead_time):\n",
        "    \"\"\"–†–∞—Å—á–µ—Ç —Å—Ç—Ä–∞—Ö–æ–≤–æ–≥–æ –∑–∞–ø–∞—Å–∞\"\"\"\n",
        "    if len(sales_series) == 0:\n",
        "        return 0\n",
        "    \n",
        "    # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–Ω–µ–≤–Ω–æ–≥–æ —Å–ø—Ä–æ—Å–∞\n",
        "    daily_demand_std = np.std(sales_series)\n",
        "    \n",
        "    # –°—Ç—Ä–∞—Ö–æ–≤–æ–π –∑–∞–ø–∞—Å\n",
        "    safety_stock = Z_SCORE * daily_demand_std * np.sqrt(lead_time)\n",
        "    \n",
        "    return max(0, safety_stock)\n",
        "\n",
        "print(\"üìä –†–∞—Å—á–µ—Ç —Å—Ç—Ä–∞—Ö–æ–≤–æ–≥–æ –∑–∞–ø–∞—Å–∞...\")\n",
        "safety_stocks = []\n",
        "\n",
        "for solo_code in all_solo_codes:\n",
        "    sc_sales = total_sales[total_sales['solo_code'] == solo_code]['sales'].values\n",
        "    \n",
        "    if len(sc_sales) == 0:\n",
        "        continue\n",
        "    \n",
        "    # –ü–æ–ª—É—á–∞–µ–º lead time\n",
        "    if shipment_analysis is not None and solo_code in shipment_analysis['solo_code'].values:\n",
        "        lead_time = shipment_analysis[shipment_analysis['solo_code'] == solo_code]['avg_lead_time'].values[0]\n",
        "    else:\n",
        "        lead_time = 7  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é\n",
        "    \n",
        "    safety_stock = calculate_safety_stock(sc_sales, lead_time)\n",
        "    \n",
        "    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
        "    avg_daily_demand = np.mean(sc_sales)\n",
        "    daily_demand_std = np.std(sc_sales)\n",
        "    \n",
        "    safety_stocks.append({\n",
        "        'solo_code': solo_code,\n",
        "        'avg_daily_demand': avg_daily_demand,\n",
        "        'daily_demand_std': daily_demand_std,\n",
        "        'lead_time': lead_time,\n",
        "        'safety_stock': safety_stock,\n",
        "        'days_cover_safety': safety_stock / avg_daily_demand if avg_daily_demand > 0 else 0\n",
        "    })\n",
        "\n",
        "safety_stock_df = pd.DataFrame(safety_stocks)\n",
        "\n",
        "print(f\"‚úÖ –°—Ç—Ä–∞—Ö–æ–≤–æ–π –∑–∞–ø–∞—Å —Ä–∞—Å—Å—á–∏—Ç–∞–Ω –¥–ª—è {len(safety_stock_df)} solo-code\")\n",
        "print(\"\\nüìã –¢–∞–±–ª–∏—Ü–∞ —Å—Ç—Ä–∞—Ö–æ–≤–æ–≥–æ –∑–∞–ø–∞—Å–∞:\")\n",
        "print(safety_stock_df.to_string(index=False))\n",
        "\n",
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# –°—Ç—Ä–∞—Ö–æ–≤–æ–π –∑–∞–ø–∞—Å –ø–æ solo-code\n",
        "ax1 = axes[0]\n",
        "ax1.bar(range(len(safety_stock_df)), safety_stock_df['safety_stock'], alpha=0.7)\n",
        "ax1.set_title('–°—Ç—Ä–∞—Ö–æ–≤–æ–π –∑–∞–ø–∞—Å –ø–æ solo-code')\n",
        "ax1.set_xlabel('Solo-code')\n",
        "ax1.set_ylabel('–°—Ç—Ä–∞—Ö–æ–≤–æ–π –∑–∞–ø–∞—Å')\n",
        "ax1.set_xticks(range(len(safety_stock_df)))\n",
        "ax1.set_xticklabels(safety_stock_df['solo_code'], rotation=45)\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# –î–Ω–∏ –ø–æ–∫—Ä—ã—Ç–∏—è —Å—Ç—Ä–∞—Ö–æ–≤—ã–º –∑–∞–ø–∞—Å–æ–º\n",
        "ax2 = axes[1]\n",
        "ax2.bar(range(len(safety_stock_df)), safety_stock_df['days_cover_safety'], alpha=0.7)\n",
        "ax2.set_title('–î–Ω–∏ –ø–æ–∫—Ä—ã—Ç–∏—è —Å—Ç—Ä–∞—Ö–æ–≤—ã–º –∑–∞–ø–∞—Å–æ–º')\n",
        "ax2.set_xlabel('Solo-code')\n",
        "ax2.set_ylabel('–î–Ω–µ–π')\n",
        "ax2.set_xticks(range(len(safety_stock_df)))\n",
        "ax2.set_xticklabels(safety_stock_df['solo_code'], rotation=45)\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≥—Ä—É–∑–æ–∫\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ count_box\n",
        "count_box_df = data.get('count_box', pd.DataFrame())\n",
        "if len(count_box_df) > 0:\n",
        "    count_box_df.columns = [col.lower().replace('-', '_') for col in count_box_df.columns]\n",
        "    if 'solo_code' not in count_box_df.columns and 'solo-code' in count_box_df.columns:\n",
        "        count_box_df['solo_code'] = count_box_df['solo-code']\n",
        "    count_col = [col for col in count_box_df.columns if '–∫–æ–ª' in col.lower() or 'count' in col.lower()][0]\n",
        "    count_box_df['count_per_box'] = count_box_df[count_col]\n",
        "    \n",
        "    print(f\"üì¶ Count box: {len(count_box_df)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Count box –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 12\")\n",
        "    count_box_df = pd.DataFrame({\n",
        "        'solo_code': all_solo_codes,\n",
        "        'count_per_box': [12] * len(all_solo_codes)\n",
        "    })\n",
        "\n",
        "def plan_shipments(solo_code, forecast_dates, forecast_values, current_stocks_wb, current_stocks_ozon, \n",
        "                  current_stocks_our, safety_stock, avg_batch_size, count_per_box, lead_time):\n",
        "    \"\"\"–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≥—Ä—É–∑–æ–∫\"\"\"\n",
        "    shipment_plan = []\n",
        "    \n",
        "    # –¢–µ–∫—É—â–∏–µ –æ—Å—Ç–∞—Ç–∫–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)\n",
        "    stock_wb = current_stocks_wb if current_stocks_wb is not None else 0\n",
        "    stock_ozon = current_stocks_ozon if current_stocks_ozon is not None else 0\n",
        "    stock_our = current_stocks_our if current_stocks_our is not None else 0\n",
        "    \n",
        "    # –¶–µ–ª–µ–≤–æ–π —É—Ä–æ–≤–µ–Ω—å –∑–∞–ø–∞—Å–∞ (—Å—Ä–µ–¥–Ω–∏–π —Å–ø—Ä–æ—Å * –ø–æ–∫—Ä—ã—Ç–∏–µ + —Å—Ç—Ä–∞—Ö–æ–≤–æ–π –∑–∞–ø–∞—Å)\n",
        "    avg_daily_demand = np.mean(forecast_values) if len(forecast_values) > 0 else 0\n",
        "    target_coverage_days = 30  # –¶–µ–ª–µ–≤–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ 30 –¥–Ω–µ–π\n",
        "    target_stock = avg_daily_demand * target_coverage_days + safety_stock\n",
        "    \n",
        "    # –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –∫–∞–∂–¥—ã–π –º–µ—Å—è—Ü\n",
        "    monthly_forecasts = []\n",
        "    monthly_dates = []\n",
        "    \n",
        "    current_month = forecast_dates[0].month\n",
        "    month_forecast = 0\n",
        "    month_start_date = forecast_dates[0]\n",
        "    \n",
        "    for i, (date, forecast) in enumerate(zip(forecast_dates, forecast_values)):\n",
        "        if date.month != current_month or i == len(forecast_dates) - 1:\n",
        "            if i == len(forecast_dates) - 1:\n",
        "                month_forecast += forecast\n",
        "            \n",
        "            monthly_forecasts.append(month_forecast)\n",
        "            monthly_dates.append(month_start_date)\n",
        "            \n",
        "            if i < len(forecast_dates) - 1:\n",
        "                current_month = date.month\n",
        "                month_forecast = forecast\n",
        "                month_start_date = date\n",
        "        else:\n",
        "            month_forecast += forecast\n",
        "    \n",
        "    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–∞—Ç –æ—Ç–≥—Ä—É–∑–æ–∫\n",
        "    for month_idx, (month_date, month_forecast) in enumerate(zip(monthly_dates, monthly_forecasts)):\n",
        "        # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–µ –æ—Å—Ç–∞—Ç–∫–∏ –≤ –∫–æ–Ω—Ü–µ –º–µ—Å—è—Ü–∞\n",
        "        projected_stock = stock_wb + stock_ozon - month_forecast\n",
        "        \n",
        "        # –ß–∏—Å—Ç–∞—è –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å\n",
        "        net_requirement = max(0, target_stock - projected_stock)\n",
        "        \n",
        "        if net_requirement > 0:\n",
        "            # –û–∫—Ä—É–≥–ª–µ–Ω–∏–µ –¥–æ –∫–æ—Ä–æ–±–∞\n",
        "            boxes_needed = int(np.ceil(net_requirement / count_per_box))\n",
        "            packages_needed = boxes_needed * count_per_box\n",
        "            \n",
        "            # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –¥–∞—Ç–∞ –æ—Ç–≥—Ä—É–∑–∫–∏ (—Å —É—á–µ—Ç–æ–º lead time)\n",
        "            shipment_date = month_date - timedelta(days=int(lead_time))\n",
        "            \n",
        "            shipment_plan.append({\n",
        "                'date': shipment_date,\n",
        "                'solo_code': solo_code,\n",
        "                'forecast_monthly': month_forecast,\n",
        "                'current_stock_wb': stock_wb,\n",
        "                'current_stock_ozon': stock_ozon,\n",
        "                'current_stock_our': stock_our,\n",
        "                'safety_stock': safety_stock,\n",
        "                'target_stock': target_stock,\n",
        "                'net_requirement': net_requirement,\n",
        "                'shipment_packages': packages_needed,\n",
        "                'shipment_boxes': boxes_needed\n",
        "            })\n",
        "            \n",
        "            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ—Å—Ç–∞—Ç–∫–æ–≤ –ø–æ—Å–ª–µ –æ—Ç–≥—Ä—É–∑–∫–∏\n",
        "            stock_our -= packages_needed\n",
        "            stock_wb += packages_needed  # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞\n",
        "    \n",
        "    return pd.DataFrame(shipment_plan)\n",
        "\n",
        "# –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≥—Ä—É–∑–æ–∫ –¥–ª—è –≤—Å–µ—Ö solo-code\n",
        "print(\"üìÖ –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≥—Ä—É–∑–æ–∫...\")\n",
        "shipment_plans = []\n",
        "\n",
        "for solo_code in all_solo_codes:\n",
        "    # –ü—Ä–æ–≥–Ω–æ–∑\n",
        "    forecast_key = f\"{solo_code}_Total\"\n",
        "    if forecast_key not in forecasts:\n",
        "        continue\n",
        "    \n",
        "    forecast_vals = forecasts[forecast_key]\n",
        "    \n",
        "    # –¢–µ–∫—É—â–∏–µ –æ—Å—Ç–∞—Ç–∫–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è)\n",
        "    current_stock_wb = None\n",
        "    if wb_stocks_clean is not None:\n",
        "        sc_stocks_wb = wb_stocks_clean[wb_stocks_clean['solo_code'] == solo_code]\n",
        "        if len(sc_stocks_wb) > 0:\n",
        "            current_stock_wb = sc_stocks_wb['stock'].iloc[-1]\n",
        "    \n",
        "    current_stock_ozon = None\n",
        "    if ozon_stocks_clean is not None:\n",
        "        sc_stocks_ozon = ozon_stocks_clean[ozon_stocks_clean['solo_code'] == solo_code]\n",
        "        if len(sc_stocks_ozon) > 0:\n",
        "            current_stock_ozon = sc_stocks_ozon['stock'].iloc[-1]\n",
        "    \n",
        "    current_stock_our = None\n",
        "    if our_stocks_clean is not None:\n",
        "        sc_stocks_our = our_stocks_clean[our_stocks_clean['solo_code'] == solo_code]\n",
        "        if len(sc_stocks_our) > 0:\n",
        "            current_stock_our = sc_stocks_our['stock'].iloc[-1]\n",
        "    \n",
        "    # –°—Ç—Ä–∞—Ö–æ–≤–æ–π –∑–∞–ø–∞—Å\n",
        "    safety_stock = 0\n",
        "    if solo_code in safety_stock_df['solo_code'].values:\n",
        "        safety_stock = safety_stock_df[safety_stock_df['solo_code'] == solo_code]['safety_stock'].values[0]\n",
        "    \n",
        "    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Ç–≥—Ä—É–∑–∫–∏\n",
        "    avg_batch_size = 200\n",
        "    if shipment_analysis is not None and solo_code in shipment_analysis['solo_code'].values:\n",
        "        avg_batch_size = shipment_analysis[shipment_analysis['solo_code'] == solo_code]['avg_batch_size'].values[0]\n",
        "    \n",
        "    count_per_box = 12\n",
        "    if solo_code in count_box_df['solo_code'].values:\n",
        "        count_per_box = count_box_df[count_box_df['solo_code'] == solo_code]['count_per_box'].values[0]\n",
        "    \n",
        "    lead_time = 7\n",
        "    if shipment_analysis is not None and solo_code in shipment_analysis['solo_code'].values:\n",
        "        lead_time = shipment_analysis[shipment_analysis['solo_code'] == solo_code]['avg_lead_time'].values[0]\n",
        "    \n",
        "    # –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "    plan = plan_shipments(solo_code, FORECAST_DATES, forecast_vals, \n",
        "                         current_stock_wb, current_stock_ozon, current_stock_our,\n",
        "                         safety_stock, avg_batch_size, count_per_box, lead_time)\n",
        "    \n",
        "    if len(plan) > 0:\n",
        "        shipment_plans.append(plan)\n",
        "\n",
        "if len(shipment_plans) > 0:\n",
        "    final_shipment_plan = pd.concat(shipment_plans, ignore_index=True)\n",
        "    final_shipment_plan = final_shipment_plan.sort_values('date')\n",
        "    \n",
        "    print(f\"‚úÖ –ü–ª–∞–Ω –æ—Ç–≥—Ä—É–∑–æ–∫ —Å–æ–∑–¥–∞–Ω: {len(final_shipment_plan)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "    print(\"\\nüìã –§–∏–Ω–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≥—Ä—É–∑–æ–∫:\")\n",
        "    print(final_shipment_plan.to_string(index=False))\n",
        "    \n",
        "    # –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV\n",
        "    final_shipment_plan.to_csv('shipment_plan.csv', index=False, encoding='utf-8-sig')\n",
        "    print(\"\\nüíæ –ü–ª–∞–Ω —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ shipment_plan.csv\")\n",
        "    \n",
        "    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "    \n",
        "    # –ì—Ä–∞—Ñ–∏–∫ –æ—Ç–≥—Ä—É–∑–æ–∫ –ø–æ –¥–∞—Ç–∞–º\n",
        "    ax1 = axes[0]\n",
        "    for solo_code in final_shipment_plan['solo_code'].unique():\n",
        "        sc_plan = final_shipment_plan[final_shipment_plan['solo_code'] == solo_code]\n",
        "        ax1.bar(sc_plan['date'], sc_plan['shipment_packages'], \n",
        "               label=solo_code, alpha=0.7, width=5)\n",
        "    ax1.set_title('–ü–ª–∞–Ω–∏—Ä—É–µ–º—ã–µ –æ—Ç–≥—Ä—É–∑–∫–∏ –ø–æ –¥–∞—Ç–∞–º')\n",
        "    ax1.set_xlabel('–î–∞—Ç–∞')\n",
        "    ax1.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫–æ–≤–æ–∫')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # –°—É–º–º–∞—Ä–Ω—ã–µ –æ—Ç–≥—Ä—É–∑–∫–∏ –ø–æ solo-code\n",
        "    ax2 = axes[1]\n",
        "    total_by_sc = final_shipment_plan.groupby('solo_code')['shipment_packages'].sum()\n",
        "    ax2.bar(range(len(total_by_sc)), total_by_sc.values, alpha=0.7)\n",
        "    ax2.set_title('–°—É–º–º–∞—Ä–Ω—ã–µ –æ—Ç–≥—Ä—É–∑–∫–∏ –ø–æ solo-code')\n",
        "    ax2.set_xlabel('Solo-code')\n",
        "    ax2.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫–æ–≤–æ–∫')\n",
        "    ax2.set_xticks(range(len(total_by_sc)))\n",
        "    ax2.set_xticklabels(total_by_sc.index, rotation=45)\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –ø–ª–∞–Ω –æ—Ç–≥—Ä—É–∑–æ–∫\")\n",
        "    final_shipment_plan = pd.DataFrame()  # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—É—Å—Ç–æ–≥–æ DataFrame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. –§–∏–Ω–∞–ª—å–Ω—ã–π Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"üìä –§–ò–ù–ê–õ–¨–ù–´–ô SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# 1. Summary –ø–æ –º–æ–¥–µ–ª—è–º\n",
        "print(\"\\n1Ô∏è‚É£ SUMMARY –ü–û –ú–û–î–ï–õ–Ø–ú\")\n",
        "print(\"-\" * 80)\n",
        "if 'validation_df' in locals() and len(validation_df) > 0:\n",
        "    print(f\"‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ –¥–ª—è {len(validation_df)} solo-code\")\n",
        "    print(f\"\\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ª—É—á—à–∏—Ö –º–æ–¥–µ–ª–µ–π:\")\n",
        "    model_dist = validation_df['best_model'].value_counts()\n",
        "    for model, count in model_dist.items():\n",
        "        print(f\"   {model}: {count} ({100*count/len(validation_df):.1f}%)\")\n",
        "    \n",
        "    print(f\"\\n–°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏:\")\n",
        "    print(f\"   MAPE: {validation_df['mape'].mean():.2f}%\")\n",
        "    print(f\"   MAE: {validation_df['mae'].mean():.2f}\")\n",
        "    print(f\"   RMSE: {validation_df['rmse'].mean():.2f}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞ (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö)\")\n",
        "\n",
        "# 2. –û—à–∏–±–∫–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "print(\"\\n2Ô∏è‚É£ –û–®–ò–ë–ö–ò –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø\")\n",
        "print(\"-\" * 80)\n",
        "if 'validation_df' in locals() and len(validation_df) > 0:\n",
        "    print(f\"   –°—Ä–µ–¥–Ω–∏–π MAPE: {validation_df['mape'].mean():.2f}%\")\n",
        "    print(f\"   –ú–µ–¥–∏–∞–Ω–Ω—ã–π MAPE: {validation_df['mape'].median():.2f}%\")\n",
        "    print(f\"   –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π MAPE: {validation_df['mape'].min():.2f}%\")\n",
        "    print(f\"   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π MAPE: {validation_df['mape'].max():.2f}%\")\n",
        "    \n",
        "    worst_forecast = validation_df.loc[validation_df['mape'].idxmax()]\n",
        "    best_forecast = validation_df.loc[validation_df['mape'].idxmin()]\n",
        "    \n",
        "    print(f\"\\n   –õ—É—á—à–∏–π –ø—Ä–æ–≥–Ω–æ–∑: {best_forecast['solo_code']} (MAPE: {best_forecast['mape']:.2f}%)\")\n",
        "    print(f\"   –•—É–¥—à–∏–π –ø—Ä–æ–≥–Ω–æ–∑: {worst_forecast['solo_code']} (MAPE: {worst_forecast['mape']:.2f}%)\")\n",
        "\n",
        "# 3. –†–∏—Å–∫–∏\n",
        "print(\"\\n3Ô∏è‚É£ –†–ò–°–ö–ò\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# –†–∏—Å–∫ –ø–æ withdraw\n",
        "if len(withdraw_df) > 0:\n",
        "    print(f\"‚ö†Ô∏è Withdraw: {len(withdraw_df)} solo-code –≤—ã–≤–µ–¥–µ–Ω—ã –∏–∑ –ø—Ä–æ–¥–∞–∂–∏\")\n",
        "    print(f\"   {list(withdraw_df['solo_code'].unique())}\")\n",
        "\n",
        "# –†–∏—Å–∫ –ø–æ defecture\n",
        "if len(defecture_df) > 0:\n",
        "    print(f\"‚ö†Ô∏è Defecture: {len(defecture_df)} solo-code –≤ –¥–µ—Ñ–µ–∫—Ç—É—Ä–µ\")\n",
        "    for _, row in defecture_df.iterrows():\n",
        "        print(f\"   {row['solo_code']}: –¥–æ {row.get('defecture_end_date', 'N/A')}\")\n",
        "\n",
        "# –†–∏—Å–∫ –ø–æ –ø–æ–∫—Ä—ã—Ç–∏—é\n",
        "if days_cover_df is not None:\n",
        "    low_cover = days_cover_df[days_cover_df['days_cover'] < 7]\n",
        "    if len(low_cover) > 0:\n",
        "        print(f\"‚ö†Ô∏è –ù–∏–∑–∫–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ (<7 –¥–Ω–µ–π): {len(low_cover)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "        low_cover_sc = low_cover.groupby('solo_code')['days_cover'].min()\n",
        "        print(f\"   Solo-codes —Å –Ω–∏–∑–∫–∏–º –ø–æ–∫—Ä—ã—Ç–∏–µ–º: {list(low_cover_sc.index[:5])}\")\n",
        "\n",
        "# –†–∏—Å–∫ –ø–æ —Å—Ç—Ä–∞—Ö–æ–≤–æ–º—É –∑–∞–ø–∞—Å—É\n",
        "if len(safety_stock_df) > 0:\n",
        "    high_safety = safety_stock_df[safety_stock_df['days_cover_safety'] > 30]\n",
        "    if len(high_safety) > 0:\n",
        "        print(f\"‚ö†Ô∏è –í—ã—Å–æ–∫–∏–π —Å—Ç—Ä–∞—Ö–æ–≤–æ–π –∑–∞–ø–∞—Å (>30 –¥–Ω–µ–π): {len(high_safety)} solo-code\")\n",
        "\n",
        "# 4. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é\n",
        "print(\"\\n4Ô∏è‚É£ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –†–ê–ó–í–ò–¢–ò–Æ\")\n",
        "print(\"-\" * 80)\n",
        "print(\"\"\"\n",
        "üìå –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö:\n",
        "   ‚Ä¢ –£–≤–µ–ª–∏—á–∏—Ç—å —á–∞—Å—Ç–æ—Ç—É –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –æ—Å—Ç–∞—Ç–∫–æ–≤ (–º–∏–Ω–∏–º—É–º –µ–∂–µ–¥–Ω–µ–≤–Ω–æ)\n",
        "   ‚Ä¢ –î–æ–±–∞–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–º–æ-–∞–∫—Ü–∏—è—Ö –∏ —Ä–µ–∫–ª–∞–º–Ω—ã—Ö –∫–∞–º–ø–∞–Ω–∏—è—Ö\n",
        "   ‚Ä¢ –í–∫–ª—é—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞—Ö –∏ —Ä—ã–Ω–æ—á–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–∞—Ö\n",
        "\n",
        "üìå –£–ª—É—á—à–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π:\n",
        "   ‚Ä¢ –î–æ–±–∞–≤–∏—Ç—å –∞–Ω—Å–∞–º–±–ª–∏ –º–æ–¥–µ–ª–µ–π (voting, stacking)\n",
        "   ‚Ä¢ –í–Ω–µ–¥—Ä–∏—Ç—å deep learning –º–æ–¥–µ–ª–∏ (LSTM, Transformer) –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤\n",
        "   ‚Ä¢ –î–æ–±–∞–≤–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (Optuna, Hyperopt)\n",
        "\n",
        "üìå –£–ª—É—á—à–µ–Ω–∏–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è:\n",
        "   ‚Ä¢ –£—á–µ—Å—Ç—å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —Å–∫–ª–∞–¥–∞ –∏ –ª–æ–≥–∏—Å—Ç–∏–∫–∏\n",
        "   ‚Ä¢ –î–æ–±–∞–≤–∏—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é —Ä–∞–∑–º–µ—Ä–∞ –ø–∞—Ä—Ç–∏–∏ (EOQ –º–æ–¥–µ–ª—å)\n",
        "   ‚Ä¢ –í–Ω–µ–¥—Ä–∏—Ç—å multi-echelon inventory optimization\n",
        "\n",
        "üìå –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è:\n",
        "   ‚Ä¢ –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ—Å—á–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ (–µ–∂–µ–¥–Ω–µ–≤–Ω–æ/–µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω–æ)\n",
        "   ‚Ä¢ –î–æ–±–∞–≤–∏—Ç—å –∞–ª–µ—Ä—Ç—ã –ø—Ä–∏ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏—è—Ö –æ—Ç –ø–ª–∞–Ω–∞\n",
        "   ‚Ä¢ –°–æ–∑–¥–∞—Ç—å –¥–∞—à–±–æ—Ä–¥ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ KPI\n",
        "\n",
        "üìå –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è:\n",
        "   ‚Ä¢ –ü–æ–¥–∫–ª—é—á–∏—Ç—å API –º–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å–æ–≤ –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏\n",
        "   ‚Ä¢ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å WMS/ERP —Å–∏—Å—Ç–µ–º–∞–º–∏\n",
        "   ‚Ä¢ –î–æ–±–∞–≤–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∑–∞–∫–∞–∑–æ–≤\n",
        "\"\"\")\n",
        "\n",
        "# 5. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–æ–≥–Ω–æ–∑–∞–º\n",
        "print(\"\\n5Ô∏è‚É£ –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ü–†–û–ì–ù–û–ó–ê–ú\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"   –ü–µ—Ä–∏–æ–¥ –ø—Ä–æ–≥–Ω–æ–∑–∞: {FORECAST_START_DATE.date()} - {FORECAST_DATES[-1].date()}\")\n",
        "print(f\"   –ì–æ—Ä–∏–∑–æ–Ω—Ç: {FORECAST_HORIZON_DAYS} –¥–Ω–µ–π ({FORECAST_HORIZON_DAYS/30:.1f} –º–µ—Å—è—Ü–µ–≤)\")\n",
        "print(f\"   Solo-code —Å –ø—Ä–æ–≥–Ω–æ–∑–∞–º–∏: {len([k for k in forecasts.keys() if '_Total' in k])}\")\n",
        "\n",
        "if 'final_shipment_plan' in locals() and len(final_shipment_plan) > 0:\n",
        "    print(f\"\\n   –ü–ª–∞–Ω–∏—Ä—É–µ–º—ã–µ –æ—Ç–≥—Ä—É–∑–∫–∏:\")\n",
        "    print(f\"   ‚Ä¢ –í—Å–µ–≥–æ –æ—Ç–≥—Ä—É–∑–æ–∫: {len(final_shipment_plan)}\")\n",
        "    print(f\"   ‚Ä¢ –°—É–º–º–∞—Ä–Ω—ã–π –æ–±—ä–µ–º: {final_shipment_plan['shipment_packages'].sum():.0f} —É–ø–∞–∫–æ–≤–æ–∫\")\n",
        "    print(f\"   ‚Ä¢ –°—É–º–º–∞—Ä–Ω—ã–π –æ–±—ä–µ–º: {final_shipment_plan['shipment_boxes'].sum():.0f} –∫–æ—Ä–æ–±–æ–≤\")\n",
        "    print(f\"   ‚Ä¢ –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –æ—Ç–≥—Ä—É–∑–∫–∏: {final_shipment_plan['shipment_packages'].mean():.0f} —É–ø–∞–∫–æ–≤–æ–∫\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# –°–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–¥–∞–∂ –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –æ—Ç–≥—Ä—É–∑–æ–∫\n",
        "## Wildberries & Ozon\n",
        "\n",
        "**–¶–µ–ª—å:** –ü–æ—Å—Ç—Ä–æ–∏—Ç—å —Å–∏—Å—Ç–µ–º—É rolling-–ø—Ä–æ–≥–Ω–æ–∑–∞ –ø—Ä–æ–¥–∞–∂ –Ω–∞ 18 –º–µ—Å—è—Ü–µ–≤ —Å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—Ç–≥—Ä—É–∑–æ–∫\n",
        "\n",
        "**–ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö:** 26.03.2024 ‚Äì 20.12.2025 (–¥–Ω–µ–≤–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞)\n",
        "\n",
        "**–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä:** solo-code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ò–º–ø–æ—Ä—Ç –±–∏–±–ª–∏–æ—Ç–µ–∫\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from prophet import Prophet\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É CSV –∏ Excel —Ñ–∞–π–ª–æ–≤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    \"\"\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö (CSV –∏–ª–∏ Excel)\"\"\"\n",
        "    if file_path.endswith('.csv'):\n",
        "        return pd.read_csv(file_path, encoding='utf-8')\n",
        "    elif file_path.endswith(('.xlsx', '.xls')):\n",
        "        return pd.read_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(f\"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_path}\")\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–¥–∞–∂\n",
        "try:\n",
        "    wb_sales = load_data('wb_sales.csv')\n",
        "    print(f\"‚úÖ WB –ø—Ä–æ–¥–∞–∂–∏: {wb_sales.shape}\")\n",
        "except:\n",
        "    try:\n",
        "        wb_sales = load_data('wb_sales.xlsx')\n",
        "        print(f\"‚úÖ WB –ø—Ä–æ–¥–∞–∂–∏: {wb_sales.shape}\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è –§–∞–π–ª wb_sales –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö\")\n",
        "        dates = pd.date_range('2024-03-26', '2025-12-20', freq='D')\n",
        "        wb_sales = pd.DataFrame({\n",
        "            '–î–∞—Ç–∞': dates,\n",
        "            'solo-code': 'SKU001',\n",
        "            'SKU': 'SKU001',\n",
        "            '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.': np.random.poisson(10, len(dates))\n",
        "        })\n",
        "\n",
        "try:\n",
        "    ozon_sales = load_data('ozon_sales.csv')\n",
        "    print(f\"‚úÖ Ozon –ø—Ä–æ–¥–∞–∂–∏: {ozon_sales.shape}\")\n",
        "except:\n",
        "    try:\n",
        "        ozon_sales = load_data('ozon_sales.xlsx')\n",
        "        print(f\"‚úÖ Ozon –ø—Ä–æ–¥–∞–∂–∏: {ozon_sales.shape}\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è –§–∞–π–ª ozon_sales –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö\")\n",
        "        dates = pd.date_range('2024-03-26', '2025-12-20', freq='D')\n",
        "        ozon_sales = pd.DataFrame({\n",
        "            '–î–∞—Ç–∞': dates,\n",
        "            'solo-code': 'SKU001',\n",
        "            'SKU': 'SKU001',\n",
        "            '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.': np.random.poisson(8, len(dates))\n",
        "        })\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤\n",
        "try:\n",
        "    wb_stocks = load_data('wb_stocks.csv')\n",
        "    print(f\"‚úÖ WB –æ—Å—Ç–∞—Ç–∫–∏: {wb_stocks.shape}\")\n",
        "except:\n",
        "    try:\n",
        "        wb_stocks = load_data('wb_stocks.xlsx')\n",
        "        print(f\"‚úÖ WB –æ—Å—Ç–∞—Ç–∫–∏: {wb_stocks.shape}\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è –§–∞–π–ª wb_stocks –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö\")\n",
        "        dates = pd.date_range('2024-03-26', '2025-12-20', freq='D')\n",
        "        wb_stocks = pd.DataFrame({\n",
        "            '–î–∞—Ç–∞': dates,\n",
        "            '–°–∫–ª–∞–¥': 'WB',\n",
        "            'solo-code': 'SKU001',\n",
        "            'SKU': 'SKU001',\n",
        "            '–û—Å—Ç–∞—Ç–æ–∫': np.random.randint(50, 200, len(dates))\n",
        "        })\n",
        "\n",
        "try:\n",
        "    ozon_stocks = load_data('ozon_stocks.csv')\n",
        "    print(f\"‚úÖ Ozon –æ—Å—Ç–∞—Ç–∫–∏: {ozon_stocks.shape}\")\n",
        "except:\n",
        "    try:\n",
        "        ozon_stocks = load_data('ozon_stocks.xlsx')\n",
        "        print(f\"‚úÖ Ozon –æ—Å—Ç–∞—Ç–∫–∏: {ozon_stocks.shape}\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è –§–∞–π–ª ozon_stocks –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö\")\n",
        "        dates = pd.date_range('2024-03-26', '2025-12-20', freq='D')\n",
        "        ozon_stocks = pd.DataFrame({\n",
        "            '–î–∞—Ç–∞': dates,\n",
        "            '–°–∫–ª–∞–¥': 'Ozon',\n",
        "            'solo-code': 'SKU001',\n",
        "            'SKU': 'SKU001',\n",
        "            '–û—Å—Ç–∞—Ç–æ–∫': np.random.randint(40, 150, len(dates))\n",
        "        })\n",
        "\n",
        "try:\n",
        "    our_stocks = load_data('our_stocks.csv')\n",
        "    print(f\"‚úÖ –ù–∞—à–∏ –æ—Å—Ç–∞—Ç–∫–∏: {our_stocks.shape}\")\n",
        "except:\n",
        "    try:\n",
        "        our_stocks = load_data('our_stocks.xlsx')\n",
        "        print(f\"‚úÖ –ù–∞—à–∏ –æ—Å—Ç–∞—Ç–∫–∏: {our_stocks.shape}\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è –§–∞–π–ª our_stocks –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö\")\n",
        "        dates = pd.date_range('2024-03-26', '2025-12-20', freq='D')\n",
        "        our_stocks = pd.DataFrame({\n",
        "            '–î–∞—Ç–∞': dates,\n",
        "            'solo-code': 'SKU001',\n",
        "            'SKU': 'SKU001',\n",
        "            '–û—Å—Ç–∞—Ç–æ–∫': np.random.randint(100, 300, len(dates))\n",
        "        })\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–æ–≤\n",
        "try:\n",
        "    withdraw = load_data('withdraw.csv')\n",
        "    print(f\"‚úÖ –¢–æ–≤–∞—Ä—ã –Ω–∞ –≤—ã–≤–æ–¥: {withdraw.shape}\")\n",
        "except:\n",
        "    try:\n",
        "        withdraw = load_data('withdraw.xlsx')\n",
        "        print(f\"‚úÖ –¢–æ–≤–∞—Ä—ã –Ω–∞ –≤—ã–≤–æ–¥: {withdraw.shape}\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è –§–∞–π–ª withdraw –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π\")\n",
        "        withdraw = pd.DataFrame(columns=['solo-code', 'SKU'])\n",
        "\n",
        "try:\n",
        "    defecture = load_data('defecture.csv')\n",
        "    print(f\"‚úÖ –î–µ—Ñ–µ–∫—Ç—É—Ä–∞: {defecture.shape}\")\n",
        "except:\n",
        "    try:\n",
        "        defecture = load_data('defecture.xlsx')\n",
        "        print(f\"‚úÖ –î–µ—Ñ–µ–∫—Ç—É—Ä–∞: {defecture.shape}\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è –§–∞–π–ª defecture –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π\")\n",
        "        defecture = pd.DataFrame(columns=['solo-code', 'SKU', '–î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –¥–µ—Ñ–µ–∫—Ç—É—Ä—ã'])\n",
        "\n",
        "try:\n",
        "    count_box = load_data('count_box.csv')\n",
        "    print(f\"‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤ –∫–æ—Ä–æ–±–µ: {count_box.shape}\")\n",
        "except:\n",
        "    try:\n",
        "        count_box = load_data('count_box.xlsx')\n",
        "        print(f\"‚úÖ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤ –∫–æ—Ä–æ–±–µ: {count_box.shape}\")\n",
        "    except:\n",
        "        print(\"‚ö†Ô∏è –§–∞–π–ª count_box –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä\")\n",
        "        count_box = pd.DataFrame({\n",
        "            'solo-code': ['SKU001'],\n",
        "            '–ö–æ–ª-–≤–æ': [12]\n",
        "        })\n",
        "\n",
        "print(\"\\nüìä –û–±–∑–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö:\")\n",
        "print(f\"WB –ø—Ä–æ–¥–∞–∂–∏: {wb_sales.shape}\")\n",
        "print(f\"Ozon –ø—Ä–æ–¥–∞–∂–∏: {ozon_sales.shape}\")\n",
        "print(f\"WB –æ—Å—Ç–∞—Ç–∫–∏: {wb_stocks.shape}\")\n",
        "print(f\"Ozon –æ—Å—Ç–∞—Ç–∫–∏: {ozon_stocks.shape}\")\n",
        "print(f\"–ù–∞—à–∏ –æ—Å—Ç–∞—Ç–∫–∏: {our_stocks.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏ –æ—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "\n",
        "### 2.1. –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –¥–∞—Ç –∫ datetime –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∫–∞–ª–µ–Ω–¥–∞—Ä—è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prepare_dates(df, date_col='–î–∞—Ç–∞'):\n",
        "    \"\"\"–ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –¥–∞—Ç –∫ datetime\"\"\"\n",
        "    df = df.copy()\n",
        "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
        "    return df\n",
        "\n",
        "# –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –¥–∞—Ç\n",
        "wb_sales = prepare_dates(wb_sales)\n",
        "ozon_sales = prepare_dates(ozon_sales)\n",
        "wb_stocks = prepare_dates(wb_stocks)\n",
        "ozon_stocks = prepare_dates(ozon_stocks)\n",
        "our_stocks = prepare_dates(our_stocks)\n",
        "\n",
        "if '–î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –¥–µ—Ñ–µ–∫—Ç—É—Ä—ã' in defecture.columns:\n",
        "    defecture['–î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –¥–µ—Ñ–µ–∫—Ç—É—Ä—ã'] = pd.to_datetime(defecture['–î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –¥–µ—Ñ–µ–∫—Ç—É—Ä—ã'], errors='coerce')\n",
        "\n",
        "# –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–±—â–µ–≥–æ –ø–µ—Ä–∏–æ–¥–∞\n",
        "all_dates = []\n",
        "for df in [wb_sales, ozon_sales, wb_stocks, ozon_stocks, our_stocks]:\n",
        "    if '–î–∞—Ç–∞' in df.columns:\n",
        "        all_dates.extend(df['–î–∞—Ç–∞'].dropna().tolist())\n",
        "\n",
        "min_date = min(all_dates) if all_dates else pd.Timestamp('2024-03-26')\n",
        "max_date = max(all_dates) if all_dates else pd.Timestamp('2025-12-20')\n",
        "\n",
        "print(f\"üìÖ –ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö: {min_date.date()} - {max_date.date()}\")\n",
        "print(f\"üìÖ –í—Å–µ–≥–æ –¥–Ω–µ–π: {(max_date - min_date).days + 1}\")\n",
        "\n",
        "# –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö solo-code\n",
        "all_solo_codes = set()\n",
        "for df in [wb_sales, ozon_sales, wb_stocks, ozon_stocks, our_stocks]:\n",
        "    if 'solo-code' in df.columns:\n",
        "        all_solo_codes.update(df['solo-code'].dropna().unique())\n",
        "\n",
        "all_solo_codes = sorted(list(all_solo_codes))\n",
        "print(f\"üì¶ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö solo-code: {len(all_solo_codes)}\")\n",
        "print(f\"üì¶ –ü—Ä–∏–º–µ—Ä—ã: {all_solo_codes[:5]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_full_calendar(min_date, max_date, solo_codes):\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∫–∞–ª–µ–Ω–¥–∞—Ä—è –¥–ª—è –≤—Å–µ—Ö solo-code\"\"\"\n",
        "    dates = pd.date_range(min_date, max_date, freq='D')\n",
        "    calendar = []\n",
        "    for solo_code in solo_codes:\n",
        "        for date in dates:\n",
        "            calendar.append({'–î–∞—Ç–∞': date, 'solo-code': solo_code})\n",
        "    return pd.DataFrame(calendar)\n",
        "\n",
        "# –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ –∫–∞–ª–µ–Ω–¥–∞—Ä—è\n",
        "full_calendar = create_full_calendar(min_date, max_date, all_solo_codes)\n",
        "print(f\"üìÖ –ü–æ–ª–Ω—ã–π –∫–∞–ª–µ–Ω–¥–∞—Ä—å: {full_calendar.shape}\")\n",
        "print(f\"üìÖ –ü—Ä–∏–º–µ—Ä –∫–∞–ª–µ–Ω–¥–∞—Ä—è:\")\n",
        "print(full_calendar.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2. –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–¥–∞–∂: –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –Ω—É–ª—è–º–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_sales(df, calendar, channel_name):\n",
        "    \"\"\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–¥–∞–∂ —Å –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ–º –ø—Ä–æ–ø—É—Å–∫–æ–≤\"\"\"\n",
        "    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –¥–∞—Ç–µ –∏ solo-code\n",
        "    sales_agg = df.groupby(['–î–∞—Ç–∞', 'solo-code'])['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'].sum().reset_index()\n",
        "    \n",
        "    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –∫–∞–ª–µ–Ω–¥–∞—Ä–µ–º\n",
        "    sales_full = calendar.merge(sales_agg, on=['–î–∞—Ç–∞', 'solo-code'], how='left')\n",
        "    sales_full['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'] = sales_full['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'].fillna(0)\n",
        "    sales_full['–ö–∞–Ω–∞–ª'] = channel_name\n",
        "    \n",
        "    return sales_full\n",
        "\n",
        "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–¥–∞–∂\n",
        "wb_sales_processed = process_sales(wb_sales, full_calendar, 'WB')\n",
        "ozon_sales_processed = process_sales(ozon_sales, full_calendar, 'Ozon')\n",
        "\n",
        "# –°—É–º–º–∞—Ä–Ω—ã–µ –ø—Ä–æ–¥–∞–∂–∏\n",
        "total_sales = wb_sales_processed.merge(\n",
        "    ozon_sales_processed[['–î–∞—Ç–∞', 'solo-code', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.']],\n",
        "    on=['–î–∞—Ç–∞', 'solo-code'],\n",
        "    how='left',\n",
        "    suffixes=('_wb', '_ozon')\n",
        ")\n",
        "total_sales['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'] = (\n",
        "    total_sales['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫._wb'].fillna(0) + \n",
        "    total_sales['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫._ozon'].fillna(0)\n",
        ")\n",
        "total_sales = total_sales[['–î–∞—Ç–∞', 'solo-code', '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.']].copy()\n",
        "total_sales['–ö–∞–Ω–∞–ª'] = 'Total'\n",
        "\n",
        "print(\"‚úÖ –ü—Ä–æ–¥–∞–∂–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã\")\n",
        "print(f\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º:\")\n",
        "print(f\"WB - –ø—Ä–æ–ø—É—Å–∫–æ–≤: {(wb_sales_processed['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'] == 0).sum() / len(wb_sales_processed) * 100:.1f}%\")\n",
        "print(f\"Ozon - –ø—Ä–æ–ø—É—Å–∫–æ–≤: {(ozon_sales_processed['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'] == 0).sum() / len(ozon_sales_processed) * 100:.1f}%\")\n",
        "print(f\"\\nüìä –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö WB:\")\n",
        "print(wb_sales_processed.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤: forward/backward fill\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_stocks(df, calendar, stock_name):\n",
        "    \"\"\"–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤ —Å forward/backward fill\"\"\"\n",
        "    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –¥–∞—Ç–µ –∏ solo-code (–µ—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–∫–ª–∞–¥–æ–≤ - —Å—É–º–º–∏—Ä—É–µ–º)\n",
        "    if '–°–∫–ª–∞–¥' in df.columns:\n",
        "        stocks_agg = df.groupby(['–î–∞—Ç–∞', 'solo-code'])['–û—Å—Ç–∞—Ç–æ–∫'].sum().reset_index()\n",
        "    else:\n",
        "        stocks_agg = df.groupby(['–î–∞—Ç–∞', 'solo-code'])['–û—Å—Ç–∞—Ç–æ–∫'].sum().reset_index()\n",
        "    \n",
        "    # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å –∫–∞–ª–µ–Ω–¥–∞—Ä–µ–º\n",
        "    stocks_full = calendar.merge(stocks_agg, on=['–î–∞—Ç–∞', 'solo-code'], how='left')\n",
        "    \n",
        "    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ solo-code –∏ –¥–∞—Ç–µ\n",
        "    stocks_full = stocks_full.sort_values(['solo-code', '–î–∞—Ç–∞'])\n",
        "    \n",
        "    # Forward fill, –∑–∞—Ç–µ–º backward fill –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä—è–¥–∞\n",
        "    stocks_full['–û—Å—Ç–∞—Ç–æ–∫'] = stocks_full.groupby('solo-code')['–û—Å—Ç–∞—Ç–æ–∫'].fillna(method='ffill')\n",
        "    stocks_full['–û—Å—Ç–∞—Ç–æ–∫'] = stocks_full.groupby('solo-code')['–û—Å—Ç–∞—Ç–æ–∫'].fillna(method='bfill')\n",
        "    stocks_full['–û—Å—Ç–∞—Ç–æ–∫'] = stocks_full['–û—Å—Ç–∞—Ç–æ–∫'].fillna(0)\n",
        "    \n",
        "    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "    stocks_full['–û—Å—Ç–∞—Ç–æ–∫'] = stocks_full['–û—Å—Ç–∞—Ç–æ–∫'].clip(lower=0)\n",
        "    \n",
        "    stocks_full['–¢–∏–ø_–æ—Å—Ç–∞—Ç–∫–∞'] = stock_name\n",
        "    \n",
        "    return stocks_full\n",
        "\n",
        "# –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞—Ç–∫–æ–≤\n",
        "wb_stocks_processed = process_stocks(wb_stocks, full_calendar, 'WB')\n",
        "ozon_stocks_processed = process_stocks(ozon_stocks, full_calendar, 'Ozon')\n",
        "our_stocks_processed = process_stocks(our_stocks, full_calendar, 'Our')\n",
        "\n",
        "print(\"‚úÖ –û—Å—Ç–∞—Ç–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã\")\n",
        "print(f\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –æ—Å—Ç–∞—Ç–∫–∞–º:\")\n",
        "print(f\"WB –æ—Å—Ç–∞—Ç–∫–∏ - —Å—Ä–µ–¥–Ω–µ–µ: {wb_stocks_processed['–û—Å—Ç–∞—Ç–æ–∫'].mean():.1f}\")\n",
        "print(f\"Ozon –æ—Å—Ç–∞—Ç–∫–∏ - —Å—Ä–µ–¥–Ω–µ–µ: {ozon_stocks_processed['–û—Å—Ç–∞—Ç–æ–∫'].mean():.1f}\")\n",
        "print(f\"–ù–∞—à–∏ –æ—Å—Ç–∞—Ç–∫–∏ - —Å—Ä–µ–¥–Ω–µ–µ: {our_stocks_processed['–û—Å—Ç–∞—Ç–æ–∫'].mean():.1f}\")\n",
        "print(f\"\\nüìä –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –æ—Å—Ç–∞—Ç–∫–æ–≤:\")\n",
        "print(wb_stocks_processed.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4. –°–≤–æ–¥–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –¥–∞–Ω–Ω—ã–º\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"üìä –°–í–û–î–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –î–ê–ù–ù–´–ú\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nüì¶ –ü—Ä–æ–¥–∞–∂–∏:\")\n",
        "print(f\"  - WB: {len(wb_sales_processed)} –∑–∞–ø–∏—Å–µ–π, {wb_sales_processed['solo-code'].nunique()} —Ç–æ–≤–∞—Ä–æ–≤\")\n",
        "print(f\"  - Ozon: {len(ozon_sales_processed)} –∑–∞–ø–∏—Å–µ–π, {ozon_sales_processed['solo-code'].nunique()} —Ç–æ–≤–∞—Ä–æ–≤\")\n",
        "print(f\"  - Total: {len(total_sales)} –∑–∞–ø–∏—Å–µ–π, {total_sales['solo-code'].nunique()} —Ç–æ–≤–∞—Ä–æ–≤\")\n",
        "\n",
        "print(f\"\\nüè¨ –û—Å—Ç–∞—Ç–∫–∏:\")\n",
        "print(f\"  - WB: {len(wb_stocks_processed)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "print(f\"  - Ozon: {len(ozon_stocks_processed)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "print(f\"  - –ù–∞—à–∏: {len(our_stocks_processed)} –∑–∞–ø–∏—Å–µ–π\")\n",
        "\n",
        "print(f\"\\nüìà –ü—Ä–æ–ø—É—Å–∫–∏ –≤ –ø—Ä–æ–¥–∞–∂–∞—Ö (–Ω—É–ª–µ–≤—ã–µ –¥–Ω–∏):\")\n",
        "for solo_code in all_solo_codes[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5\n",
        "    wb_data = wb_sales_processed[wb_sales_processed['solo-code'] == solo_code]\n",
        "    ozon_data = ozon_sales_processed[ozon_sales_processed['solo-code'] == solo_code]\n",
        "    total_data = total_sales[total_sales['solo-code'] == solo_code]\n",
        "    \n",
        "    wb_zeros = (wb_data['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'] == 0).sum() / len(wb_data) * 100\n",
        "    ozon_zeros = (ozon_data['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'] == 0).sum() / len(ozon_data) * 100\n",
        "    total_zeros = (total_data['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'] == 0).sum() / len(total_data) * 100\n",
        "    \n",
        "    print(f\"  {solo_code}: WB={wb_zeros:.1f}%, Ozon={ozon_zeros:.1f}%, Total={total_zeros:.1f}%\")\n",
        "\n",
        "print(\"\\n‚úÖ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. EDA –∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–ø—Ä–æ—Å–∞\n",
        "\n",
        "–î–ª—è –∫–∞–∂–¥–æ–≥–æ solo-code –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º:\n",
        "- –ì—Ä–∞—Ñ–∏–∫ –ø—Ä–æ–¥–∞–∂\n",
        "- –î–æ–ª—è –Ω—É–ª–µ–≤—ã—Ö –¥–Ω–µ–π\n",
        "- –°—Ä–µ–¥–Ω–∏–π —Å–ø—Ä–æ—Å\n",
        "- –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏ (CV)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_demand_metrics(sales_data, solo_code):\n",
        "    \"\"\"–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ —Å–ø—Ä–æ—Å–∞ –¥–ª—è —Ç–æ–≤–∞—Ä–∞\"\"\"\n",
        "    data = sales_data[sales_data['solo-code'] == solo_code].copy()\n",
        "    data = data.sort_values('–î–∞—Ç–∞')\n",
        "    \n",
        "    demand = data['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'].values\n",
        "    \n",
        "    metrics = {\n",
        "        'solo-code': solo_code,\n",
        "        'mean_demand': np.mean(demand),\n",
        "        'std_demand': np.std(demand),\n",
        "        'cv': np.std(demand) / (np.mean(demand) + 1e-6),  # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏\n",
        "        'zero_ratio': (demand == 0).sum() / len(demand),\n",
        "        'total_demand': np.sum(demand),\n",
        "        'max_demand': np.max(demand),\n",
        "        'min_demand': np.min(demand)\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –¥–ª—è –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤\n",
        "demand_metrics = []\n",
        "for solo_code in all_solo_codes:\n",
        "    metrics = calculate_demand_metrics(total_sales, solo_code)\n",
        "    demand_metrics.append(metrics)\n",
        "\n",
        "demand_df = pd.DataFrame(demand_metrics)\n",
        "\n",
        "print(\"üìä –ú–µ—Ç—Ä–∏–∫–∏ —Å–ø—Ä–æ—Å–∞ –ø–æ —Ç–æ–≤–∞—Ä–∞–º:\")\n",
        "print(demand_df.head(10))\n",
        "print(f\"\\nüìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
        "print(demand_df.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def classify_demand(zero_ratio, cv):\n",
        "    \"\"\"–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–∞ —Å–ø—Ä–æ—Å–∞\"\"\"\n",
        "    if zero_ratio > 0.5:\n",
        "        return 'intermittent'  # –ü—Ä–µ—Ä—ã–≤–∏—Å—Ç—ã–π —Å–ø—Ä–æ—Å (>50% –Ω—É–ª–µ–π)\n",
        "    elif cv > 1.0:\n",
        "        return 'seasonal'  # –°–µ–∑–æ–Ω–Ω—ã–π/–Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–π (–≤—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å)\n",
        "    else:\n",
        "        return 'stable'  # –°—Ç–∞–±–∏–ª—å–Ω—ã–π —Å–ø—Ä–æ—Å\n",
        "\n",
        "# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–æ–≤–∞—Ä–æ–≤\n",
        "demand_df['demand_type'] = demand_df.apply(\n",
        "    lambda x: classify_demand(x['zero_ratio'], x['cv']), \n",
        "    axis=1\n",
        ")\n",
        "\n",
        "print(\"üìä –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–ø—Ä–æ—Å–∞:\")\n",
        "print(demand_df['demand_type'].value_counts())\n",
        "print(f\"\\nüìä –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º:\")\n",
        "print(demand_df.groupby('demand_type').agg({\n",
        "    'mean_demand': 'mean',\n",
        "    'cv': 'mean',\n",
        "    'zero_ratio': 'mean'\n",
        "}).round(2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–æ–≤–∞—Ä–∞\n",
        "example_solo_code = all_solo_codes[0]\n",
        "example_data = total_sales[total_sales['solo-code'] == example_solo_code].sort_values('–î–∞—Ç–∞')\n",
        "\n",
        "fig, axes = plt.subplots(2, 1, figsize=(14, 10))\n",
        "\n",
        "# –ì—Ä–∞—Ñ–∏–∫ –ø—Ä–æ–¥–∞–∂\n",
        "axes[0].plot(example_data['–î–∞—Ç–∞'], example_data['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'], linewidth=1.5)\n",
        "axes[0].set_title(f'–ü—Ä–æ–¥–∞–∂–∏ –ø–æ –¥–Ω—è–º: {example_solo_code}', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('–î–∞—Ç–∞')\n",
        "axes[0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫–æ–≤–æ–∫')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "# –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø—Ä–æ—Å–∞\n",
        "axes[1].hist(example_data['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'], bins=30, edgecolor='black', alpha=0.7)\n",
        "axes[1].set_title(f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–ø—Ä–æ—Å–∞: {example_solo_code}', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫–æ–≤–æ–∫')\n",
        "axes[1].set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞\n",
        "example_metrics = demand_df[demand_df['solo-code'] == example_solo_code].iloc[0]\n",
        "print(f\"\\nüìä –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è {example_solo_code}:\")\n",
        "print(f\"  –¢–∏–ø —Å–ø—Ä–æ—Å–∞: {example_metrics['demand_type']}\")\n",
        "print(f\"  –°—Ä–µ–¥–Ω–∏–π —Å–ø—Ä–æ—Å: {example_metrics['mean_demand']:.2f}\")\n",
        "print(f\"  CV: {example_metrics['cv']:.2f}\")\n",
        "print(f\"  –î–æ–ª—è –Ω—É–ª–µ–π: {example_metrics['zero_ratio']:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "\n",
        "–°–æ–∑–¥–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –¥–∞–Ω–Ω—ã—Ö:\n",
        "- –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–¥–µ–Ω—å –Ω–µ–¥–µ–ª–∏, –º–µ—Å—è—Ü, –∫–≤–∞—Ä—Ç–∞–ª –∏ —Ç.–¥.)\n",
        "- –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (sin/cos)\n",
        "- –ü—Ä–∞–∑–¥–Ω–∏–∫–∏ –†–§ —Å –æ–∫–Ω–∞–º–∏ –≤–ª–∏—è–Ω–∏—è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_calendar_features(df):\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "    df['day_of_week'] = df['–î–∞—Ç–∞'].dt.dayofweek  # 0=–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫, 6=–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ\n",
        "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
        "    df['week_of_year'] = df['–î–∞—Ç–∞'].dt.isocalendar().week\n",
        "    df['month'] = df['–î–∞—Ç–∞'].dt.month\n",
        "    df['quarter'] = df['–î–∞—Ç–∞'].dt.quarter\n",
        "    df['year'] = df['–î–∞—Ç–∞'].dt.year\n",
        "    df['day_of_month'] = df['–î–∞—Ç–∞'].dt.day\n",
        "    df['day_of_year'] = df['–î–∞—Ç–∞'].dt.dayofyear\n",
        "    \n",
        "    # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –º–µ—Å—è—Ü–∞ (12 –º–µ—Å—è—Ü–µ–≤)\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "    \n",
        "    # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–µ–¥–µ–ª–∏ (52 –Ω–µ–¥–µ–ª–∏)\n",
        "    df['week_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\n",
        "    df['week_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\n",
        "    \n",
        "    # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –¥–Ω—è –Ω–µ–¥–µ–ª–∏ (7 –¥–Ω–µ–π)\n",
        "    df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
        "    df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
        "    \n",
        "    return df\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ –ø—Ä–æ–¥–∞–∂–∞–º\n",
        "total_sales = create_calendar_features(total_sales)\n",
        "wb_sales_processed = create_calendar_features(wb_sales_processed)\n",
        "ozon_sales_processed = create_calendar_features(ozon_sales_processed)\n",
        "\n",
        "print(\"‚úÖ –ë–∞–∑–æ–≤—ã–µ –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ–∑–¥–∞–Ω—ã\")\n",
        "print(f\"\\nüìä –ü—Ä–∏–º–µ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
        "print(total_sales[['–î–∞—Ç–∞', 'day_of_week', 'is_weekend', 'month', 'quarter', \n",
        "                   'month_sin', 'month_cos']].head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_holiday_features(df):\n",
        "    \"\"\"–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø—Ä–∞–∑–¥–Ω–∏–∫–æ–≤ –†–§ —Å –æ–∫–Ω–∞–º–∏ –≤–ª–∏—è–Ω–∏—è\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    # –ù–æ–≤—ã–π –≥–æ–¥: 15.12 - 10.01\n",
        "    df['is_new_year_period'] = 0\n",
        "    df.loc[\n",
        "        ((df['–î–∞—Ç–∞'].dt.month == 12) & (df['–î–∞—Ç–∞'].dt.day >= 15)) |\n",
        "        ((df['–î–∞—Ç–∞'].dt.month == 1) & (df['–î–∞—Ç–∞'].dt.day <= 10)),\n",
        "        'is_new_year_period'\n",
        "    ] = 1\n",
        "    \n",
        "    # 14 —Ñ–µ–≤—Ä–∞–ª—è: ¬±7 –¥–Ω–µ–π\n",
        "    df['is_feb_14_window'] = 0\n",
        "    for year in df['–î–∞—Ç–∞'].dt.year.unique():\n",
        "        feb_14 = pd.Timestamp(f'{year}-02-14')\n",
        "        mask = (df['–î–∞—Ç–∞'] >= feb_14 - timedelta(days=7)) & (df['–î–∞—Ç–∞'] <= feb_14 + timedelta(days=7))\n",
        "        df.loc[mask, 'is_feb_14_window'] = 1\n",
        "    \n",
        "    # 23 —Ñ–µ–≤—Ä–∞–ª—è: ¬±7 –¥–Ω–µ–π\n",
        "    df['is_feb_23_window'] = 0\n",
        "    for year in df['–î–∞—Ç–∞'].dt.year.unique():\n",
        "        feb_23 = pd.Timestamp(f'{year}-02-23')\n",
        "        mask = (df['–î–∞—Ç–∞'] >= feb_23 - timedelta(days=7)) & (df['–î–∞—Ç–∞'] <= feb_23 + timedelta(days=7))\n",
        "        df.loc[mask, 'is_feb_23_window'] = 1\n",
        "    \n",
        "    # 8 –º–∞—Ä—Ç–∞: ¬±7 –¥–Ω–µ–π\n",
        "    df['is_mar_8_window'] = 0\n",
        "    for year in df['–î–∞—Ç–∞'].dt.year.unique():\n",
        "        mar_8 = pd.Timestamp(f'{year}-03-08')\n",
        "        mask = (df['–î–∞—Ç–∞'] >= mar_8 - timedelta(days=7)) & (df['–î–∞—Ç–∞'] <= mar_8 + timedelta(days=7))\n",
        "        df.loc[mask, 'is_mar_8_window'] = 1\n",
        "    \n",
        "    # 1 —Å–µ–Ω—Ç—è–±—Ä—è: ¬±7 –¥–Ω–µ–π\n",
        "    df['is_sep_1_window'] = 0\n",
        "    for year in df['–î–∞—Ç–∞'].dt.year.unique():\n",
        "        sep_1 = pd.Timestamp(f'{year}-09-01')\n",
        "        mask = (df['–î–∞—Ç–∞'] >= sep_1 - timedelta(days=7)) & (df['–î–∞—Ç–∞'] <= sep_1 + timedelta(days=7))\n",
        "        df.loc[mask, 'is_sep_1_window'] = 1\n",
        "    \n",
        "    # –ß–µ—Ä–Ω–∞—è –ø—è—Ç–Ω–∏—Ü–∞: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –Ω–µ–¥–µ–ª–∏ –Ω–æ—è–±—Ä—è\n",
        "    df['is_black_friday_period'] = 0\n",
        "    df.loc[\n",
        "        (df['–î–∞—Ç–∞'].dt.month == 11) & (df['–î–∞—Ç–∞'].dt.day >= 15),\n",
        "        'is_black_friday_period'\n",
        "    ] = 1\n",
        "    \n",
        "    return df\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º –ø—Ä–∞–∑–¥–Ω–∏—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "total_sales = create_holiday_features(total_sales)\n",
        "wb_sales_processed = create_holiday_features(wb_sales_processed)\n",
        "ozon_sales_processed = create_holiday_features(ozon_sales_processed)\n",
        "\n",
        "print(\"‚úÖ –ü—Ä–∞–∑–¥–Ω–∏—á–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ–∑–¥–∞–Ω—ã\")\n",
        "print(f\"\\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø—Ä–∞–∑–¥–Ω–∏–∫–∞–º:\")\n",
        "holiday_cols = ['is_new_year_period', 'is_feb_14_window', 'is_feb_23_window', \n",
        "                'is_mar_8_window', 'is_sep_1_window', 'is_black_friday_period']\n",
        "print(total_sales[holiday_cols].sum())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–∞–∑–¥–Ω–∏–∫–æ–≤ —Å –ø—Ä–æ–¥–∞–∂–∞–º–∏\n",
        "holiday_corr = []\n",
        "for solo_code in all_solo_codes[:10]:  # –ü–µ—Ä–≤—ã–µ 10 —Ç–æ–≤–∞—Ä–æ–≤\n",
        "    data = total_sales[total_sales['solo-code'] == solo_code]\n",
        "    for holiday in holiday_cols:\n",
        "        if data[holiday].sum() > 0:  # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–∞–∑–¥–Ω–∏—á–Ω—ã–µ –¥–Ω–∏\n",
        "            corr = data['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'].corr(data[holiday])\n",
        "            holiday_corr.append({\n",
        "                'solo-code': solo_code,\n",
        "                'holiday': holiday,\n",
        "                'correlation': corr\n",
        "            })\n",
        "\n",
        "if holiday_corr:\n",
        "    holiday_corr_df = pd.DataFrame(holiday_corr)\n",
        "    print(\"\\nüìä –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –ø—Ä–∞–∑–¥–Ω–∏–∫–æ–≤ —Å –ø—Ä–æ–¥–∞–∂–∞–º–∏ (–ø—Ä–∏–º–µ—Ä—ã):\")\n",
        "    print(holiday_corr_df.groupby('holiday')['correlation'].mean().sort_values(ascending=False))\n",
        "\n",
        "# –ü—Ä–∏–º–µ—Ä –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "print(\"\\nüìä –ü—Ä–∏–º–µ—Ä –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
        "example_calendar = total_sales[\n",
        "    ['–î–∞—Ç–∞', 'day_of_week', 'is_weekend', 'month', 'is_new_year_period', \n",
        "     'is_feb_14_window', 'is_mar_8_window', 'is_black_friday_period']\n",
        "].head(20)\n",
        "print(example_calendar)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∏–∑–Ω–µ—Å-–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π\n",
        "\n",
        "### 5.1. –¢–æ–≤–∞—Ä—ã –Ω–∞ –≤—ã–≤–æ–¥ (withdraw)\n",
        "### 5.2. –î–µ—Ñ–µ–∫—Ç—É—Ä–∞ (defecture)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_withdraw_constraint(sales_df, stocks_wb, stocks_ozon, stocks_our, withdraw_df):\n",
        "    \"\"\"–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –¥–ª—è —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ –≤—ã–≤–æ–¥\"\"\"\n",
        "    if len(withdraw_df) == 0:\n",
        "        return sales_df.copy(), stocks_wb.copy(), stocks_ozon.copy(), stocks_our.copy()\n",
        "    \n",
        "    withdraw_codes = set(withdraw_df['solo-code'].unique())\n",
        "    \n",
        "    sales_df = sales_df.copy()\n",
        "    stocks_wb = stocks_wb.copy()\n",
        "    stocks_ozon = stocks_ozon.copy()\n",
        "    stocks_our = stocks_our.copy()\n",
        "    \n",
        "    # –î–ª—è —Ç–æ–≤–∞—Ä–æ–≤ –Ω–∞ –≤—ã–≤–æ–¥: –ø—Ä–æ–¥–∞–∂–∏ = 0, –æ—Å—Ç–∞—Ç–∫–∏ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è\n",
        "    for solo_code in withdraw_codes:\n",
        "        # –ü—Ä–æ–¥–∞–∂–∏ = 0\n",
        "        sales_df.loc[sales_df['solo-code'] == solo_code, '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'] = 0\n",
        "        \n",
        "        # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—Å—Ç–∞—Ç–∫–æ–≤ –¥–æ –Ω—É–ª—è\n",
        "        for stocks_df in [stocks_wb, stocks_ozon, stocks_our]:\n",
        "            mask = stocks_df['solo-code'] == solo_code\n",
        "            if mask.sum() > 0:\n",
        "                total_stock = stocks_df.loc[mask, '–û—Å—Ç–∞—Ç–æ–∫'].iloc[-1]  # –ü–æ—Å–ª–µ–¥–Ω–∏–π –æ—Å—Ç–∞—Ç–æ–∫\n",
        "                days_left = len(stocks_df.loc[mask])\n",
        "                if days_left > 0:\n",
        "                    daily_decrease = total_stock / days_left\n",
        "                    stocks_df.loc[mask, '–û—Å—Ç–∞—Ç–æ–∫'] = np.maximum(\n",
        "                        0, \n",
        "                        total_stock - np.arange(days_left) * daily_decrease\n",
        "                    )\n",
        "    \n",
        "    return sales_df, stocks_wb, stocks_ozon, stocks_our\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ –≤—ã–≤–æ–¥\n",
        "total_sales, wb_stocks_processed, ozon_stocks_processed, our_stocks_processed = apply_withdraw_constraint(\n",
        "    total_sales, wb_stocks_processed, ozon_stocks_processed, our_stocks_processed, withdraw\n",
        ")\n",
        "\n",
        "print(\"‚úÖ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ –≤—ã–≤–æ–¥ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã\")\n",
        "if len(withdraw) > 0:\n",
        "    print(f\"üì¶ –¢–æ–≤–∞—Ä–æ–≤ –Ω–∞ –≤—ã–≤–æ–¥: {len(withdraw)}\")\n",
        "    print(f\"üì¶ –ö–æ–¥—ã: {withdraw['solo-code'].unique()[:5]}\")\n",
        "else:\n",
        "    print(\"üì¶ –¢–æ–≤–∞—Ä–æ–≤ –Ω–∞ –≤—ã–≤–æ–¥ –Ω–µ—Ç\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_defecture_constraint(sales_df, defecture_df):\n",
        "    \"\"\"–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –¥–µ—Ñ–µ–∫—Ç—É—Ä—ã\"\"\"\n",
        "    if len(defecture_df) == 0:\n",
        "        return sales_df.copy()\n",
        "    \n",
        "    sales_df = sales_df.copy()\n",
        "    \n",
        "    for idx, row in defecture_df.iterrows():\n",
        "        solo_code = row['solo-code']\n",
        "        end_date = row['–î–∞—Ç–∞ –æ–∫–æ–Ω—á–∞–Ω–∏—è –¥–µ—Ñ–µ–∫—Ç—É—Ä—ã']\n",
        "        \n",
        "        if pd.isna(end_date):\n",
        "            continue\n",
        "        \n",
        "        # –í –ø–µ—Ä–∏–æ–¥ –¥–æ –æ–∫–æ–Ω—á–∞–Ω–∏—è –¥–µ—Ñ–µ–∫—Ç—É—Ä—ã: –ø—Ä–æ–¥–∞–∂–∏ = 0\n",
        "        mask = (sales_df['solo-code'] == solo_code) & (sales_df['–î–∞—Ç–∞'] <= end_date)\n",
        "        sales_df.loc[mask, '–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'] = 0\n",
        "    \n",
        "    return sales_df\n",
        "\n",
        "# –ü—Ä–∏–º–µ–Ω—è–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–µ—Ñ–µ–∫—Ç—É—Ä—ã\n",
        "total_sales = apply_defecture_constraint(total_sales, defecture)\n",
        "\n",
        "print(\"‚úÖ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –¥–µ—Ñ–µ–∫—Ç—É—Ä—ã –ø—Ä–∏–º–µ–Ω–µ–Ω—ã\")\n",
        "if len(defecture) > 0:\n",
        "    print(f\"‚ö†Ô∏è –¢–æ–≤–∞—Ä–æ–≤ –≤ –¥–µ—Ñ–µ–∫—Ç—É—Ä–µ: {len(defecture)}\")\n",
        "    print(f\"‚ö†Ô∏è –ü—Ä–∏–º–µ—Ä—ã:\")\n",
        "    print(defecture.head())\n",
        "else:\n",
        "    print(\"‚úÖ –î–µ—Ñ–µ–∫—Ç—É—Ä—ã –Ω–µ—Ç\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. –ú–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "\n",
        "–†–µ–∞–ª–∏–∑—É–µ–º –Ω–∞–±–æ—Ä –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–∞–∂–¥–æ–≥–æ solo-code:\n",
        "- Baseline (Naive, Moving Average, Seasonal Naive)\n",
        "- –î–ª—è intermittent demand (Croston, SBA, TSB)\n",
        "- –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ (Linear Regression, ARIMA, SARIMA, SARIMAX, Prophet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline –º–æ–¥–µ–ª–∏\n",
        "\n",
        "def naive_forecast(data, horizon):\n",
        "    \"\"\"Naive: –ø–æ—Å–ª–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\"\"\"\n",
        "    last_value = data.iloc[-1]\n",
        "    return [last_value] * horizon\n",
        "\n",
        "def moving_average_forecast(data, window=7, horizon=30):\n",
        "    \"\"\"Moving Average\"\"\"\n",
        "    ma_value = data[-window:].mean()\n",
        "    return [ma_value] * horizon\n",
        "\n",
        "def seasonal_naive_forecast(data, season_length=7, horizon=30):\n",
        "    \"\"\"Seasonal Naive: –∑–Ω–∞—á–µ–Ω–∏–µ —Å –ø—Ä–æ—à–ª–æ–≥–æ —Å–µ–∑–æ–Ω–∞\"\"\"\n",
        "    if len(data) < season_length:\n",
        "        return [data.mean()] * horizon\n",
        "    seasonal_values = data[-season_length:].values\n",
        "    forecast = []\n",
        "    for i in range(horizon):\n",
        "        forecast.append(seasonal_values[i % season_length])\n",
        "    return forecast\n",
        "\n",
        "print(\"‚úÖ Baseline –º–æ–¥–µ–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ú–æ–¥–µ–ª–∏ –¥–ª—è intermittent demand\n",
        "\n",
        "def croston_forecast(data, alpha=0.1, horizon=30):\n",
        "    \"\"\"Croston's method –¥–ª—è –ø—Ä–µ—Ä—ã–≤–∏—Å—Ç–æ–≥–æ —Å–ø—Ä–æ—Å–∞\"\"\"\n",
        "    if len(data) < 2:\n",
        "        return [data.mean()] * horizon\n",
        "    \n",
        "    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –Ω–µ–Ω—É–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã\n",
        "    non_zero = data[data > 0]\n",
        "    if len(non_zero) < 2:\n",
        "        return [non_zero.mean() if len(non_zero) > 0 else 0] * horizon\n",
        "    \n",
        "    # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∑–∞–∫–∞–∑–∞\n",
        "    sizes = non_zero.values\n",
        "    avg_size = sizes[0]\n",
        "    for size in sizes[1:]:\n",
        "        avg_size = alpha * size + (1 - alpha) * avg_size\n",
        "    \n",
        "    # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª–∞\n",
        "    intervals = []\n",
        "    last_idx = 0\n",
        "    for i in range(1, len(data)):\n",
        "        if data.iloc[i] > 0:\n",
        "            intervals.append(i - last_idx)\n",
        "            last_idx = i\n",
        "    \n",
        "    if len(intervals) < 2:\n",
        "        avg_interval = len(data) / max(len(non_zero), 1)\n",
        "    else:\n",
        "        avg_interval = intervals[0]\n",
        "        for interval in intervals[1:]:\n",
        "            avg_interval = alpha * interval + (1 - alpha) * avg_interval\n",
        "    \n",
        "    # –ü—Ä–æ–≥–Ω–æ–∑ = —Å—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä / —Å—Ä–µ–¥–Ω–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª\n",
        "    forecast_demand = avg_size / max(avg_interval, 1)\n",
        "    \n",
        "    return [forecast_demand] * horizon\n",
        "\n",
        "def sba_forecast(data, alpha=0.1, horizon=30):\n",
        "    \"\"\"Syntetos-Boylan Approximation\"\"\"\n",
        "    croston_fc = croston_forecast(data, alpha, horizon)\n",
        "    # SBA = Croston * (1 - alpha/2)\n",
        "    return [x * (1 - alpha / 2) for x in croston_fc]\n",
        "\n",
        "def tsb_forecast(data, alpha=0.1, beta=0.1, horizon=30):\n",
        "    \"\"\"Teunter-Syntetos-Babai method\"\"\"\n",
        "    if len(data) < 2:\n",
        "        return [data.mean()] * horizon\n",
        "    \n",
        "    non_zero = data[data > 0]\n",
        "    if len(non_zero) < 2:\n",
        "        return [non_zero.mean() if len(non_zero) > 0 else 0] * horizon\n",
        "    \n",
        "    # –†–∞–∑–º–µ—Ä –∑–∞–∫–∞–∑–∞\n",
        "    sizes = non_zero.values\n",
        "    avg_size = sizes[0]\n",
        "    for size in sizes[1:]:\n",
        "        avg_size = alpha * size + (1 - alpha) * avg_size\n",
        "    \n",
        "    # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –Ω–µ–Ω—É–ª–µ–≤–æ–≥–æ —Å–ø—Ä–æ—Å–∞\n",
        "    prob = (data > 0).sum() / len(data)\n",
        "    last_prob = prob\n",
        "    for i in range(1, len(data)):\n",
        "        if data.iloc[i] > 0:\n",
        "            last_prob = beta * 1 + (1 - beta) * last_prob\n",
        "        else:\n",
        "            last_prob = beta * 0 + (1 - beta) * last_prob\n",
        "    \n",
        "    forecast_demand = avg_size * last_prob\n",
        "    return [forecast_demand] * horizon\n",
        "\n",
        "print(\"‚úÖ –ú–æ–¥–µ–ª–∏ –¥–ª—è intermittent demand –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –º–æ–¥–µ–ª–∏\n",
        "\n",
        "def linear_regression_forecast(train_data, test_features, calendar_features):\n",
        "    \"\"\"Linear Regression —Å –ª–∞–≥–∞–º–∏ –∏ –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏\"\"\"\n",
        "    if len(train_data) < 10:\n",
        "        return [train_data.mean()] * len(test_features)\n",
        "    \n",
        "    # –°–æ–∑–¥–∞–Ω–∏–µ –ª–∞–≥–æ–≤\n",
        "    lags = [1, 7, 14, 30]\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    \n",
        "    for i in range(max(lags), len(train_data)):\n",
        "        features = []\n",
        "        # –õ–∞–≥–∏\n",
        "        for lag in lags:\n",
        "            if i - lag >= 0:\n",
        "                features.append(train_data.iloc[i - lag])\n",
        "            else:\n",
        "                features.append(0)\n",
        "        # –ö–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "        if calendar_features is not None and i < len(calendar_features):\n",
        "            cal_features = calendar_features.iloc[i][\n",
        "                ['day_of_week', 'is_weekend', 'month', 'quarter', \n",
        "                 'month_sin', 'month_cos', 'dow_sin', 'dow_cos'] +\n",
        "                ['is_new_year_period', 'is_feb_14_window', 'is_feb_23_window', \n",
        "                 'is_mar_8_window', 'is_black_friday_period']\n",
        "            ].values\n",
        "            features.extend(cal_features)\n",
        "        else:\n",
        "            features.extend([0] * 13)\n",
        "        \n",
        "        X_train.append(features)\n",
        "        y_train.append(train_data.iloc[i])\n",
        "    \n",
        "    if len(X_train) == 0:\n",
        "        return [train_data.mean()] * len(test_features)\n",
        "    \n",
        "    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "    try:\n",
        "        model = LinearRegression()\n",
        "        model.fit(X_train, y_train)\n",
        "        \n",
        "        # –ü—Ä–æ–≥–Ω–æ–∑\n",
        "        forecast = []\n",
        "        last_values = train_data[-max(lags):].values.tolist()\n",
        "        \n",
        "        for i, test_row in enumerate(test_features.iterrows()):\n",
        "            features = []\n",
        "            # –õ–∞–≥–∏ –∏–∑ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "            for lag in lags:\n",
        "                if len(last_values) >= lag:\n",
        "                    features.append(last_values[-lag])\n",
        "                else:\n",
        "                    features.append(0)\n",
        "            # –ö–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "            cal_features = test_row[1][\n",
        "                ['day_of_week', 'is_weekend', 'month', 'quarter',\n",
        "                 'month_sin', 'month_cos', 'dow_sin', 'dow_cos'] +\n",
        "                ['is_new_year_period', 'is_feb_14_window', 'is_feb_23_window',\n",
        "                 'is_mar_8_window', 'is_black_friday_period']\n",
        "            ].values\n",
        "            features.extend(cal_features)\n",
        "            \n",
        "            pred = model.predict([features])[0]\n",
        "            forecast.append(max(0, pred))  # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–µ –¥–æ–ø—É—Å–∫–∞–µ–º\n",
        "            last_values.append(pred)\n",
        "        return forecast\n",
        "    except:\n",
        "        return [train_data.mean()] * len(test_features)\n",
        "\n",
        "print(\"‚úÖ Linear Regression –º–æ–¥–µ–ª—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def arima_forecast(data, horizon=30, order=(1, 1, 1)):\n",
        "    \"\"\"ARIMA –º–æ–¥–µ–ª—å\"\"\"\n",
        "    if len(data) < 10:\n",
        "        return [data.mean()] * horizon\n",
        "    \n",
        "    try:\n",
        "        # –£–±–∏—Ä–∞–µ–º –Ω—É–ª–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "        data_clean = data.copy()\n",
        "        data_clean = data_clean.replace(0, 0.01)\n",
        "        \n",
        "        model = ARIMA(data_clean, order=order)\n",
        "        fitted_model = model.fit()\n",
        "        forecast = fitted_model.forecast(steps=horizon)\n",
        "        return [max(0, x) for x in forecast]\n",
        "    except:\n",
        "        return [data.mean()] * horizon\n",
        "\n",
        "def sarima_forecast(data, horizon=30, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7)):\n",
        "    \"\"\"SARIMA –º–æ–¥–µ–ª—å\"\"\"\n",
        "    if len(data) < 20:\n",
        "        return [data.mean()] * horizon\n",
        "    \n",
        "    try:\n",
        "        data_clean = data.copy()\n",
        "        data_clean = data_clean.replace(0, 0.01)\n",
        "        \n",
        "        model = SARIMAX(data_clean, order=order, seasonal_order=seasonal_order)\n",
        "        fitted_model = model.fit(disp=False)\n",
        "        forecast = fitted_model.forecast(steps=horizon)\n",
        "        return [max(0, x) for x in forecast]\n",
        "    except:\n",
        "        return [data.mean()] * horizon\n",
        "\n",
        "def sarimax_forecast(data, exog_train, exog_test, horizon=30):\n",
        "    \"\"\"SARIMAX —Å —ç–∫–∑–æ–≥–µ–Ω–Ω—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ (–∫–∞–ª–µ–Ω–¥–∞—Ä—å)\"\"\"\n",
        "    if len(data) < 20 or exog_train is None or exog_test is None:\n",
        "        return [data.mean()] * horizon\n",
        "    \n",
        "    try:\n",
        "        data_clean = data.copy()\n",
        "        data_clean = data_clean.replace(0, 0.01)\n",
        "        \n",
        "        # –í—ã–±–∏—Ä–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "        exog_cols = ['is_weekend', 'month_sin', 'month_cos', \n",
        "                     'is_new_year_period', 'is_black_friday_period']\n",
        "        exog_train_sel = exog_train[exog_cols] if all(c in exog_train.columns for c in exog_cols) else None\n",
        "        exog_test_sel = exog_test[exog_cols] if all(c in exog_test.columns for c in exog_cols) else None\n",
        "        \n",
        "        if exog_train_sel is None or exog_test_sel is None:\n",
        "            return sarima_forecast(data, horizon)\n",
        "        \n",
        "        model = SARIMAX(data_clean, exog=exog_train_sel, \n",
        "                       order=(1, 1, 1), seasonal_order=(1, 1, 1, 7))\n",
        "        fitted_model = model.fit(disp=False)\n",
        "        forecast = fitted_model.forecast(steps=horizon, exog=exog_test_sel)\n",
        "        return [max(0, x) for x in forecast]\n",
        "    except:\n",
        "        return [data.mean()] * horizon\n",
        "\n",
        "print(\"‚úÖ ARIMA/SARIMA/SARIMAX –º–æ–¥–µ–ª–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def prophet_forecast(data, dates, horizon_dates, holidays_df=None):\n",
        "    \"\"\"Prophet –º–æ–¥–µ–ª—å —Å –ø—Ä–∞–∑–¥–Ω–∏–∫–∞–º–∏\"\"\"\n",
        "    if len(data) < 30:\n",
        "        return [data.mean()] * len(horizon_dates)\n",
        "    \n",
        "    try:\n",
        "        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è Prophet\n",
        "        prophet_data = pd.DataFrame({\n",
        "            'ds': dates,\n",
        "            'y': data.values\n",
        "        })\n",
        "        \n",
        "        # –ü—Ä–∞–∑–¥–Ω–∏–∫–∏ –¥–ª—è Prophet\n",
        "        if holidays_df is None:\n",
        "            holidays_list = []\n",
        "            # –ù–æ–≤—ã–π –≥–æ–¥\n",
        "            for year in [2024, 2025]:\n",
        "                holidays_list.append({'holiday': 'new_year', 'ds': f'{year}-12-31', 'lower_window': -15, 'upper_window': 10})\n",
        "                holidays_list.append({'holiday': 'feb_14', 'ds': f'{year}-02-14', 'lower_window': -7, 'upper_window': 7})\n",
        "                holidays_list.append({'holiday': 'feb_23', 'ds': f'{year}-02-23', 'lower_window': -7, 'upper_window': 7})\n",
        "                holidays_list.append({'holiday': 'mar_8', 'ds': f'{year}-03-08', 'lower_window': -7, 'upper_window': 7})\n",
        "                holidays_list.append({'holiday': 'sep_1', 'ds': f'{year}-09-01', 'lower_window': -7, 'upper_window': 7})\n",
        "            \n",
        "            holidays_df = pd.DataFrame(holidays_list)\n",
        "            holidays_df['ds'] = pd.to_datetime(holidays_df['ds'])\n",
        "        \n",
        "        # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
        "        model = Prophet(holidays=holidays_df, yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=False)\n",
        "        model.fit(prophet_data)\n",
        "        \n",
        "        # –ü—Ä–æ–≥–Ω–æ–∑\n",
        "        future = pd.DataFrame({'ds': horizon_dates})\n",
        "        forecast = model.predict(future)\n",
        "        \n",
        "        return [max(0, x) for x in forecast['yhat'].values]\n",
        "    except Exception as e:\n",
        "        return [data.mean()] * len(horizon_dates)\n",
        "\n",
        "print(\"‚úÖ Prophet –º–æ–¥–µ–ª—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏\n",
        "\n",
        "Rolling backtesting —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏: MAE, RMSE, MAPE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_metrics(y_true, y_pred):\n",
        "    \"\"\"–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è\"\"\"\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    \n",
        "    # MAE\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    \n",
        "    # RMSE\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    \n",
        "    # MAPE (—É—Å—Ç–æ–π—á–∏–≤—ã–π –∫ –Ω—É–ª—è–º)\n",
        "    mask = y_true > 0\n",
        "    if mask.sum() > 0:\n",
        "        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100\n",
        "    else:\n",
        "        mape = 0\n",
        "    \n",
        "    return {'MAE': mae, 'RMSE': rmse, 'MAPE': mape}\n",
        "\n",
        "def rolling_backtest(data, calendar_data, models_dict, train_size=180, test_size=30, step=30):\n",
        "    \"\"\"Rolling backtesting –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π\"\"\"\n",
        "    results = []\n",
        "    \n",
        "    if len(data) < train_size + test_size:\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    for start_test in range(train_size, len(data) - test_size + 1, step):\n",
        "        train_data = data.iloc[:start_test]\n",
        "        test_data = data.iloc[start_test:start_test + test_size]\n",
        "        \n",
        "        train_calendar = calendar_data.iloc[:start_test] if calendar_data is not None else None\n",
        "        test_calendar = calendar_data.iloc[start_test:start_test + test_size] if calendar_data is not None else None\n",
        "        \n",
        "        for model_name, model_func in models_dict.items():\n",
        "            try:\n",
        "                if model_name == 'Linear Regression':\n",
        "                    forecast = model_func(train_data, test_calendar, train_calendar)\n",
        "                elif model_name == 'SARIMAX':\n",
        "                    forecast = model_func(train_data, train_calendar, test_calendar, len(test_data))\n",
        "                elif model_name == 'Prophet':\n",
        "                    train_dates = train_calendar['–î–∞—Ç–∞'] if train_calendar is not None else pd.date_range('2024-01-01', periods=len(train_data), freq='D')\n",
        "                    test_dates = test_calendar['–î–∞—Ç–∞'] if test_calendar is not None else pd.date_range('2024-01-01', periods=len(test_data), freq='D')\n",
        "                    forecast = model_func(train_data, train_dates, test_dates)\n",
        "                else:\n",
        "                    forecast = model_func(train_data, len(test_data))\n",
        "                \n",
        "                metrics = calculate_metrics(test_data.values, forecast[:len(test_data)])\n",
        "                metrics['model'] = model_name\n",
        "                metrics['fold'] = start_test\n",
        "                results.append(metrics)\n",
        "            except Exception as e:\n",
        "                continue\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "print(\"‚úÖ –§—É–Ω–∫—Ü–∏–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ —Ç–æ–≤–∞—Ä–∞\n",
        "example_solo_code = all_solo_codes[0]\n",
        "example_sales = total_sales[total_sales['solo-code'] == example_solo_code].sort_values('–î–∞—Ç–∞')\n",
        "example_calendar = example_sales.copy()\n",
        "\n",
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "sales_series = example_sales['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'].reset_index(drop=True)\n",
        "demand_type = demand_df[demand_df['solo-code'] == example_solo_code]['demand_type'].iloc[0]\n",
        "\n",
        "print(f\"üì¶ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è {example_solo_code}\")\n",
        "print(f\"üìä –¢–∏–ø —Å–ø—Ä–æ—Å–∞: {demand_type}\")\n",
        "print(f\"üìä –î–ª–∏–Ω–∞ —Ä—è–¥–∞: {len(sales_series)}\")\n",
        "\n",
        "# –í—ã–±–æ—Ä –º–æ–¥–µ–ª–µ–π –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Å–ø—Ä–æ—Å–∞\n",
        "models_to_test = {}\n",
        "\n",
        "# Baseline –≤—Å–µ–≥–¥–∞\n",
        "models_to_test['Naive'] = lambda data, h: naive_forecast(data, h)\n",
        "models_to_test['Moving Average'] = lambda data, h: moving_average_forecast(data, window=7, horizon=h)\n",
        "models_to_test['Seasonal Naive'] = lambda data, h: seasonal_naive_forecast(data, season_length=7, horizon=h)\n",
        "\n",
        "# –î–ª—è intermittent\n",
        "if demand_type == 'intermittent':\n",
        "    models_to_test['Croston'] = lambda data, h: croston_forecast(data, horizon=h)\n",
        "    models_to_test['SBA'] = lambda data, h: sba_forecast(data, horizon=h)\n",
        "    models_to_test['TSB'] = lambda data, h: tsb_forecast(data, horizon=h)\n",
        "\n",
        "# –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ (–µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö)\n",
        "if len(sales_series) >= 30:\n",
        "    models_to_test['Linear Regression'] = lambda data, cal, cal_train: linear_regression_forecast(data, cal, cal_train)\n",
        "    models_to_test['ARIMA'] = lambda data, h: arima_forecast(data, horizon=h)\n",
        "    \n",
        "if len(sales_series) >= 60:\n",
        "    models_to_test['SARIMA'] = lambda data, h: sarima_forecast(data, horizon=h)\n",
        "    models_to_test['SARIMAX'] = lambda data, exog_train, exog_test, h: sarimax_forecast(data, exog_train, exog_test, h)\n",
        "    \n",
        "if len(sales_series) >= 90:\n",
        "    models_to_test['Prophet'] = lambda data, dates, test_dates: prophet_forecast(data, dates, test_dates)\n",
        "\n",
        "# Backtesting\n",
        "if len(sales_series) >= 180:\n",
        "    backtest_results = rolling_backtest(\n",
        "        sales_series, \n",
        "        example_calendar,\n",
        "        models_to_test,\n",
        "        train_size=120,\n",
        "        test_size=30,\n",
        "        step=30\n",
        "    )\n",
        "    \n",
        "    if len(backtest_results) > 0:\n",
        "        print(\"\\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã backtesting:\")\n",
        "        model_comparison = backtest_results.groupby('model').agg({\n",
        "            'MAE': 'mean',\n",
        "            'RMSE': 'mean',\n",
        "            'MAPE': 'mean'\n",
        "        }).round(2).sort_values('MAE')\n",
        "        print(model_comparison)\n",
        "        \n",
        "        # –í—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\n",
        "        best_model = model_comparison.index[0]\n",
        "        print(f\"\\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model}\")\n",
        "        print(f\"   MAE: {model_comparison.loc[best_model, 'MAE']:.2f}\")\n",
        "        print(f\"   RMSE: {model_comparison.loc[best_model, 'RMSE']:.2f}\")\n",
        "        print(f\"   MAPE: {model_comparison.loc[best_model, 'MAPE']:.2f}%\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è backtesting\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ backtesting (–Ω—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º 180 –¥–Ω–µ–π)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Rolling-–ø—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂ –Ω–∞ 18 –º–µ—Å—è—Ü–µ–≤\n",
        "\n",
        "–°–æ–∑–¥–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ solo-code –Ω–∞ 18 –º–µ—Å—è—Ü–µ–≤ –≤–ø–µ—Ä–µ–¥\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_forecast(solo_code, sales_data, calendar_data, demand_type, models_dict, horizon_months=18):\n",
        "    \"\"\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–æ–≤–∞—Ä–∞\"\"\"\n",
        "    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
        "    item_data = sales_data[sales_data['solo-code'] == solo_code].sort_values('–î–∞—Ç–∞')\n",
        "    item_calendar = calendar_data[calendar_data['solo-code'] == solo_code].sort_values('–î–∞—Ç–∞')\n",
        "    \n",
        "    if len(item_data) == 0:\n",
        "        return None\n",
        "    \n",
        "    sales_series = item_data['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'].reset_index(drop=True)\n",
        "    \n",
        "    # –°–æ–∑–¥–∞–Ω–∏–µ –∫–∞–ª–µ–Ω–¥–∞—Ä—è –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞\n",
        "    last_date = item_data['–î–∞—Ç–∞'].max()\n",
        "    forecast_dates = pd.date_range(last_date + timedelta(days=1), periods=horizon_months * 30, freq='D')\n",
        "    forecast_calendar = create_calendar_features(pd.DataFrame({'–î–∞—Ç–∞': forecast_dates}))\n",
        "    forecast_calendar = create_holiday_features(forecast_calendar)\n",
        "    forecast_calendar['solo-code'] = solo_code\n",
        "    \n",
        "    # –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Å–ø—Ä–æ—Å–∞ –∏ –¥–ª–∏–Ω—ã –∏—Å—Ç–æ—Ä–∏–∏\n",
        "    selected_model = None\n",
        "    model_name = None\n",
        "    \n",
        "    # –õ–æ–≥–∏–∫–∞ –≤—ã–±–æ—Ä–∞ –º–æ–¥–µ–ª–∏\n",
        "    if demand_type == 'intermittent' and len(sales_series) >= 20:\n",
        "        # –î–ª—è –ø—Ä–µ—Ä—ã–≤–∏—Å—Ç–æ–≥–æ —Å–ø—Ä–æ—Å–∞\n",
        "        if 'TSB' in models_dict:\n",
        "            selected_model = lambda: tsb_forecast(sales_series, horizon=len(forecast_dates))\n",
        "            model_name = 'TSB'\n",
        "        elif 'Croston' in models_dict:\n",
        "            selected_model = lambda: croston_forecast(sales_series, horizon=len(forecast_dates))\n",
        "            model_name = 'Croston'\n",
        "        else:\n",
        "            selected_model = lambda: moving_average_forecast(sales_series, horizon=len(forecast_dates))\n",
        "            model_name = 'Moving Average'\n",
        "    elif len(sales_series) >= 90:\n",
        "        # –î–ª—è –¥–ª–∏–Ω–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏ - Prophet –∏–ª–∏ SARIMA\n",
        "        if 'Prophet' in models_dict:\n",
        "            train_dates = item_calendar['–î–∞—Ç–∞'].values\n",
        "            selected_model = lambda: prophet_forecast(sales_series, train_dates, forecast_dates)\n",
        "            model_name = 'Prophet'\n",
        "        elif 'SARIMA' in models_dict:\n",
        "            selected_model = lambda: sarima_forecast(sales_series, horizon=len(forecast_dates))\n",
        "            model_name = 'SARIMA'\n",
        "        else:\n",
        "            selected_model = lambda: sarima_forecast(sales_series, horizon=len(forecast_dates))\n",
        "            model_name = 'SARIMA'\n",
        "    elif len(sales_series) >= 30:\n",
        "        # –î–ª—è —Å—Ä–µ–¥–Ω–µ–π –∏—Å—Ç–æ—Ä–∏–∏ - ARIMA –∏–ª–∏ Linear Regression\n",
        "        if 'ARIMA' in models_dict:\n",
        "            selected_model = lambda: arima_forecast(sales_series, horizon=len(forecast_dates))\n",
        "            model_name = 'ARIMA'\n",
        "        elif 'Linear Regression' in models_dict:\n",
        "            selected_model = lambda: linear_regression_forecast(sales_series, forecast_calendar, item_calendar)\n",
        "            model_name = 'Linear Regression'\n",
        "        else:\n",
        "            selected_model = lambda: moving_average_forecast(sales_series, horizon=len(forecast_dates))\n",
        "            model_name = 'Moving Average'\n",
        "    else:\n",
        "        # –î–ª—è –∫–æ—Ä–æ—Ç–∫–æ–π –∏—Å—Ç–æ—Ä–∏–∏ - –ø—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏\n",
        "        selected_model = lambda: moving_average_forecast(sales_series, horizon=len(forecast_dates))\n",
        "        model_name = 'Moving Average'\n",
        "    \n",
        "    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞\n",
        "    try:\n",
        "        forecast_values = selected_model()\n",
        "        forecast_values = [max(0, x) for x in forecast_values]  # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–µ –¥–æ–ø—É—Å–∫–∞–µ–º\n",
        "    except Exception as e:\n",
        "        # Fallback –∫ –ø—Ä–æ—Å—Ç–æ–π –º–æ–¥–µ–ª–∏\n",
        "        forecast_values = moving_average_forecast(sales_series, horizon=len(forecast_dates))\n",
        "        model_name = 'Moving Average (fallback)'\n",
        "    \n",
        "    # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
        "    result = pd.DataFrame({\n",
        "        '–î–∞—Ç–∞': forecast_dates[:len(forecast_values)],\n",
        "        'solo-code': solo_code,\n",
        "        '–ü—Ä–æ–≥–Ω–æ–∑': forecast_values,\n",
        "        '–ú–æ–¥–µ–ª—å': model_name\n",
        "    })\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"‚úÖ –§—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –¥–ª—è –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤\n",
        "all_forecasts = []\n",
        "\n",
        "print(\"üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –¥–ª—è –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤...\")\n",
        "for idx, solo_code in enumerate(all_solo_codes):\n",
        "    if (idx + 1) % 10 == 0:\n",
        "        print(f\"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {idx + 1}/{len(all_solo_codes)}\")\n",
        "    \n",
        "    demand_type = demand_df[demand_df['solo-code'] == solo_code]['demand_type'].iloc[0]\n",
        "    \n",
        "    # –ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è —Å—É–º–º–∞—Ä–Ω—ã—Ö –ø—Ä–æ–¥–∞–∂\n",
        "    forecast_total = generate_forecast(\n",
        "        solo_code, \n",
        "        total_sales, \n",
        "        total_sales,\n",
        "        demand_type,\n",
        "        models_to_test,\n",
        "        horizon_months=18\n",
        "    )\n",
        "    \n",
        "    if forecast_total is not None:\n",
        "        forecast_total['–ö–∞–Ω–∞–ª'] = 'Total'\n",
        "        all_forecasts.append(forecast_total)\n",
        "    \n",
        "    # –ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è WB\n",
        "    forecast_wb = generate_forecast(\n",
        "        solo_code,\n",
        "        wb_sales_processed,\n",
        "        wb_sales_processed,\n",
        "        demand_type,\n",
        "        models_to_test,\n",
        "        horizon_months=18\n",
        "    )\n",
        "    \n",
        "    if forecast_wb is not None:\n",
        "        forecast_wb['–ö–∞–Ω–∞–ª'] = 'WB'\n",
        "        all_forecasts.append(forecast_wb)\n",
        "    \n",
        "    # –ü—Ä–æ–≥–Ω–æ–∑ –¥–ª—è Ozon\n",
        "    forecast_ozon = generate_forecast(\n",
        "        solo_code,\n",
        "        ozon_sales_processed,\n",
        "        ozon_sales_processed,\n",
        "        demand_type,\n",
        "        models_to_test,\n",
        "        horizon_months=18\n",
        "    )\n",
        "    \n",
        "    if forecast_ozon is not None:\n",
        "        forecast_ozon['–ö–∞–Ω–∞–ª'] = 'Ozon'\n",
        "        all_forecasts.append(forecast_ozon)\n",
        "\n",
        "if all_forecasts:\n",
        "    forecasts_df = pd.concat(all_forecasts, ignore_index=True)\n",
        "    print(f\"\\n‚úÖ –ü—Ä–æ–≥–Ω–æ–∑—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã: {forecasts_df.shape}\")\n",
        "    print(f\"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–≤–∞—Ä–æ–≤: {forecasts_df['solo-code'].nunique()}\")\n",
        "    print(f\"üìä –ö–∞–Ω–∞–ª–æ–≤: {forecasts_df['–ö–∞–Ω–∞–ª'].unique()}\")\n",
        "    print(f\"\\nüìä –ü—Ä–∏–º–µ—Ä –ø—Ä–æ–≥–Ω–æ–∑–∞:\")\n",
        "    print(forecasts_df.head(10))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è –ü—Ä–æ–≥–Ω–æ–∑—ã –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ —Ç–æ–≤–∞—Ä–∞\n",
        "if 'forecasts_df' in locals() and len(forecasts_df) > 0:\n",
        "    example_solo_code = all_solo_codes[0]\n",
        "    \n",
        "    # –§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ\n",
        "    actual_total = total_sales[total_sales['solo-code'] == example_solo_code].sort_values('–î–∞—Ç–∞')\n",
        "    actual_wb = wb_sales_processed[wb_sales_processed['solo-code'] == example_solo_code].sort_values('–î–∞—Ç–∞')\n",
        "    actual_ozon = ozon_sales_processed[ozon_sales_processed['solo-code'] == example_solo_code].sort_values('–î–∞—Ç–∞')\n",
        "    \n",
        "    # –ü—Ä–æ–≥–Ω–æ–∑—ã\n",
        "    forecast_total = forecasts_df[(forecasts_df['solo-code'] == example_solo_code) & \n",
        "                                  (forecasts_df['–ö–∞–Ω–∞–ª'] == 'Total')].sort_values('–î–∞—Ç–∞')\n",
        "    forecast_wb = forecasts_df[(forecasts_df['solo-code'] == example_solo_code) & \n",
        "                                (forecasts_df['–ö–∞–Ω–∞–ª'] == 'WB')].sort_values('–î–∞—Ç–∞')\n",
        "    forecast_ozon = forecasts_df[(forecasts_df['solo-code'] == example_solo_code) & \n",
        "                                 (forecasts_df['–ö–∞–Ω–∞–ª'] == 'Ozon')].sort_values('–î–∞—Ç–∞')\n",
        "    \n",
        "    fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
        "    \n",
        "    # Total\n",
        "    axes[0].plot(actual_total['–î–∞—Ç–∞'], actual_total['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'], \n",
        "                 label='–§–∞–∫—Ç', linewidth=2, color='blue')\n",
        "    if len(forecast_total) > 0:\n",
        "        axes[0].plot(forecast_total['–î–∞—Ç–∞'], forecast_total['–ü—Ä–æ–≥–Ω–æ–∑'], \n",
        "                    label='–ü—Ä–æ–≥–Ω–æ–∑', linewidth=2, color='red', linestyle='--')\n",
        "        axes[0].axvline(x=actual_total['–î–∞—Ç–∞'].max(), color='gray', linestyle=':', alpha=0.7, label='–ù–∞—á–∞–ª–æ –ø—Ä–æ–≥–Ω–æ–∑–∞')\n",
        "    axes[0].set_title(f'–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂: {example_solo_code} - Total', fontsize=14, fontweight='bold')\n",
        "    axes[0].set_xlabel('–î–∞—Ç–∞')\n",
        "    axes[0].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫–æ–≤–æ–∫')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # WB\n",
        "    axes[1].plot(actual_wb['–î–∞—Ç–∞'], actual_wb['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'], \n",
        "                 label='–§–∞–∫—Ç WB', linewidth=2, color='blue')\n",
        "    if len(forecast_wb) > 0:\n",
        "        axes[1].plot(forecast_wb['–î–∞—Ç–∞'], forecast_wb['–ü—Ä–æ–≥–Ω–æ–∑'], \n",
        "                    label='–ü—Ä–æ–≥–Ω–æ–∑ WB', linewidth=2, color='red', linestyle='--')\n",
        "        axes[1].axvline(x=actual_wb['–î–∞—Ç–∞'].max(), color='gray', linestyle=':', alpha=0.7)\n",
        "    axes[1].set_title(f'–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂: {example_solo_code} - Wildberries', fontsize=14, fontweight='bold')\n",
        "    axes[1].set_xlabel('–î–∞—Ç–∞')\n",
        "    axes[1].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫–æ–≤–æ–∫')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Ozon\n",
        "    axes[2].plot(actual_ozon['–î–∞—Ç–∞'], actual_ozon['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'], \n",
        "                 label='–§–∞–∫—Ç Ozon', linewidth=2, color='blue')\n",
        "    if len(forecast_ozon) > 0:\n",
        "        axes[2].plot(forecast_ozon['–î–∞—Ç–∞'], forecast_ozon['–ü—Ä–æ–≥–Ω–æ–∑'], \n",
        "                    label='–ü—Ä–æ–≥–Ω–æ–∑ Ozon', linewidth=2, color='red', linestyle='--')\n",
        "        axes[2].axvline(x=actual_ozon['–î–∞—Ç–∞'].max(), color='gray', linestyle=':', alpha=0.7)\n",
        "    axes[2].set_title(f'–ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂: {example_solo_code} - Ozon', fontsize=14, fontweight='bold')\n",
        "    axes[2].set_xlabel('–î–∞—Ç–∞')\n",
        "    axes[2].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫–æ–≤–æ–∫')\n",
        "    axes[2].legend()\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\nüìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å: {forecast_total['–ú–æ–¥–µ–ª—å'].iloc[0] if len(forecast_total) > 0 else 'N/A'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. –†–∞—Å—á–µ—Ç —Å—Ç—Ä–∞—Ö–æ–≤–æ–≥–æ –∑–∞–ø–∞—Å–∞\n",
        "\n",
        "–§–æ—Ä–º—É–ª–∞: Safety Stock = Z * œÉ * sqrt(lead_time)\n",
        "\n",
        "–≥–¥–µ:\n",
        "- Z = 1.65 –¥–ª—è service level 95%\n",
        "- œÉ = —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–Ω–µ–≤–Ω–æ–≥–æ —Å–ø—Ä–æ—Å–∞\n",
        "- lead_time = –≤—Ä–µ–º—è –ø–æ—Å—Ç–∞–≤–∫–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 30 –¥–Ω–µ–π)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_safety_stock(sales_data, service_level=0.95, lead_time=30):\n",
        "    \"\"\"–†–∞—Å—á–µ—Ç —Å—Ç—Ä–∞—Ö–æ–≤–æ–≥–æ –∑–∞–ø–∞—Å–∞ –¥–ª—è –≤—Å–µ—Ö —Ç–æ–≤–∞—Ä–æ–≤\"\"\"\n",
        "    # Z-score –¥–ª—è service level\n",
        "    from scipy import stats\n",
        "    z_score = stats.norm.ppf(service_level)\n",
        "    \n",
        "    safety_stocks = []\n",
        "    \n",
        "    for solo_code in all_solo_codes:\n",
        "        item_data = sales_data[sales_data['solo-code'] == solo_code].sort_values('–î–∞—Ç–∞')\n",
        "        \n",
        "        if len(item_data) == 0:\n",
        "            continue\n",
        "        \n",
        "        demand = item_data['–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–ø–∞–∫.'].values\n",
        "        \n",
        "        # –°—Ä–µ–¥–Ω–∏–π –¥–Ω–µ–≤–Ω–æ–π —Å–ø—Ä–æ—Å\n",
        "        mean_demand = np.mean(demand)\n",
        "        \n",
        "        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –¥–Ω–µ–≤–Ω–æ–≥–æ —Å–ø—Ä–æ—Å–∞\n",
        "        std_demand = np.std(demand)\n",
        "        \n",
        "        # –°—Ç—Ä–∞—Ö–æ–≤–æ–π –∑–∞–ø–∞—Å\n",
        "        safety_stock = z_score * std_demand * np.sqrt(lead_time)\n",
        "        safety_stock = max(0, safety_stock)  # –ù–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º\n",
        "        \n",
        "        safety_stocks.append({\n",
        "            'solo-code': solo_code,\n",
        "            'mean_demand': mean_demand,\n",
        "            'std_demand': std_demand,\n",
        "            'safety_stock': safety_stock,\n",
        "            'lead_time': lead_time,\n",
        "            'service_level': service_level\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(safety_stocks)\n",
        "\n",
        "# –†–∞—Å—á–µ—Ç —Å—Ç—Ä–∞—Ö–æ–≤–æ–≥–æ –∑–∞–ø–∞—Å–∞\n",
        "safety_stock_df = calculate_safety_stock(total_sales, service_level=0.95, lead_time=30)\n",
        "\n",
        "print(\"üìä –°—Ç—Ä–∞—Ö–æ–≤–æ–π –∑–∞–ø–∞—Å —Ä–∞—Å—Å—á–∏—Ç–∞–Ω:\")\n",
        "print(safety_stock_df.head(10))\n",
        "print(f\"\\nüìä –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
        "print(safety_stock_df[['mean_demand', 'std_demand', 'safety_stock']].describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≥—Ä—É–∑–æ–∫\n",
        "\n",
        "–ù–∞ –æ—Å–Ω–æ–≤–µ:\n",
        "- –ü—Ä–æ–≥–Ω–æ–∑–∞ –ø—Ä–æ–¥–∞–∂\n",
        "- –û—Å—Ç–∞—Ç–∫–æ–≤ –Ω–∞ —Å–∫–ª–∞–¥–∞—Ö (WB, Ozon, –Ω–∞—à —Å–∫–ª–∞–¥)\n",
        "- –°—Ç—Ä–∞—Ö–æ–≤–æ–≥–æ –∑–∞–ø–∞—Å–∞\n",
        "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–∞ —É–ø–∞–∫–æ–≤–æ–∫ –≤ –∫–æ—Ä–æ–±–µ (count_box)\n",
        "\n",
        "–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º:\n",
        "- –ß–∏—Å—Ç—É—é –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å\n",
        "- –û—Ç–≥—Ä—É–∑–∫—É –≤ —É–ø–∞–∫–æ–≤–∫–∞—Ö –∏ –∫–æ—Ä–æ–±–∞—Ö\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plan_shipments(forecasts_df, stocks_wb, stocks_ozon, stocks_our, safety_stock_df, count_box_df, horizon_days=30):\n",
        "    \"\"\"–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≥—Ä—É–∑–æ–∫\"\"\"\n",
        "    shipment_plan = []\n",
        "    \n",
        "    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è count_box\n",
        "    count_box_dict = dict(zip(count_box_df['solo-code'], count_box_df['–ö–æ–ª-–≤–æ']))\n",
        "    \n",
        "    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –æ—Å—Ç–∞—Ç–∫–∏\n",
        "    last_date = max(\n",
        "        stocks_wb['–î–∞—Ç–∞'].max(),\n",
        "        stocks_ozon['–î–∞—Ç–∞'].max(),\n",
        "        stocks_our['–î–∞—Ç–∞'].max()\n",
        "    )\n",
        "    \n",
        "    last_stocks_wb = stocks_wb[stocks_wb['–î–∞—Ç–∞'] == last_date].set_index('solo-code')['–û—Å—Ç–∞—Ç–æ–∫'].to_dict()\n",
        "    last_stocks_ozon = stocks_ozon[stocks_ozon['–î–∞—Ç–∞'] == last_date].set_index('solo-code')['–û—Å—Ç–∞—Ç–æ–∫'].to_dict()\n",
        "    last_stocks_our = stocks_our[stocks_our['–î–∞—Ç–∞'] == last_date].set_index('solo-code')['–û—Å—Ç–∞—Ç–æ–∫'].to_dict()\n",
        "    \n",
        "    # –°–ª–æ–≤–∞—Ä—å —Å—Ç—Ä–∞—Ö–æ–≤–æ–≥–æ –∑–∞–ø–∞—Å–∞\n",
        "    safety_stock_dict = dict(zip(safety_stock_df['solo-code'], safety_stock_df['safety_stock']))\n",
        "    \n",
        "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –ø–æ –∫–∞–Ω–∞–ª–∞–º\n",
        "    for channel in ['WB', 'Ozon']:\n",
        "        channel_forecasts = forecasts_df[\n",
        "            (forecasts_df['–ö–∞–Ω–∞–ª'] == channel) & \n",
        "            (forecasts_df['–î–∞—Ç–∞'] <= last_date + timedelta(days=horizon_days))\n",
        "        ].copy()\n",
        "        \n",
        "        for solo_code in channel_forecasts['solo-code'].unique():\n",
        "            item_forecast = channel_forecasts[channel_forecasts['solo-code'] == solo_code].sort_values('–î–∞—Ç–∞')\n",
        "            \n",
        "            # –¢–µ–∫—É—â–∏–µ –æ—Å—Ç–∞—Ç–∫–∏\n",
        "            stock_wb = last_stocks_wb.get(solo_code, 0)\n",
        "            stock_ozon = last_stocks_ozon.get(solo_code, 0)\n",
        "            stock_our = last_stocks_our.get(solo_code, 0)\n",
        "            \n",
        "            # –°—Ç—Ä–∞—Ö–æ–≤–æ–π –∑–∞–ø–∞—Å\n",
        "            safety_stock = safety_stock_dict.get(solo_code, 0)\n",
        "            \n",
        "            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤ –∫–æ—Ä–æ–±–µ\n",
        "            boxes_per_pack = count_box_dict.get(solo_code, 1)\n",
        "            \n",
        "            # –ü—Ä–æ–≥–Ω–æ–∑ –ø—Ä–æ–¥–∞–∂ –Ω–∞ –ø–µ—Ä–∏–æ–¥\n",
        "            forecast_demand = item_forecast['–ü—Ä–æ–≥–Ω–æ–∑'].sum()\n",
        "            \n",
        "            # –¢–µ–∫—É—â–∏–π –æ—Å—Ç–∞—Ç–æ–∫ –Ω–∞ –∫–∞–Ω–∞–ª–µ\n",
        "            if channel == 'WB':\n",
        "                current_stock = stock_wb\n",
        "            else:\n",
        "                current_stock = stock_ozon\n",
        "            \n",
        "            # –ß–∏—Å—Ç–∞—è –ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å = –ø—Ä–æ–≥–Ω–æ–∑ + —Å—Ç—Ä–∞—Ö–æ–≤–æ–π –∑–∞–ø–∞—Å - —Ç–µ–∫—É—â–∏–π –æ—Å—Ç–∞—Ç–æ–∫\n",
        "            net_requirement = forecast_demand + safety_stock - current_stock\n",
        "            net_requirement = max(0, net_requirement)  # –ù–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º\n",
        "            \n",
        "            # –û–∫—Ä—É–≥–ª–µ–Ω–∏–µ –¥–æ —Ü–µ–ª–æ–≥–æ –∫–æ—Ä–æ–±–∞\n",
        "            shipment_boxes = np.ceil(net_requirement / boxes_per_pack) if boxes_per_pack > 0 else 0\n",
        "            shipment_packs = shipment_boxes * boxes_per_pack\n",
        "            \n",
        "            shipment_plan.append({\n",
        "                '–î–∞—Ç–∞_–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è': last_date,\n",
        "                'solo-code': solo_code,\n",
        "                '–ö–∞–Ω–∞–ª': channel,\n",
        "                '–ü—Ä–æ–≥–Ω–æ–∑_–ø—Ä–æ–¥–∞–∂': forecast_demand,\n",
        "                '–û—Å—Ç–∞—Ç–æ–∫_WB': stock_wb,\n",
        "                '–û—Å—Ç–∞—Ç–æ–∫_Ozon': stock_ozon,\n",
        "                '–û—Å—Ç–∞—Ç–æ–∫_–Ω–∞—à': stock_our,\n",
        "                '–°—Ç—Ä–∞—Ö–æ–≤–æ–π_–∑–∞–ø–∞—Å': safety_stock,\n",
        "                '–ß–∏—Å—Ç–∞—è_–ø–æ—Ç—Ä–µ–±–Ω–æ—Å—Ç—å': net_requirement,\n",
        "                '–û—Ç–≥—Ä—É–∑–∫–∞_—É–ø–∞–∫–æ–≤–æ–∫': shipment_packs,\n",
        "                '–û—Ç–≥—Ä—É–∑–∫–∞_–∫–æ—Ä–æ–±–æ–≤': shipment_boxes,\n",
        "                '–£–ø–∞–∫–æ–≤–æ–∫_–≤_–∫–æ—Ä–æ–±–µ': boxes_per_pack\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(shipment_plan)\n",
        "\n",
        "# –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≥—Ä—É–∑–æ–∫\n",
        "if 'forecasts_df' in locals() and len(forecasts_df) > 0:\n",
        "    shipment_plan_df = plan_shipments(\n",
        "        forecasts_df,\n",
        "        wb_stocks_processed,\n",
        "        ozon_stocks_processed,\n",
        "        our_stocks_processed,\n",
        "        safety_stock_df,\n",
        "        count_box,\n",
        "        horizon_days=30\n",
        "    )\n",
        "    \n",
        "    print(\"üì¶ –ü–ª–∞–Ω –æ—Ç–≥—Ä—É–∑–æ–∫ —Å–æ–∑–¥–∞–Ω:\")\n",
        "    print(shipment_plan_df.head(20))\n",
        "    print(f\"\\nüìä –°–≤–æ–¥–∫–∞ –ø–æ –æ—Ç–≥—Ä—É–∑–∫–∞–º:\")\n",
        "    print(shipment_plan_df.groupby('–ö–∞–Ω–∞–ª').agg({\n",
        "        '–û—Ç–≥—Ä—É–∑–∫–∞_—É–ø–∞–∫–æ–≤–æ–∫': 'sum',\n",
        "        '–û—Ç–≥—Ä—É–∑–∫–∞_–∫–æ—Ä–æ–±–æ–≤': 'sum'\n",
        "    }).round(2))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è –ü—Ä–æ–≥–Ω–æ–∑—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –ø–ª–∞–Ω –æ—Ç–≥—Ä—É–∑–æ–∫ –Ω–µ —Å–æ–∑–¥–∞–Ω\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"üìä –ò–¢–û–ì–û–í–ê–Ø –°–í–û–î–ö–ê –°–ò–°–¢–ï–ú–´ –ü–†–û–ì–ù–û–ó–ò–†–û–í–ê–ù–ò–Ø\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nüì¶ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ç–æ–≤–∞—Ä–æ–≤: {len(all_solo_codes)}\")\n",
        "print(f\"üìÖ –ü–µ—Ä–∏–æ–¥ –¥–∞–Ω–Ω—ã—Ö: {min_date.date()} - {max_date.date()}\")\n",
        "print(f\"üìÖ –í—Å–µ–≥–æ –¥–Ω–µ–π –≤ –∏—Å—Ç–æ—Ä–∏–∏: {(max_date - min_date).days + 1}\")\n",
        "\n",
        "print(f\"\\nüìä –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–ø—Ä–æ—Å–∞:\")\n",
        "if 'demand_df' in locals():\n",
        "    print(demand_df['demand_type'].value_counts())\n",
        "\n",
        "print(f\"\\nüìà –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏:\")\n",
        "if 'forecasts_df' in locals():\n",
        "    model_usage = forecasts_df.groupby('–ú–æ–¥–µ–ª—å').size().sort_values(ascending=False)\n",
        "    print(model_usage)\n",
        "\n",
        "print(f\"\\nüì¶ –ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≥—Ä—É–∑–æ–∫:\")\n",
        "if 'shipment_plan_df' in locals():\n",
        "    total_shipment = shipment_plan_df.groupby('–ö–∞–Ω–∞–ª').agg({\n",
        "        '–û—Ç–≥—Ä—É–∑–∫–∞_—É–ø–∞–∫–æ–≤–æ–∫': 'sum',\n",
        "        '–û—Ç–≥—Ä—É–∑–∫–∞_–∫–æ—Ä–æ–±–æ–≤': 'sum',\n",
        "        'solo-code': 'nunique'\n",
        "    })\n",
        "    print(total_shipment)\n",
        "\n",
        "print(f\"\\nüè¨ –°—Ç—Ä–∞—Ö–æ–≤–æ–π –∑–∞–ø–∞—Å:\")\n",
        "if 'safety_stock_df' in locals():\n",
        "    print(f\"  –°—Ä–µ–¥–Ω–∏–π —Å—Ç—Ä–∞—Ö–æ–≤–æ–π –∑–∞–ø–∞—Å: {safety_stock_df['safety_stock'].mean():.2f} —É–ø–∞–∫–æ–≤–æ–∫\")\n",
        "    print(f\"  –ú–µ–¥–∏–∞–Ω–Ω—ã–π —Å—Ç—Ä–∞—Ö–æ–≤–æ–π –∑–∞–ø–∞—Å: {safety_stock_df['safety_stock'].median():.2f} —É–ø–∞–∫–æ–≤–æ–∫\")\n",
        "    print(f\"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Å—Ç—Ä–∞—Ö–æ–≤–æ–π –∑–∞–ø–∞—Å: {safety_stock_df['safety_stock'].max():.2f} —É–ø–∞–∫–æ–≤–æ–∫\")\n",
        "\n",
        "print(\"\\n‚úÖ –°–∏—Å—Ç–µ–º–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"‚ö†Ô∏è –†–ò–°–ö–ò –ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nüî¥ –†–ò–°–ö–ò:\")\n",
        "print(\"1. –ö–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥–Ω–æ–∑–∞ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –¥–ª–∏–Ω—ã –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö\")\n",
        "print(\"   - –î–ª—è —Ç–æ–≤–∞—Ä–æ–≤ —Å –∫–æ—Ä–æ—Ç–∫–æ–π –∏—Å—Ç–æ—Ä–∏–µ–π (<30 –¥–Ω–µ–π) –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏\")\n",
        "print(\"   - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –Ω–∞–∫–∞–ø–ª–∏–≤–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –º–∏–Ω–∏–º—É–º 3-6 –º–µ—Å—è—Ü–µ–≤\")\n",
        "print(\"\\n2. –ü—Ä–µ—Ä—ã–≤–∏—Å—Ç—ã–π —Å–ø—Ä–æ—Å (intermittent) —Å–ª–æ–∂–µ–Ω –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è\")\n",
        "print(\"   - –í—ã—Å–æ–∫–∞—è –¥–æ–ª—è –Ω—É–ª–µ–≤—ã—Ö –¥–Ω–µ–π —Å–Ω–∏–∂–∞–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å\")\n",
        "print(\"   - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (Croston, TSB)\")\n",
        "print(\"\\n3. –í–Ω–µ—à–Ω–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã –Ω–µ —É—á—Ç–µ–Ω—ã\")\n",
        "print(\"   - –ü—Ä–æ–º–æ–∞–∫—Ü–∏–∏, –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω, –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–∞—è —Å—Ä–µ–¥–∞\")\n",
        "print(\"   - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –¥–æ–±–∞–≤–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏\")\n",
        "print(\"\\n4. –î–µ—Ñ–µ–∫—Ç—É—Ä–∞ –∏ –≤—ã–≤–æ–¥ —Ç–æ–≤–∞—Ä–æ–≤\")\n",
        "print(\"   - –ú–æ–≥—É—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ –≤–ª–∏—è—Ç—å –Ω–∞ –ø—Ä–æ–≥–Ω–æ–∑\")\n",
        "print(\"   - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: —Ä–µ–≥—É–ª—è—Ä–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏\")\n",
        "print(\"\\n5. –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å\")\n",
        "print(\"   - –ú–æ–¥–µ–ª–∏ —É—á–∏—Ç—ã–≤–∞—é—Ç –±–∞–∑–æ–≤—É—é —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å, –Ω–æ –º–æ–≥—É—Ç –Ω–µ —É–ª–∞–≤–ª–∏–≤–∞—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏–∫—É –∫–∞—Ç–µ–≥–æ—Ä–∏–∏\")\n",
        "print(\"   - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å –ø–æ–¥ –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ç–æ–≤–∞—Ä–∞\")\n",
        "\n",
        "print(\"\\nüü¢ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –†–ê–ó–í–ò–¢–ò–Æ:\")\n",
        "print(\"1. –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö\")\n",
        "print(\"   - –ï–∂–µ–¥–Ω–µ–≤–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\")\n",
        "print(\"   - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –ø–µ—Ä–µ—Å—á–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑–æ–≤\")\n",
        "print(\"\\n2. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞\")\n",
        "print(\"   - –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ (MAE, RMSE, MAPE) –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏\")\n",
        "print(\"   - –ê–ª–µ—Ä—Ç—ã –ø—Ä–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–º –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–∏\")\n",
        "print(\"\\n3. A/B —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\")\n",
        "print(\"   - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö\")\n",
        "print(\"   - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏\")\n",
        "print(\"\\n4. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≤–Ω–µ—à–Ω–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏\")\n",
        "print(\"   - –ü—Ä–æ–º–æ–∞–∫—Ü–∏–∏, –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ü–µ–Ω\")\n",
        "print(\"   - –≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\")\n",
        "print(\"   - –î–∞–Ω–Ω—ã–µ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã)\")\n",
        "print(\"\\n5. –ú—É–ª—å—Ç–∏—É—Ä–æ–≤–Ω–µ–≤–æ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ\")\n",
        "print(\"   - –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —É—Ä–æ–≤–Ω–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ + –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ SKU\")\n",
        "print(\"   - –£—á–µ—Ç –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–æ–≤–∞—Ä–∞–º–∏\")\n",
        "print(\"\\n6. –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–ø–∞—Å–æ–≤\")\n",
        "print(\"   - –£—á–µ—Ç –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π –ø–æ —Å–∫–ª–∞–¥–∞–º\")\n",
        "print(\"   - –ú–∏–Ω–∏–º–∏–∑–∞—Ü–∏—è –æ–±—â–∏—Ö –∑–∞—Ç—Ä–∞—Ç (—Ö—Ä–∞–Ω–µ–Ω–∏–µ + –¥–µ—Ñ–∏—Ü–∏—Ç)\")\n",
        "print(\"\\n7. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –¥–∞—à–±–æ—Ä–¥—ã\")\n",
        "print(\"   - –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤\")\n",
        "print(\"   - –ê–Ω–∞–ª–∏–∑ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–π\")\n",
        "print(\"   - –ü–ª–∞–Ω—ã –æ—Ç–≥—Ä—É–∑–æ–∫ –≤ —É–¥–æ–±–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ\")\n",
        "\n",
        "print(\"\\n‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
